{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c66c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the shapes\n",
    "x_shape = (5, 224, 224, 3)\n",
    "y_shape = (5, 1000)\n",
    "\n",
    "# Generate random x_train data\n",
    "x_train = np.random.rand(*x_shape)\n",
    "\n",
    "# Generate random y_train labels\n",
    "y_train = np.random.rand(*y_shape)\n",
    "\n",
    "# Now, x_train has shape (5, 224, 224, 3) and y_train has shape (5, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dedb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ba78d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1000) (5, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_train),np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab306bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/vit-large-patch32-224-in21k were not used when initializing TFViTForImageClassification: ['vit/pooler/dense/bias:0', 'vit/pooler/dense/kernel:0']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFViTForImageClassification were not initialized from the model checkpoint at google/vit-large-patch32-224-in21k and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification,TFViTForImageClassification\n",
    "model_name = \"google/vit-large-patch32-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor(model_name)\n",
    "model = TFViTForImageClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2cf8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  305510400 \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 305,512,450\n",
      "Trainable params: 305,512,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d23f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.training_args because of the following error (look up to see its traceback):\ncannot import name 'is_torch_npu_available' from 'transformers.utils' (C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\import_utils.py:1099\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__spec__ \u001b[38;5;241m=\u001b[39m module_spec\n\u001b[1;32m-> 1099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__path__ \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(module_file)]\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m extra_objects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m extra_objects\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\training_args.py:31\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DebugOption\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     EvaluationStrategy,\n\u001b[0;32m     33\u001b[0m     FSDPOption,\n\u001b[0;32m     34\u001b[0m     HubStrategy,\n\u001b[0;32m     35\u001b[0m     IntervalStrategy,\n\u001b[0;32m     36\u001b[0m     SchedulerType,\n\u001b[0;32m     37\u001b[0m     ShardedDDPOption,\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     40\u001b[0m     ExplicitEnum,\n\u001b[0;32m     41\u001b[0m     cached_property,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     requires_backends,\n\u001b[0;32m     55\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\trainer_utils.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     33\u001b[0m     ExplicitEnum,\n\u001b[0;32m     34\u001b[0m     is_psutil_available,\n\u001b[0;32m     35\u001b[0m     is_tf_available,\n\u001b[0;32m     36\u001b[0m     is_torch_available,\n\u001b[0;32m     37\u001b[0m     is_torch_cuda_available,\n\u001b[0;32m     38\u001b[0m     is_torch_mps_available,\n\u001b[0;32m     39\u001b[0m     is_torch_npu_available,\n\u001b[0;32m     40\u001b[0m     is_torch_tpu_available,\n\u001b[0;32m     41\u001b[0m     requires_backends,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_torch_npu_available' from 'transformers.utils' (C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTFeatureExtractor, TFViTForImageClassification, TrainingArguments, Trainer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the pre-trained ViT model and feature extractor\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\import_utils.py:1089\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, module_file, import_structure, module_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extra_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1089\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(import_structure\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   1091\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\import_utils.py:1101\u001b[0m, in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__path__ \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(module_file)]\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objects \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;28;01mif\u001b[39;00m extra_objects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m extra_objects\n\u001b[1;32m-> 1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_import_structure \u001b[38;5;241m=\u001b[39m import_structure\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.training_args because of the following error (look up to see its traceback):\ncannot import name 'is_torch_npu_available' from 'transformers.utils' (C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Load the pre-trained ViT model and feature extractor\n",
    "model_name = \"google/vit-large-patch32-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "model = TFViTForImageClassification.from_pretrained(model_name, num_labels=1000)  # Change num_labels to 1000\n",
    "\n",
    "# Prepare your training data\n",
    "# Assuming x_train contains your input images and y_train contains the corresponding labels\n",
    "\n",
    "# Define your data collator (for padding)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=feature_extractor)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit_finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,  # Adjust the number of training epochs as needed\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,  # Set to True if you want to upload to the Hugging Face model hub\n",
    "    logging_dir=\"./logs\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7163337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=list(zip(x_train, y_train)),  # Combine x_train and y_train as a dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the ViT model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbee72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c0697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9f04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c9d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c97b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4708d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98980f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4747499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8668a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce56bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db010f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79324537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff17b35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990186ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d4f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500db15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4e708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e60fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d2550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888605cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4ab13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bdb36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb633e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb408ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03286a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf88f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
