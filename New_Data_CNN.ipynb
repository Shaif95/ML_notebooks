{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896f6083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 256\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "hidden_units = 256\n",
    "projection_units = 128\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463d11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4944a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "trn1='D:/data/invasive-aquatic-species-data/noninvasive/*/'\n",
    "trn2='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Ostracod Image1/*/'\n",
    "trn3='D:/data/invasive-aquatic-species-data/invasive/*/'\n",
    "\n",
    "tr1= glob(trn1)\n",
    "tr2= glob(trn2)\n",
    "tr3= glob(trn3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91d6e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac0d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4879610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700 19 674 4393\n"
     ]
    }
   ],
   "source": [
    "print(len(tr1),len(tr2),len(tr3),(len(tr1)+len(tr2)+len(tr3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66852ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "tr1 = shuffle(tr1)\n",
    "tr2 = shuffle(tr2)\n",
    "tr3 = shuffle(tr3)\n",
    "tran_index_noninv = np.round( len(tr1)* .6  )\n",
    "tran_index_osc = np.round( len(tr2)* .7  )\n",
    "tran_index_inv = np.round( len(tr3)* .7  )\n",
    "tran_index_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca8b263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "897.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329a5c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2500239",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "for i in tr1[:(int) (tran_index_noninv)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "for i in tr2[:(int)(tran_index_osc)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(1)\n",
    "        \n",
    "for i in tr3[:(int)(tran_index_inv)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(2)\n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 30, 30)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(30,30,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f87b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),30,30,3))\n",
    "# One hot vector representation of labels\n",
    "Y_train = to_categorical (label)\n",
    "\n",
    "X_train,Y_train = shuffle(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ccd8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b8ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "for i in tr1[(int) (tran_index_noninv) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "for i in tr2[(int)(tran_index_osc) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(1)\n",
    "        \n",
    "for i in tr3[(int)(tran_index_inv) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(2)\n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 30, 30)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(30,30,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deff7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),30,30,3))\n",
    "# One hot vector representation of labels\n",
    "Y_test = to_categorical(label)\n",
    "\n",
    "X_test,Y_test = shuffle(X_test , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fb79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e3ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 12, 12, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                65600     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,603\n",
      "Trainable params: 122,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(30, 30, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Dense(10))\n",
    "model.add(layers.Dense(3, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace01100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142352"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a425a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08afc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hardcoded number is for train/val split\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create a dataset for the training data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train[:120000], Y_train[:120000].astype('float32')))\n",
    "\n",
    "# Shuffle, batch, and repeat the dataset for training\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "# Create a dataset for the validation/test data\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_train[120000:], Y_train[120000:].astype('float32')))\n",
    "\n",
    "# Batch the dataset for validation/test\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Now, you can use the `train_dataset` and `val_dataset` to train and validate your Keras model.\n",
    "\n",
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=([get_f1]))\n",
    "model.fit(train_dataset,  \n",
    "          validation_data=val_dataset,  \n",
    "          epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0f7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cb3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1547/1547 [==============================] - 25s 10ms/step - loss: 0.2155 - get_f1: 0.9117 - val_loss: 0.1527 - val_get_f1: 0.9387\n",
      "Epoch 2/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.1608 - get_f1: 0.9342 - val_loss: 0.1361 - val_get_f1: 0.9463\n",
      "Epoch 3/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.1478 - get_f1: 0.9399 - val_loss: 0.1453 - val_get_f1: 0.9478\n",
      "Epoch 4/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.1360 - get_f1: 0.9457 - val_loss: 0.1632 - val_get_f1: 0.9396\n",
      "Epoch 5/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.1237 - get_f1: 0.9505 - val_loss: 0.1196 - val_get_f1: 0.9526\n",
      "Epoch 6/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.1193 - get_f1: 0.9530 - val_loss: 0.1250 - val_get_f1: 0.9510\n",
      "Epoch 7/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.1097 - get_f1: 0.9562 - val_loss: 0.1171 - val_get_f1: 0.9553\n",
      "Epoch 8/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.1007 - get_f1: 0.9595 - val_loss: 0.1234 - val_get_f1: 0.9536\n",
      "Epoch 9/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.0992 - get_f1: 0.9604 - val_loss: 0.1225 - val_get_f1: 0.9532\n",
      "Epoch 10/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0899 - get_f1: 0.9637 - val_loss: 0.1173 - val_get_f1: 0.9564\n",
      "Epoch 11/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0858 - get_f1: 0.9655 - val_loss: 0.1272 - val_get_f1: 0.9496\n",
      "Epoch 12/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.0788 - get_f1: 0.9690 - val_loss: 0.1257 - val_get_f1: 0.9527\n",
      "Epoch 13/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0745 - get_f1: 0.9700 - val_loss: 0.1300 - val_get_f1: 0.9516\n",
      "Epoch 14/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.0705 - get_f1: 0.9728 - val_loss: 0.1712 - val_get_f1: 0.9422\n",
      "Epoch 15/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0643 - get_f1: 0.9741 - val_loss: 0.1274 - val_get_f1: 0.9539\n",
      "Epoch 16/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0611 - get_f1: 0.9754 - val_loss: 0.1324 - val_get_f1: 0.9544\n",
      "Epoch 17/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0575 - get_f1: 0.9766 - val_loss: 0.1609 - val_get_f1: 0.9492\n",
      "Epoch 18/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0523 - get_f1: 0.9788 - val_loss: 0.1448 - val_get_f1: 0.9534\n",
      "Epoch 19/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0515 - get_f1: 0.9791 - val_loss: 0.1488 - val_get_f1: 0.9523\n",
      "Epoch 20/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0501 - get_f1: 0.9803 - val_loss: 0.1682 - val_get_f1: 0.9490\n",
      "Epoch 21/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0448 - get_f1: 0.9823 - val_loss: 0.1543 - val_get_f1: 0.9525\n",
      "Epoch 22/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0447 - get_f1: 0.9825 - val_loss: 0.1629 - val_get_f1: 0.9545\n",
      "Epoch 23/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0398 - get_f1: 0.9840 - val_loss: 0.1706 - val_get_f1: 0.9496\n",
      "Epoch 24/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0380 - get_f1: 0.9842 - val_loss: 0.1850 - val_get_f1: 0.9525\n",
      "Epoch 25/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0372 - get_f1: 0.9864 - val_loss: 0.1848 - val_get_f1: 0.9474\n",
      "Epoch 26/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0343 - get_f1: 0.9862 - val_loss: 0.1877 - val_get_f1: 0.9511\n",
      "Epoch 27/50\n",
      "1547/1547 [==============================] - 15s 10ms/step - loss: 0.0316 - get_f1: 0.9873 - val_loss: 0.1855 - val_get_f1: 0.9525\n",
      "Epoch 28/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0338 - get_f1: 0.9873 - val_loss: 0.4521 - val_get_f1: 0.9367\n",
      "Epoch 29/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0365 - get_f1: 0.9875 - val_loss: 0.2389 - val_get_f1: 0.9477\n",
      "Epoch 30/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0291 - get_f1: 0.9890 - val_loss: 0.1787 - val_get_f1: 0.9543\n",
      "Epoch 31/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.0272 - get_f1: 0.9900 - val_loss: 0.1893 - val_get_f1: 0.9511\n",
      "Epoch 32/50\n",
      "1547/1547 [==============================] - 15s 9ms/step - loss: 0.0278 - get_f1: 0.9894 - val_loss: 0.2562 - val_get_f1: 0.9501\n",
      "Epoch 33/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0271 - get_f1: 0.9901 - val_loss: 0.2233 - val_get_f1: 0.9512\n",
      "Epoch 34/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0249 - get_f1: 0.9908 - val_loss: 0.2314 - val_get_f1: 0.9499\n",
      "Epoch 35/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0243 - get_f1: 0.9910 - val_loss: 0.2272 - val_get_f1: 0.9506\n",
      "Epoch 36/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0233 - get_f1: 0.9915 - val_loss: 0.2047 - val_get_f1: 0.9529\n",
      "Epoch 37/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0234 - get_f1: 0.9919 - val_loss: 0.2357 - val_get_f1: 0.9495\n",
      "Epoch 38/50\n",
      "1547/1547 [==============================] - 14s 9ms/step - loss: 0.0254 - get_f1: 0.9908 - val_loss: 0.2728 - val_get_f1: 0.9468\n",
      "Epoch 39/50\n",
      "1547/1547 [==============================] - 38s 24ms/step - loss: 0.0206 - get_f1: 0.9929 - val_loss: 0.1862 - val_get_f1: 0.9486\n",
      "Epoch 40/50\n",
      "1547/1547 [==============================] - 47s 31ms/step - loss: 0.0219 - get_f1: 0.9918 - val_loss: 0.2733 - val_get_f1: 0.9490\n",
      "Epoch 41/50\n",
      "1547/1547 [==============================] - 42s 27ms/step - loss: 0.0196 - get_f1: 0.9936 - val_loss: 0.3117 - val_get_f1: 0.9510\n",
      "Epoch 42/50\n",
      "1547/1547 [==============================] - 43s 28ms/step - loss: 0.0193 - get_f1: 0.9932 - val_loss: 0.2399 - val_get_f1: 0.9498\n",
      "Epoch 43/50\n",
      "1547/1547 [==============================] - 45s 29ms/step - loss: 0.0197 - get_f1: 0.9933 - val_loss: 0.2735 - val_get_f1: 0.9488\n",
      "Epoch 44/50\n",
      "1547/1547 [==============================] - 44s 28ms/step - loss: 0.0180 - get_f1: 0.9938 - val_loss: 0.2259 - val_get_f1: 0.9519\n",
      "Epoch 45/50\n",
      "1243/1547 [=======================>......] - ETA: 7s - loss: 0.0175 - get_f1: 0.9946"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=([get_f1]))\n",
    "model.fit( np.array(X_train) , np.array(Y_train).astype('float32') , epochs=50, batch_size=32, validation_split = .3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc470258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "te_df = model.predict(np.array(X_test),batch_size=16)\n",
    "y_pred = np.argmax(te_df, axis=1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "# plot confusion matrix as heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41834c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "327ca5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "# Define the custom object function\n",
    "def get_f1(y_true, y_pred):\n",
    "    # Custom F1 score calculation\n",
    "    # Define your custom F1 score implementation here\n",
    "    return f1_score\n",
    "\n",
    "# Load the model with the custom object function\n",
    "model = load_model('my_model.h5', custom_objects={'get_f1': get_f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db503db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "model.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256059c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class 1: 97.70%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for class 1\n",
    "f1_class1 = f1_score(y_true, y_pred, labels=[0], average='macro')\n",
    "print(\"F1 score for class 1: {:.2f}%\".format(f1_class1 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ef204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c206aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn10='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Not/*/'\n",
    "trn11='E:/USGS Labled Zebra/USGS Labled Zebra/Preserved Zebra D-Hinge 1 Baylor/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Not/*/'\n",
    "trn12='E:/USGS Labled Zebra/USGS Labled Zebra/Baylor Preserved Zebra Umbo 1a Image1/Baylor Preserved Zebra Umbo 1a Image1/Sorted Images/Not/*/'\n",
    "trn13='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Not/*/'\n",
    "trn14='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Not/*/'\n",
    "\n",
    "tr1= glob(trn10)\n",
    "tr= glob(trn11)\n",
    "tr1.extend(tr)\n",
    "tr= glob(trn12)\n",
    "tr1.extend(tr)\n",
    "tr= glob(trn13)\n",
    "tr1.extend(tr)\n",
    "tr= glob(trn14)\n",
    "tr1.extend(tr)\n",
    "\n",
    "\n",
    "\n",
    "trn20='E:/USGS Labled Zebra/USGS Labled Zebra/Preserved Zebra D-Hinge 1 Baylor/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Ostracod/*/'\n",
    "trn21='D:/data/Ostracod/Ostracod Day 2 Image12 Short To Baylor/Ostracod Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn22='D:/data/Ostracod/Ostracods Day 2 Image1 To Baylor/Ostracods Day 2 Image1 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn23='D:/data/Ostracod/Ostracods Day 2 Image2 To Baylor/Ostracods Day 2 Image2 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn24='D:/data/Ostracod/Ostracods Day 2 Image3 To Baylor/Ostracods Day 2 Image3 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn25='D:/data/Ostracod/Ostracods Day 2 Image12 To Baylor/Ostracods Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trn26='D:/data/Ostracod/Preserved Ostracods 1 To Baylor/Preserved Ostracods 1 To Baylor/Sorted Images/Preserve Ostracods/*/'\n",
    "trn27='D:/data/Ostracod/Preserved Ostracods 1a To Baylor/Preserved Ostracods 1a To Baylor/Sorted Images/Preserved Ostracods 1a/*/'\n",
    "\n",
    "tr2= glob(trn20)\n",
    "tr= glob(trn21)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn22)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn23)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn24)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn25)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn26)\n",
    "tr2.extend(tr)\n",
    "tr= glob(trn27)\n",
    "tr2.extend(tr)\n",
    "\n",
    "\n",
    "\n",
    "trn30='E:/USGS Labled Zebra/USGS Labled Zebra/Preserved Zebra D-Hinge 1 Baylor/Preserved Zebra D-Hinge 1 Baylor/Sorted Images/Zebra D-Hinge/*/'\n",
    "\n",
    "tr3= glob(trn30)\n",
    "\n",
    "\n",
    "trn40='E:/USGS Labled Zebra/USGS Labled Zebra/Baylor Preserved Zebra Umbo 1a Image1/Baylor Preserved Zebra Umbo 1a Image1/Sorted Images/Umbonal/*/'\n",
    "trn41='E:/USGS Labled Zebra/USGS Labled Zebra/Baylor Preserved Zebra Umbo 1 Image1/Baylor Preserved Zebra Umbo 1 Image1/Sorted Images/Umbonal/*/'\n",
    "\n",
    "tr4= glob(trn40)\n",
    "tr= glob(trn41)\n",
    "tr4.extend(tr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trn50='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Pedi-Zebra Veligers/*/'\n",
    "trn51='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Preserved Zebra Ped 1a/*/'\n",
    "\n",
    "tr5= glob(trn50)\n",
    "tr= glob(trn51)\n",
    "tr5.extend(tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71bbb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523d86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "tr1 = shuffle(tr1)\n",
    "tr2 = shuffle(tr2)\n",
    "tr3 = shuffle(tr3)\n",
    "tr4 = shuffle(tr4)\n",
    "tr5 = shuffle(tr5)\n",
    "\n",
    "tran_index_noninv = np.round( len(tr1)* .7  )\n",
    "tran_index_osc = np.round( len(tr2)* .7  )\n",
    "tran_index_dh = np.round( len(tr3)* .7  )\n",
    "tran_index_obm = np.round( len(tr4)* .7  )\n",
    "tran_index_pd = np.round( len(tr5)* .7  )\n",
    "tran_index_dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "\n",
    "for i in tr1[:(int) (tran_index_noninv)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "for i in tr2[:(int)(tran_index_osc)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "for i in tr3[:(int)(tran_index_dh)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(2)\n",
    "        \n",
    "for i in tr4[:(int)(tran_index_obm)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(3)\n",
    "        \n",
    "for i in tr5[:(int)(tran_index_pd)]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(4)\n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 30, 30)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(30,30,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),30,30,3))\n",
    "# One hot vector representation of labels\n",
    "Y_train = to_categorical (label)\n",
    "\n",
    "X_train,Y_train = shuffle(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d007b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "\n",
    "for i in tr1[(int) (tran_index_noninv) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(0)\n",
    "\n",
    "\n",
    "for i in tr2[(int)(tran_index_osc) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(1)\n",
    "\n",
    "\n",
    "for i in tr3[(int)(tran_index_dh) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(2)\n",
    "        \n",
    "for i in tr4[(int)(tran_index_obm) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(3)\n",
    "        \n",
    "for i in tr5[(int)(tran_index_pd) + 1 :]:\n",
    "    for j in glob(i+'/*'):\n",
    "        data.append(j)\n",
    "        label.append(3)\n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 30, 30)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(30,30,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test1 = idata\n",
    "X_test1 = X_test1.astype('float32') / 255.\n",
    "X_test1 = np.reshape(X_test1, (len(X_test1),30,30,3))\n",
    "# One hot vector representation of labels\n",
    "Y_test1 = to_categorical(label)\n",
    "\n",
    "X_test1,Y_test1 = shuffle(X_test1 , Y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c455bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5867"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d130800d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15013, 5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1fa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757452db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "\n",
    "# Remove the last layer\n",
    "new_model_layers = model.layers[:-1]\n",
    "\n",
    "# Add a new Dense layer for classification with 5 classes\n",
    "num_classes = 5\n",
    "new_output = Dense(num_classes, activation='softmax',name=\"final\")(new_model_layers[-1].output)\n",
    "\n",
    "# Create a new model with the modified architecture\n",
    "new_model = Model(inputs=model.input, outputs=new_output)\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=([get_f1]))\n",
    "new_model.fit( np.array(X_train) , np.array(Y_train).astype('float32') , epochs=50, batch_size=32, validation_split = .3, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "17732752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#new_model.save('new_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f903adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "# Define the custom object function\n",
    "def get_f1(y_true, y_pred):\n",
    "    # Custom F1 score calculation\n",
    "    # Define your custom F1 score implementation here\n",
    "    return f1_score\n",
    "\n",
    "# Load the model with the custom object function\n",
    "#new_model = load_model('new_model.h5', custom_objects={'get_f1': get_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5efc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for class 1\n",
    "f1_class1 = f1_score(y_true, y_pred, labels=[0], average='macro')\n",
    "print(\"F1 score for class 1: {:.2f}%\".format(f1_class1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d82344a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "te_df = new_model.predict(np.array(X_test),batch_size=16)\n",
    "y_pred = np.argmax(te_df, axis=1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "# plot confusion matrix as heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06519c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "te_df = new_model.predict(np.array(X_test),batch_size=16)\n",
    "y_pred = np.argmax(te_df, axis=1)\n",
    "y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "# plot confusion matrix as heatmap\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0136a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8d605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04013c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d38ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f0077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c27d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
