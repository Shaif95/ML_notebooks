{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acba090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa13c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf0822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeab214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a504b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2590.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn1='D:/INV/data/invasive-aquatic-species-data/invasive/*/'\n",
    "trn2='D:/INV/data/invasive-aquatic-species-data/noninvasive/*/'\n",
    "tr1= glob(trn1)\n",
    "tr2= glob(trn2)\n",
    "tr1= shuffle(tr1)\n",
    "tr2= shuffle(tr2)\n",
    "\n",
    "tran_index_inv = np.round( len(tr1)* .7 )\n",
    "tran_index_noninv = np.round( len(tr2)* .7  )\n",
    "tran_index_noninv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c51980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr1[:(int) (tran_index_inv)]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[:(int) (tran_index_noninv)]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr1[:(int) (tran_index_inv)])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr2[:(int) (tran_index_noninv)])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])        \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = a.resize((28, 28))\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),28,28,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "079c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3062, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "train_df= []\n",
    "breath = 6\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*6+k)\n",
    "        \n",
    "        deff.append(X_train[index])\n",
    "        \n",
    "    train_df.append(deff)\n",
    "\n",
    "Y_train = to_categorical(label)\n",
    "train_df = np.array(train_df)\n",
    "YY_Train = label\n",
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24d262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr1[(int) (tran_index_inv) + 1 :]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[ (int)(tran_index_noninv) + 1:]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr1[(int) (tran_index_inv) + 1 :])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr2[ (int)(tran_index_noninv) + 1:])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])        \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = a.resize((28, 28))\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),28,28,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661c2759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1310, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "test_df= []\n",
    "breath = 6\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*6+k)\n",
    "        \n",
    "        deff.append(X_test[index])\n",
    "        \n",
    "    test_df.append(deff)\n",
    "    \n",
    "Y_test = to_categorical(label)\n",
    "test_df = np.array(test_df)\n",
    "YY_Test = label\n",
    "np.shape(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56c507ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52b5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = ( 6, 28, 28, 3 )\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 32\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae32102",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88acaa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c674b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6123, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "31947b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc77872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 6, 32)        75488       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 6, 32)        64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 6, 32)        8416        layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 6, 32)        8320        layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 32)        0           multi_head_attention_4[0][0]     \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 6, 32)        64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 6, 32)        1056        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 32)        0           sequential_7[0][0]               \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 6, 32)        64          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 6, 32)        8416        layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 6, 32)        8320        layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 32)        0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 6, 32)        64          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 6, 32)        1056        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 32)        0           sequential_8[0][0]               \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 6, 32)        64          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 2)            66          global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 111,458\n",
      "Trainable params: 111,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c833aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_df = train_df.reshape( train_df.shape[0] , train_df.shape[1],( train_df.shape[2] * train_df.shape[3] * 3)  )\n",
    "tt_df = test_df.reshape(test_df.shape[0] ,test_df.shape[1],( test_df.shape[2] * test_df.shape[3] * 3)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e83af007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 26s 88ms/step - loss: 0.4420 - accuracy: 0.8138 - val_loss: 0.1964 - val_accuracy: 0.9413\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1924 - accuracy: 0.9249 - val_loss: 0.0976 - val_accuracy: 0.9527\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1753 - accuracy: 0.9269 - val_loss: 0.0625 - val_accuracy: 0.9674\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1690 - accuracy: 0.9302 - val_loss: 0.0467 - val_accuracy: 0.9821\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1641 - accuracy: 0.9290 - val_loss: 0.1929 - val_accuracy: 0.9168\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1613 - accuracy: 0.9339 - val_loss: 0.1284 - val_accuracy: 0.9413\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1447 - accuracy: 0.9396 - val_loss: 0.1251 - val_accuracy: 0.9560\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1379 - accuracy: 0.9473 - val_loss: 0.0865 - val_accuracy: 0.9690\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1416 - accuracy: 0.9424 - val_loss: 0.0652 - val_accuracy: 0.9755\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1539 - accuracy: 0.9355 - val_loss: 0.1722 - val_accuracy: 0.9217\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1305 - accuracy: 0.9461 - val_loss: 0.0674 - val_accuracy: 0.9739\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1224 - accuracy: 0.9486 - val_loss: 0.0871 - val_accuracy: 0.9657\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1228 - accuracy: 0.9502 - val_loss: 0.0885 - val_accuracy: 0.9641\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.1185 - accuracy: 0.9526 - val_loss: 0.1122 - val_accuracy: 0.9494\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1144 - accuracy: 0.9530 - val_loss: 0.0467 - val_accuracy: 0.9821\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1131 - accuracy: 0.9559 - val_loss: 0.0870 - val_accuracy: 0.9608\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1029 - accuracy: 0.9600 - val_loss: 0.0501 - val_accuracy: 0.9772\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1140 - accuracy: 0.9490 - val_loss: 0.1235 - val_accuracy: 0.9560\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1076 - accuracy: 0.9547 - val_loss: 0.0462 - val_accuracy: 0.9772\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0966 - accuracy: 0.9604 - val_loss: 0.1002 - val_accuracy: 0.9608\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0947 - accuracy: 0.9592 - val_loss: 0.1940 - val_accuracy: 0.9233\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0988 - accuracy: 0.9575 - val_loss: 0.1444 - val_accuracy: 0.9445\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0789 - accuracy: 0.9653 - val_loss: 0.1298 - val_accuracy: 0.9527\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0833 - accuracy: 0.9694 - val_loss: 0.0220 - val_accuracy: 0.9935\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0891 - accuracy: 0.9677 - val_loss: 0.1827 - val_accuracy: 0.9233\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0782 - accuracy: 0.9706 - val_loss: 0.0574 - val_accuracy: 0.9788\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0806 - accuracy: 0.9694 - val_loss: 0.0810 - val_accuracy: 0.9641\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0732 - accuracy: 0.9714 - val_loss: 0.0485 - val_accuracy: 0.9821\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0606 - accuracy: 0.9743 - val_loss: 0.1188 - val_accuracy: 0.9462\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0821 - accuracy: 0.9649 - val_loss: 0.2235 - val_accuracy: 0.9217\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0674 - accuracy: 0.9739 - val_loss: 0.0737 - val_accuracy: 0.9690\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0746 - accuracy: 0.9710 - val_loss: 0.1426 - val_accuracy: 0.9429\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0563 - accuracy: 0.9784 - val_loss: 0.0998 - val_accuracy: 0.9608\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0751 - accuracy: 0.9718 - val_loss: 0.0365 - val_accuracy: 0.9853\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0569 - accuracy: 0.9788 - val_loss: 0.1103 - val_accuracy: 0.9527\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0424 - accuracy: 0.9816 - val_loss: 0.2793 - val_accuracy: 0.9217\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0551 - accuracy: 0.9796 - val_loss: 0.1024 - val_accuracy: 0.9560\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0403 - accuracy: 0.9853 - val_loss: 0.2298 - val_accuracy: 0.9331\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0461 - accuracy: 0.9804 - val_loss: 0.3996 - val_accuracy: 0.9054\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0527 - accuracy: 0.9788 - val_loss: 0.1553 - val_accuracy: 0.9462\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0519 - accuracy: 0.9800 - val_loss: 0.1571 - val_accuracy: 0.9413\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0331 - accuracy: 0.9857 - val_loss: 0.1529 - val_accuracy: 0.9511\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0289 - accuracy: 0.9894 - val_loss: 0.1501 - val_accuracy: 0.9608\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0344 - accuracy: 0.9869 - val_loss: 0.2270 - val_accuracy: 0.9347\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0294 - accuracy: 0.9873 - val_loss: 0.1793 - val_accuracy: 0.9478\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0411 - accuracy: 0.9829 - val_loss: 0.2454 - val_accuracy: 0.9364\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0326 - accuracy: 0.9869 - val_loss: 0.1157 - val_accuracy: 0.9690\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0190 - accuracy: 0.9931 - val_loss: 0.1232 - val_accuracy: 0.9674\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0204 - accuracy: 0.9906 - val_loss: 0.1545 - val_accuracy: 0.9690\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0445 - accuracy: 0.9812 - val_loss: 0.3186 - val_accuracy: 0.9152\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.2099 - val_accuracy: 0.9494\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0196 - accuracy: 0.9931 - val_loss: 0.1146 - val_accuracy: 0.9739\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0194 - accuracy: 0.9927 - val_loss: 0.1775 - val_accuracy: 0.9576\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0475 - accuracy: 0.9824 - val_loss: 0.1095 - val_accuracy: 0.9560\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0286 - accuracy: 0.9910 - val_loss: 0.2133 - val_accuracy: 0.9478\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0373 - accuracy: 0.9865 - val_loss: 0.1046 - val_accuracy: 0.9706\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0220 - accuracy: 0.9918 - val_loss: 0.2567 - val_accuracy: 0.9445\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.2561 - val_accuracy: 0.9445\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0316 - accuracy: 0.9882 - val_loss: 0.2209 - val_accuracy: 0.9347\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0108 - accuracy: 0.9967 - val_loss: 0.1871 - val_accuracy: 0.9641\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0112 - accuracy: 0.9959 - val_loss: 0.1885 - val_accuracy: 0.9576\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.2658 - val_accuracy: 0.9462\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.1321 - val_accuracy: 0.9739\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0373 - accuracy: 0.9898 - val_loss: 0.1078 - val_accuracy: 0.9657\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0160 - accuracy: 0.9931 - val_loss: 0.2181 - val_accuracy: 0.9494\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0214 - accuracy: 0.9918 - val_loss: 0.2073 - val_accuracy: 0.9511\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0260 - accuracy: 0.9902 - val_loss: 0.0984 - val_accuracy: 0.9739\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0345 - accuracy: 0.9869 - val_loss: 0.1617 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0231 - accuracy: 0.9910 - val_loss: 0.2522 - val_accuracy: 0.9429\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0107 - accuracy: 0.9976 - val_loss: 0.2566 - val_accuracy: 0.9462\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.2621 - val_accuracy: 0.9527\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 0.3464 - val_accuracy: 0.9331\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0327 - accuracy: 0.9873 - val_loss: 0.0882 - val_accuracy: 0.9739\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0134 - accuracy: 0.9959 - val_loss: 0.2103 - val_accuracy: 0.9560\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0641 - val_accuracy: 0.9788\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0222 - accuracy: 0.9918 - val_loss: 0.1616 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.2612 - val_accuracy: 0.9543\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.3146 - val_accuracy: 0.9494\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.1782 - val_accuracy: 0.9690\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0223 - accuracy: 0.9931 - val_loss: 0.1287 - val_accuracy: 0.9674\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 0.1768 - val_accuracy: 0.9592\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0112 - accuracy: 0.9955 - val_loss: 0.1892 - val_accuracy: 0.9592\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.2927 - val_accuracy: 0.9445\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.2879 - val_accuracy: 0.9478\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 0.1989 - val_accuracy: 0.9543\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0232 - accuracy: 0.9898 - val_loss: 0.1595 - val_accuracy: 0.9657\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.3086 - val_accuracy: 0.9413\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 0.1613 - val_accuracy: 0.9657\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2165 - val_accuracy: 0.9625\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.1839 - val_accuracy: 0.9608\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0151 - accuracy: 0.9935 - val_loss: 0.2100 - val_accuracy: 0.9592\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0216 - accuracy: 0.9927 - val_loss: 0.1820 - val_accuracy: 0.9494\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0187 - accuracy: 0.9927 - val_loss: 0.1678 - val_accuracy: 0.9576\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0171 - accuracy: 0.9951 - val_loss: 0.2133 - val_accuracy: 0.9576\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.4369 - val_accuracy: 0.9299\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0276 - accuracy: 0.9931 - val_loss: 0.2509 - val_accuracy: 0.9445\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2267 - val_accuracy: 0.9560\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 8.8309e-04 - accuracy: 1.0000 - val_loss: 0.2360 - val_accuracy: 0.9560\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 6.3644e-04 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9527\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 3s 37ms/step - loss: 5.0884e-04 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20923c57e20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3abe3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1109,    0],\n",
       "       [   0,  201]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "786c9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87100053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_3 (PatchEncoder)  (None, 6, 32)        75488       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 6, 32)        64          patch_encoder_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 6, 32)        8416        layer_normalization_25[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 6, 32)        0           multi_head_attention_10[0][0]    \n",
      "                                                                 patch_encoder_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 6, 32)        64          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 6, 32)        1056        layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 32)        0           sequential_14[0][0]              \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 6, 32)        64          add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 6, 32)        8416        layer_normalization_27[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 6, 32)        0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 6, 32)        64          add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 6, 32)        1056        layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 6, 32)        0           sequential_15[0][0]              \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 6, 32)        64          add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 94,818\n",
      "Trainable params: 94,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 12s 47ms/step - loss: 0.2790 - accuracy: 0.8783 - val_loss: 0.0928 - val_accuracy: 0.9543\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1682 - accuracy: 0.9294 - val_loss: 0.2265 - val_accuracy: 0.8907\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1701 - accuracy: 0.9302 - val_loss: 0.0314 - val_accuracy: 0.9918\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1472 - accuracy: 0.9445 - val_loss: 0.1172 - val_accuracy: 0.9494\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1489 - accuracy: 0.9383 - val_loss: 0.1258 - val_accuracy: 0.9429\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1472 - accuracy: 0.9379 - val_loss: 0.2192 - val_accuracy: 0.9070\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1544 - accuracy: 0.9339 - val_loss: 0.1849 - val_accuracy: 0.9086\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1405 - accuracy: 0.9437 - val_loss: 0.1909 - val_accuracy: 0.9119\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1257 - accuracy: 0.9481 - val_loss: 0.0254 - val_accuracy: 0.9902\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1425 - accuracy: 0.9367 - val_loss: 0.0262 - val_accuracy: 0.9935\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1400 - accuracy: 0.9441 - val_loss: 0.1889 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1164 - accuracy: 0.9510 - val_loss: 0.1038 - val_accuracy: 0.9527\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1273 - accuracy: 0.9490 - val_loss: 0.0457 - val_accuracy: 0.9772\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.1330 - val_accuracy: 0.9445\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1025 - accuracy: 0.9563 - val_loss: 0.0641 - val_accuracy: 0.9674\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1129 - accuracy: 0.9514 - val_loss: 0.1119 - val_accuracy: 0.9527\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1048 - accuracy: 0.9604 - val_loss: 0.0834 - val_accuracy: 0.9592\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1052 - accuracy: 0.9551 - val_loss: 0.0691 - val_accuracy: 0.9674\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1115 - accuracy: 0.9535 - val_loss: 0.1969 - val_accuracy: 0.9184\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0936 - accuracy: 0.9641 - val_loss: 0.0389 - val_accuracy: 0.9804\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0909 - accuracy: 0.9645 - val_loss: 0.2659 - val_accuracy: 0.9054\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0863 - accuracy: 0.9645 - val_loss: 0.1013 - val_accuracy: 0.9592\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 35ms/step - loss: 0.0809 - accuracy: 0.9698 - val_loss: 0.1611 - val_accuracy: 0.9494\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0750 - accuracy: 0.9694 - val_loss: 0.0934 - val_accuracy: 0.9657\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0820 - accuracy: 0.9694 - val_loss: 0.1012 - val_accuracy: 0.9592\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0690 - accuracy: 0.9735 - val_loss: 0.1078 - val_accuracy: 0.9641\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0729 - accuracy: 0.9710 - val_loss: 0.0679 - val_accuracy: 0.9755\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0737 - accuracy: 0.9702 - val_loss: 0.1705 - val_accuracy: 0.9478\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0576 - accuracy: 0.9784 - val_loss: 0.1849 - val_accuracy: 0.9445\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0729 - accuracy: 0.9702 - val_loss: 0.1034 - val_accuracy: 0.9608\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0610 - accuracy: 0.9767 - val_loss: 0.0533 - val_accuracy: 0.9837\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0546 - accuracy: 0.9784 - val_loss: 0.1672 - val_accuracy: 0.9576\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0552 - accuracy: 0.9759 - val_loss: 0.2031 - val_accuracy: 0.9429\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0543 - accuracy: 0.9792 - val_loss: 0.0950 - val_accuracy: 0.9657\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0763 - accuracy: 0.9702 - val_loss: 0.1132 - val_accuracy: 0.9657\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0512 - accuracy: 0.9784 - val_loss: 0.1733 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0503 - accuracy: 0.9784 - val_loss: 0.0733 - val_accuracy: 0.9739\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0430 - accuracy: 0.9837 - val_loss: 0.1604 - val_accuracy: 0.9543\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0402 - accuracy: 0.9829 - val_loss: 0.2186 - val_accuracy: 0.9478\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0656 - accuracy: 0.9747 - val_loss: 0.0952 - val_accuracy: 0.9527\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0471 - accuracy: 0.9808 - val_loss: 0.0967 - val_accuracy: 0.9723\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0544 - accuracy: 0.9792 - val_loss: 0.2420 - val_accuracy: 0.9462\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0475 - accuracy: 0.9800 - val_loss: 0.1309 - val_accuracy: 0.9592\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0377 - accuracy: 0.9861 - val_loss: 0.1681 - val_accuracy: 0.9576\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.1371 - val_accuracy: 0.9608\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0247 - accuracy: 0.9902 - val_loss: 0.1664 - val_accuracy: 0.9560\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0281 - accuracy: 0.9894 - val_loss: 0.2423 - val_accuracy: 0.9462\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0458 - accuracy: 0.9816 - val_loss: 0.1290 - val_accuracy: 0.9592\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.1359 - val_accuracy: 0.9592\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0167 - accuracy: 0.9959 - val_loss: 0.2052 - val_accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0403 - accuracy: 0.9869 - val_loss: 0.1929 - val_accuracy: 0.9511\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.2314 - val_accuracy: 0.9527\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0373 - accuracy: 0.9873 - val_loss: 0.2695 - val_accuracy: 0.9364\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.3173 - val_accuracy: 0.9462\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0185 - accuracy: 0.9931 - val_loss: 0.2376 - val_accuracy: 0.9396\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0295 - accuracy: 0.9882 - val_loss: 0.1306 - val_accuracy: 0.9706\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0337 - accuracy: 0.9861 - val_loss: 0.1834 - val_accuracy: 0.9560\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0169 - accuracy: 0.9935 - val_loss: 0.3142 - val_accuracy: 0.9429\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0264 - accuracy: 0.9902 - val_loss: 0.2932 - val_accuracy: 0.9494\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.1400 - val_accuracy: 0.9608\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.1906 - val_accuracy: 0.9543\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0145 - accuracy: 0.9959 - val_loss: 0.1673 - val_accuracy: 0.9592\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.2367 - val_accuracy: 0.9527\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.3590 - val_accuracy: 0.9364\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0279 - accuracy: 0.9890 - val_loss: 0.1551 - val_accuracy: 0.9608\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0436 - accuracy: 0.9841 - val_loss: 0.2427 - val_accuracy: 0.9527\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.3569 - val_accuracy: 0.9429\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0091 - accuracy: 0.9984 - val_loss: 0.3479 - val_accuracy: 0.9478\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.2849 - val_accuracy: 0.9380\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0267 - accuracy: 0.9902 - val_loss: 0.0687 - val_accuracy: 0.9723\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0372 - accuracy: 0.9845 - val_loss: 0.1943 - val_accuracy: 0.9576\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.2390 - val_accuracy: 0.9543\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 3s 33ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.3710 - val_accuracy: 0.9413\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.3887 - val_accuracy: 0.9135\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.2744 - val_accuracy: 0.9462\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.3364 - val_accuracy: 0.9462\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0290 - accuracy: 0.9918 - val_loss: 0.3349 - val_accuracy: 0.9331\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 0.3601 - val_accuracy: 0.9380\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.1759 - val_accuracy: 0.9592\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.3238 - val_accuracy: 0.9380\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.3573 - val_accuracy: 0.9413\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0213 - accuracy: 0.9914 - val_loss: 0.4856 - val_accuracy: 0.9021\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.5060 - val_accuracy: 0.9201\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0541 - accuracy: 0.9796 - val_loss: 0.2363 - val_accuracy: 0.9413\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0446 - accuracy: 0.9837 - val_loss: 0.2589 - val_accuracy: 0.9478\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 0.2900 - val_accuracy: 0.9347\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.2186 - val_accuracy: 0.9511\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.1806 - val_accuracy: 0.9657\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2190 - val_accuracy: 0.9608\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0083 - accuracy: 0.9963 - val_loss: 0.2750 - val_accuracy: 0.9527\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0174 - accuracy: 0.9931 - val_loss: 0.4152 - val_accuracy: 0.9250\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0560 - accuracy: 0.9784 - val_loss: 0.2111 - val_accuracy: 0.9511\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0107 - accuracy: 0.9959 - val_loss: 0.1770 - val_accuracy: 0.9576\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.2556 - val_accuracy: 0.9527\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0142 - accuracy: 0.9943 - val_loss: 0.1129 - val_accuracy: 0.9723\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.2598 - val_accuracy: 0.9560\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.2788 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.3667 - val_accuracy: 0.9511\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.4830 - val_accuracy: 0.9233\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0139 - accuracy: 0.9947 - val_loss: 0.2799 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2090ca48f70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7ea818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995049504950495 0.999096657633243 0.9901477832512315\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dd475d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 6, 32)        75488       input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 6, 32)        64          patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 6, 32)        8320        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 6, 32)        0           lstm_7[0][0]                     \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 6, 32)        64          add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 6, 32)        1056        layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 6, 32)        0           sequential_18[0][0]              \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 6, 32)        64          add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 6, 32)        8320        layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 6, 32)        0           lstm_8[0][0]                     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 6, 32)        64          add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 6, 32)        1056        layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 6, 32)        0           sequential_19[0][0]              \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 6, 32)        64          add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 2)            66          global_average_pooling1d_6[0][0] \n",
      "==================================================================================================\n",
      "Total params: 94,626\n",
      "Trainable params: 94,626\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 25s 93ms/step - loss: 0.4303 - accuracy: 0.8191 - val_loss: 0.1449 - val_accuracy: 0.9706\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1954 - accuracy: 0.9171 - val_loss: 0.0553 - val_accuracy: 0.9821\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1754 - accuracy: 0.9285 - val_loss: 0.0516 - val_accuracy: 0.9821\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1594 - accuracy: 0.9343 - val_loss: 0.2064 - val_accuracy: 0.9168\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1491 - accuracy: 0.9392 - val_loss: 0.1056 - val_accuracy: 0.9511\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1456 - accuracy: 0.9375 - val_loss: 0.0218 - val_accuracy: 0.9951\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1490 - accuracy: 0.9371 - val_loss: 0.0531 - val_accuracy: 0.9788\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1475 - accuracy: 0.9408 - val_loss: 0.0332 - val_accuracy: 0.9853\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1487 - accuracy: 0.9392 - val_loss: 0.1693 - val_accuracy: 0.9266\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1248 - accuracy: 0.9477 - val_loss: 0.1424 - val_accuracy: 0.9380\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1277 - accuracy: 0.9461 - val_loss: 0.1451 - val_accuracy: 0.9413\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1142 - accuracy: 0.9506 - val_loss: 0.0339 - val_accuracy: 0.9853\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1391 - accuracy: 0.9432 - val_loss: 0.1257 - val_accuracy: 0.9445\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1160 - accuracy: 0.9522 - val_loss: 0.0984 - val_accuracy: 0.9543\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1132 - accuracy: 0.9510 - val_loss: 0.1452 - val_accuracy: 0.9396\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.1226 - accuracy: 0.9502 - val_loss: 0.0661 - val_accuracy: 0.9755\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.1126 - accuracy: 0.9551 - val_loss: 0.0840 - val_accuracy: 0.9592\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.1165 - accuracy: 0.9535 - val_loss: 0.1038 - val_accuracy: 0.9494\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1182 - accuracy: 0.9526 - val_loss: 0.1699 - val_accuracy: 0.9266\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1075 - accuracy: 0.9551 - val_loss: 0.1835 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1000 - accuracy: 0.9575 - val_loss: 0.0616 - val_accuracy: 0.9804\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1021 - accuracy: 0.9547 - val_loss: 0.1200 - val_accuracy: 0.9445\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1027 - accuracy: 0.9530 - val_loss: 0.0669 - val_accuracy: 0.9772\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1020 - accuracy: 0.9588 - val_loss: 0.0452 - val_accuracy: 0.9837\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0930 - accuracy: 0.9624 - val_loss: 0.0343 - val_accuracy: 0.9869\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0931 - accuracy: 0.9628 - val_loss: 0.1138 - val_accuracy: 0.9543\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0883 - accuracy: 0.9665 - val_loss: 0.0796 - val_accuracy: 0.9755\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0987 - accuracy: 0.9600 - val_loss: 0.0354 - val_accuracy: 0.9853\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1084 - accuracy: 0.9530 - val_loss: 0.2048 - val_accuracy: 0.9184\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0937 - accuracy: 0.9628 - val_loss: 0.1353 - val_accuracy: 0.9429\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0859 - accuracy: 0.9620 - val_loss: 0.1486 - val_accuracy: 0.9331\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0841 - accuracy: 0.9673 - val_loss: 0.0675 - val_accuracy: 0.9739\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0740 - accuracy: 0.9690 - val_loss: 0.1191 - val_accuracy: 0.9511\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0805 - accuracy: 0.9682 - val_loss: 0.1474 - val_accuracy: 0.9331\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0787 - accuracy: 0.9673 - val_loss: 0.1581 - val_accuracy: 0.9413\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.0754 - accuracy: 0.9686 - val_loss: 0.0677 - val_accuracy: 0.9755\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0722 - accuracy: 0.9726 - val_loss: 0.0839 - val_accuracy: 0.9739\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0806 - accuracy: 0.9673 - val_loss: 0.0849 - val_accuracy: 0.9641\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0682 - accuracy: 0.9731 - val_loss: 0.1809 - val_accuracy: 0.9347\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0752 - accuracy: 0.9722 - val_loss: 0.0863 - val_accuracy: 0.9608\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0562 - accuracy: 0.9796 - val_loss: 0.0728 - val_accuracy: 0.9739\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0592 - accuracy: 0.9780 - val_loss: 0.2071 - val_accuracy: 0.9184\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0532 - accuracy: 0.9775 - val_loss: 0.0765 - val_accuracy: 0.9804\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0686 - accuracy: 0.9726 - val_loss: 0.0584 - val_accuracy: 0.9821\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0523 - accuracy: 0.9808 - val_loss: 0.1186 - val_accuracy: 0.9478\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0464 - accuracy: 0.9837 - val_loss: 0.0852 - val_accuracy: 0.9772\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0549 - accuracy: 0.9775 - val_loss: 0.0665 - val_accuracy: 0.9739\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0473 - accuracy: 0.9808 - val_loss: 0.1366 - val_accuracy: 0.9527\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0488 - accuracy: 0.9820 - val_loss: 0.1204 - val_accuracy: 0.9429\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0408 - accuracy: 0.9849 - val_loss: 0.2290 - val_accuracy: 0.9299\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0516 - accuracy: 0.9829 - val_loss: 0.0634 - val_accuracy: 0.9804\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0461 - accuracy: 0.9833 - val_loss: 0.0373 - val_accuracy: 0.9886\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.0562 - val_accuracy: 0.9853\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0312 - accuracy: 0.9886 - val_loss: 0.0937 - val_accuracy: 0.9739\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0366 - accuracy: 0.9857 - val_loss: 0.0464 - val_accuracy: 0.9821\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0375 - accuracy: 0.9845 - val_loss: 0.1089 - val_accuracy: 0.9657\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0329 - accuracy: 0.9865 - val_loss: 0.0884 - val_accuracy: 0.9641\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0391 - accuracy: 0.9861 - val_loss: 0.1290 - val_accuracy: 0.9625\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0488 - accuracy: 0.9800 - val_loss: 0.1248 - val_accuracy: 0.9560\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.1069 - val_accuracy: 0.9690\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0272 - accuracy: 0.9894 - val_loss: 0.1204 - val_accuracy: 0.9608\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0310 - accuracy: 0.9853 - val_loss: 0.1229 - val_accuracy: 0.9657\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0356 - accuracy: 0.9845 - val_loss: 0.2249 - val_accuracy: 0.9315\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0513 - accuracy: 0.9792 - val_loss: 0.1057 - val_accuracy: 0.9657\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0183 - accuracy: 0.9943 - val_loss: 0.0971 - val_accuracy: 0.9739\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.0157 - accuracy: 0.9943 - val_loss: 0.0844 - val_accuracy: 0.9723\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0233 - accuracy: 0.9914 - val_loss: 0.0358 - val_accuracy: 0.9853\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0639 - accuracy: 0.9739 - val_loss: 0.0887 - val_accuracy: 0.9690\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0248 - accuracy: 0.9927 - val_loss: 0.0628 - val_accuracy: 0.9853\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0153 - accuracy: 0.9967 - val_loss: 0.1201 - val_accuracy: 0.9608\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0122 - accuracy: 0.9959 - val_loss: 0.1612 - val_accuracy: 0.9527\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.1269 - val_accuracy: 0.9592\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.0830 - val_accuracy: 0.9755\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0159 - accuracy: 0.9943 - val_loss: 0.1315 - val_accuracy: 0.9690\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.2061 - val_accuracy: 0.9543\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.1099 - val_accuracy: 0.9690\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.0920 - val_accuracy: 0.9739\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0344 - accuracy: 0.9857 - val_loss: 0.3520 - val_accuracy: 0.9038\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0483 - accuracy: 0.9812 - val_loss: 0.0835 - val_accuracy: 0.9788\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0376 - accuracy: 0.9849 - val_loss: 0.0986 - val_accuracy: 0.9723\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.1295 - val_accuracy: 0.9657\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0139 - accuracy: 0.9943 - val_loss: 0.1286 - val_accuracy: 0.9690\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.1071 - val_accuracy: 0.9674\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.1218 - val_accuracy: 0.9706\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0910 - val_accuracy: 0.9739\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.1781 - val_accuracy: 0.9478\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0141 - accuracy: 0.9939 - val_loss: 0.0838 - val_accuracy: 0.9804\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0183 - accuracy: 0.9955 - val_loss: 0.1206 - val_accuracy: 0.9690\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0145 - accuracy: 0.9963 - val_loss: 0.1257 - val_accuracy: 0.9657\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.1035 - val_accuracy: 0.9772\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.1087 - val_accuracy: 0.9755\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0040 - accuracy: 0.9996 - val_loss: 0.1695 - val_accuracy: 0.9625\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0364 - accuracy: 0.9849 - val_loss: 0.0589 - val_accuracy: 0.9772\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0293 - accuracy: 0.9890 - val_loss: 0.1559 - val_accuracy: 0.9527\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0107 - accuracy: 0.9984 - val_loss: 0.1661 - val_accuracy: 0.9543\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0101 - accuracy: 0.9967 - val_loss: 0.1546 - val_accuracy: 0.9674\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0423 - accuracy: 0.9849 - val_loss: 0.1447 - val_accuracy: 0.9511\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0180 - accuracy: 0.9939 - val_loss: 0.1294 - val_accuracy: 0.9608\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 0.1187 - val_accuracy: 0.9674\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.1092 - val_accuracy: 0.9739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20910438f10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output =LSTM(32,return_sequences=True,dropout=0.1)(x1)  \n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c6c53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9975062344139651 0.9975 1.0\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a6163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26057200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_1 (PatchEncoder)  (None, 6, 32)        11096       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 6, 32)        64          patch_encoder_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 6, 32)        8416        layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 6, 32)        8320        layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 6, 32)        0           multi_head_attention_2[0][0]     \n",
      "                                                                 patch_encoder_1[0][0]            \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 6, 32)        64          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 6, 32)        1056        layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 6, 32)        0           sequential_4[0][0]               \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 6, 32)        64          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 6, 32)        8416        layer_normalization_7[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 6, 32)        8320        layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 6, 32)        0           multi_head_attention_3[0][0]     \n",
      "                                                                 add_5[0][0]                      \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 6, 32)        64          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 6, 32)        1056        layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 6, 32)        0           sequential_5[0][0]               \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 6, 32)        64          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 2)            66          global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 47,066\n",
      "Trainable params: 47,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2676f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 51s 210ms/step - loss: 0.3287 - accuracy: 0.8583 - val_loss: 0.0711 - val_accuracy: 0.9804\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 0.2162 - accuracy: 0.9118 - val_loss: 0.2419 - val_accuracy: 0.8711\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.1743 - accuracy: 0.9318 - val_loss: 0.0891 - val_accuracy: 0.9674\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.1629 - accuracy: 0.9326 - val_loss: 0.0384 - val_accuracy: 0.9869\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 0.1415 - accuracy: 0.9449 - val_loss: 0.0920 - val_accuracy: 0.9641\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.1400 - accuracy: 0.9473 - val_loss: 0.0806 - val_accuracy: 0.9641\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.1264 - accuracy: 0.9486 - val_loss: 0.1229 - val_accuracy: 0.9527\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.1019 - accuracy: 0.9633 - val_loss: 0.0737 - val_accuracy: 0.9706\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.1076 - accuracy: 0.9579 - val_loss: 0.1255 - val_accuracy: 0.9511\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0876 - accuracy: 0.9649 - val_loss: 0.1674 - val_accuracy: 0.9445\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0810 - accuracy: 0.9669 - val_loss: 0.1649 - val_accuracy: 0.9413\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0779 - accuracy: 0.9661 - val_loss: 0.0588 - val_accuracy: 0.9755\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.0797 - accuracy: 0.9686 - val_loss: 0.0842 - val_accuracy: 0.9674\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.0626 - accuracy: 0.9759 - val_loss: 0.1307 - val_accuracy: 0.9511\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0585 - accuracy: 0.9767 - val_loss: 0.1063 - val_accuracy: 0.9560\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.0601 - accuracy: 0.9731 - val_loss: 0.0690 - val_accuracy: 0.9706\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 0.0544 - accuracy: 0.9796 - val_loss: 0.0722 - val_accuracy: 0.9755\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 0.0494 - accuracy: 0.9829 - val_loss: 0.1538 - val_accuracy: 0.9396\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.0588 - accuracy: 0.9747 - val_loss: 0.2532 - val_accuracy: 0.9201\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0286 - accuracy: 0.9886 - val_loss: 0.1534 - val_accuracy: 0.9560\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 7s 94ms/step - loss: 0.0512 - accuracy: 0.9820 - val_loss: 0.0858 - val_accuracy: 0.9690\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0458 - accuracy: 0.9804 - val_loss: 0.0691 - val_accuracy: 0.9755\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.1201 - val_accuracy: 0.9674\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0343 - accuracy: 0.9845 - val_loss: 0.2035 - val_accuracy: 0.9396\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0361 - accuracy: 0.9886 - val_loss: 0.0494 - val_accuracy: 0.9804\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 0.3650 - val_accuracy: 0.8891\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0377 - accuracy: 0.9865 - val_loss: 0.0611 - val_accuracy: 0.9706\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.1712 - val_accuracy: 0.9576\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0380 - accuracy: 0.9865 - val_loss: 0.1456 - val_accuracy: 0.9445\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.0242 - accuracy: 0.9902 - val_loss: 0.1293 - val_accuracy: 0.9657\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0137 - accuracy: 0.9943 - val_loss: 0.1796 - val_accuracy: 0.9576\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 0.0166 - accuracy: 0.9922 - val_loss: 0.1360 - val_accuracy: 0.9657\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.0603 - val_accuracy: 0.9837\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 0.0389 - accuracy: 0.9808 - val_loss: 0.0926 - val_accuracy: 0.9723\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.1292 - val_accuracy: 0.9674\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.0843 - val_accuracy: 0.9788\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0324 - accuracy: 0.9886 - val_loss: 0.1650 - val_accuracy: 0.9576\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.0182 - accuracy: 0.9931 - val_loss: 0.0889 - val_accuracy: 0.9706\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.1309 - val_accuracy: 0.9706\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9690\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 5.6323e-04 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9641\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 7s 94ms/step - loss: 3.0760e-04 - accuracy: 1.0000 - val_loss: 0.1833 - val_accuracy: 0.9657\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 2.6813e-04 - accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9657\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 3.9911e-04 - accuracy: 1.0000 - val_loss: 0.2598 - val_accuracy: 0.9592\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2454 - val_accuracy: 0.9576\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 10s 133ms/step - loss: 0.0370 - accuracy: 0.9869 - val_loss: 0.2882 - val_accuracy: 0.9380\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 9s 115ms/step - loss: 0.0221 - accuracy: 0.9902 - val_loss: 0.1249 - val_accuracy: 0.9657\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 12s 154ms/step - loss: 0.0136 - accuracy: 0.9943 - val_loss: 0.1671 - val_accuracy: 0.9608\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.0227 - val_accuracy: 0.9918\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0272 - accuracy: 0.9898 - val_loss: 0.2266 - val_accuracy: 0.9494\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.1633 - val_accuracy: 0.9657\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.1694 - val_accuracy: 0.9641\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.1088 - val_accuracy: 0.9723\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0235 - accuracy: 0.9931 - val_loss: 0.1114 - val_accuracy: 0.9706\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.1839 - val_accuracy: 0.9592\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 9s 113ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.1890 - val_accuracy: 0.9674\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.1415 - val_accuracy: 0.9706\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 11s 139ms/step - loss: 3.4898e-04 - accuracy: 1.0000 - val_loss: 0.1863 - val_accuracy: 0.9674\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 9s 119ms/step - loss: 0.0145 - accuracy: 0.9943 - val_loss: 0.3640 - val_accuracy: 0.9364\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0216 - accuracy: 0.9918 - val_loss: 0.1234 - val_accuracy: 0.9739\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0917 - val_accuracy: 0.9804\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 0.0221 - accuracy: 0.9918 - val_loss: 0.0419 - val_accuracy: 0.9869\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 9s 119ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.1220 - val_accuracy: 0.9739\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9772\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 8s 109ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.2812 - val_accuracy: 0.9315\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.0351 - accuracy: 0.9894 - val_loss: 0.1902 - val_accuracy: 0.9527\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 8s 104ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.1994 - val_accuracy: 0.9608\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 8s 107ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9690\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 3.3627e-04 - accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9674\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 5.1611e-04 - accuracy: 1.0000 - val_loss: 0.1819 - val_accuracy: 0.9706\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 2.9474e-04 - accuracy: 1.0000 - val_loss: 0.1737 - val_accuracy: 0.9706\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 1.4657e-04 - accuracy: 1.0000 - val_loss: 0.1984 - val_accuracy: 0.9674\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 2.8352e-04 - accuracy: 1.0000 - val_loss: 0.1749 - val_accuracy: 0.9723\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 1.1157e-04 - accuracy: 1.0000 - val_loss: 0.1868 - val_accuracy: 0.9674\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 1.4871e-04 - accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9739\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 8.8516e-05 - accuracy: 1.0000 - val_loss: 0.1881 - val_accuracy: 0.9690\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 9.0661e-05 - accuracy: 1.0000 - val_loss: 0.2020 - val_accuracy: 0.9674\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 1.1914e-04 - accuracy: 1.0000 - val_loss: 0.2211 - val_accuracy: 0.9657\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 7.2163e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9674s\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 5.0619e-05 - accuracy: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9674\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 9s 114ms/step - loss: 5.7123e-05 - accuracy: 1.0000 - val_loss: 0.1801 - val_accuracy: 0.9706\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 5.2668e-05 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9706\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 5.5588e-05 - accuracy: 1.0000 - val_loss: 0.1759 - val_accuracy: 0.9755\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 3.7607e-05 - accuracy: 1.0000 - val_loss: 0.1925 - val_accuracy: 0.9674\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 9s 115ms/step - loss: 3.5284e-05 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 0.9674\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 2.7841e-05 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9690\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 2.3525e-05 - accuracy: 1.0000 - val_loss: 0.2035 - val_accuracy: 0.9690\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 3.3023e-05 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.9674\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 11s 137ms/step - loss: 3.1773e-05 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9723\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 2.5920e-05 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9723\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 2.3962e-05 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9739\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0874 - accuracy: 0.9682 - val_loss: 0.1453 - val_accuracy: 0.9445\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 9s 114ms/step - loss: 0.0378 - accuracy: 0.9845 - val_loss: 0.0873 - val_accuracy: 0.9723\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 0.0173 - accuracy: 0.9927 - val_loss: 0.2435 - val_accuracy: 0.9543\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 8s 108ms/step - loss: 0.0212 - accuracy: 0.9939 - val_loss: 0.1989 - val_accuracy: 0.9511\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.1821 - val_accuracy: 0.9657\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 9s 113ms/step - loss: 0.0471 - accuracy: 0.9865 - val_loss: 0.1044 - val_accuracy: 0.9706\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 9s 113ms/step - loss: 0.0219 - accuracy: 0.9918 - val_loss: 0.1702 - val_accuracy: 0.9657\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 9s 111ms/step - loss: 0.0265 - accuracy: 0.9902 - val_loss: 0.1002 - val_accuracy: 0.9723\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 9s 112ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.1574 - val_accuracy: 0.9657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x209001e7e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef7bac82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1109,    0],\n",
       "       [   0,  201]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b4d89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c36be08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514281067975519"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3afd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cf3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( train_df,YY_Train , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cc7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(X_train , y_train , \"train\")\n",
    "validloader = prepare_dataloader(X_val, y_val, \"valid\")\n",
    "testloader = prepare_dataloader(test_df,YY_Test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0523831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6b82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28803bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tubelet_embedding (TubeletEmbed (None, 9, 32)        24608       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoder (PositionalE (None, 9, 32)        288         tubelet_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 9, 32)        64          positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 9, 32)        4224        layer_normalization_15[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 9, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 9, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 9, 32)        8352        layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 9, 32)        0           sequential_9[0][0]               \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 9, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 9, 32)        4224        layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 9, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 9, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 9, 32)        8352        layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 9, 32)        0           sequential_10[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 9, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            66          global_average_pooling1d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 50,434\n",
      "Trainable params: 50,434\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "PROJECTION_DIM = 32\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf326e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa5b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 20s 78ms/step - loss: 0.3926 - accuracy: 0.8436 - top-5-accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.8254 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.2927 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.8418 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2794 - accuracy: 0.8653 - top-5-accuracy: 1.0000 - val_loss: 0.2698 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2298 - accuracy: 0.8881 - top-5-accuracy: 1.0000 - val_loss: 0.2940 - val_accuracy: 0.8499 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.2393 - accuracy: 0.8877 - top-5-accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.8760 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.2201 - accuracy: 0.8918 - top-5-accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.2056 - accuracy: 0.9028 - top-5-accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.8711 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1891 - accuracy: 0.9110 - top-5-accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1907 - accuracy: 0.9138 - top-5-accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.8989 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1848 - accuracy: 0.9167 - top-5-accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1675 - accuracy: 0.9261 - top-5-accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1725 - accuracy: 0.9306 - top-5-accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1630 - accuracy: 0.9290 - top-5-accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1800 - accuracy: 0.9220 - top-5-accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1566 - accuracy: 0.9355 - top-5-accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1476 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1507 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1488 - accuracy: 0.9367 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1385 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1394 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1332 - accuracy: 0.9453 - top-5-accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1340 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1256 - accuracy: 0.9490 - top-5-accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1275 - accuracy: 0.9449 - top-5-accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1226 - accuracy: 0.9502 - top-5-accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1257 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1256 - accuracy: 0.9461 - top-5-accuracy: 1.0000 - val_loss: 0.1774 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1153 - accuracy: 0.9539 - top-5-accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1192 - accuracy: 0.9510 - top-5-accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1188 - accuracy: 0.9535 - top-5-accuracy: 1.0000 - val_loss: 0.1698 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1002 - accuracy: 0.9575 - top-5-accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0950 - accuracy: 0.9645 - top-5-accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1041 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1014 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0927 - accuracy: 0.9624 - top-5-accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0882 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0882 - accuracy: 0.9628 - top-5-accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0879 - accuracy: 0.9600 - top-5-accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0828 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0872 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0748 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0807 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0703 - accuracy: 0.9686 - top-5-accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0779 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.0776 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.0707 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0759 - accuracy: 0.9690 - top-5-accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0656 - accuracy: 0.9735 - top-5-accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0671 - accuracy: 0.9714 - top-5-accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0598 - accuracy: 0.9755 - top-5-accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9527 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0708 - accuracy: 0.9698 - top-5-accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.0569 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0569 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0515 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.1930 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0562 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1689 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0487 - accuracy: 0.9820 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0623 - accuracy: 0.9751 - top-5-accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0609 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0538 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0446 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0544 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0602 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0460 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0383 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0504 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0452 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0495 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0388 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.1932 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0435 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0436 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0387 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0437 - accuracy: 0.9824 - top-5-accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0354 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0331 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0396 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.2242 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0461 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0346 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0306 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0453 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0390 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0405 - accuracy: 0.9837 - top-5-accuracy: 1.0000 - val_loss: 0.2045 - val_accuracy: 0.9494 - val_top-5-accuracy: 1.0000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0337 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0243 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0212 - accuracy: 0.9947 - top-5-accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0521 - accuracy: 0.9808 - top-5-accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0250 - accuracy: 0.9939 - top-5-accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.0295 - accuracy: 0.9890 - top-5-accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 5s 64ms/step - loss: 0.0298 - accuracy: 0.9882 - top-5-accuracy: 1.0000 - val_loss: 0.2464 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0415 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0401 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0265 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0365 - accuracy: 0.9869 - top-5-accuracy: 1.0000 - val_loss: 0.3200 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0349 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0632 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.2011 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0355 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.2601 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0273 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0286 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0280 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0741 - accuracy: 0.9794 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 97.94%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c6df61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1104,   22],\n",
       "       [   5,  179]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY_Test = YY_Test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = YY_Test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7d70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298701298701298 0.9362829932798964 0.9728260869565217\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3620fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398567119155354"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f75542c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "199466a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train_df[19]\n",
    "y = train_df[3]\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [x[0], x[1], x[2], x[3],y[0], y[1], y[2], y[3]]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3eb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027188ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea8a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 6, 28, 28, 64)     5248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 6, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 6, 14, 14, 128)    221312    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 3, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 3, 7, 7, 256)      884992    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 1, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 1, 3, 3, 512)      3539456   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 9,372,674\n",
      "Trainable params: 9,372,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3, 3, 3), activation=\"relu\",name=\"conv1\",   input_shape=(6,28,28,3), strides=(1, 1, 1), padding=\"same\"))  \n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
    "model.add(Conv3D(128, (3, 3, 3), activation=\"relu\",name=\"conv2\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
    "model.add(Conv3D(256, (3, 3, 3), activation=\"relu\",name=\"conv3a\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
    "model.add(Conv3D(512, (3, 3, 3), activation=\"relu\",name=\"conv4a\", strides=(1, 1, 1), padding=\"same\"))   \n",
    "\n",
    "model.add(Flatten())\n",
    "                     \n",
    "    # FC layers group\n",
    "model.add(Dense(1024, activation='relu', name='fc6'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax', name='fc8'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d1b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 357s 5s/step - loss: 0.3074 - accuracy: 0.8591 - val_loss: 0.1795 - val_accuracy: 0.9184\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 356s 5s/step - loss: 0.2036 - accuracy: 0.9224 - val_loss: 0.0635 - val_accuracy: 0.9723\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 338s 4s/step - loss: 0.1969 - accuracy: 0.9216 - val_loss: 0.0981 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 360s 5s/step - loss: 0.1657 - accuracy: 0.9306 - val_loss: 0.0700 - val_accuracy: 0.9674\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 358s 5s/step - loss: 0.1690 - accuracy: 0.9310 - val_loss: 0.0765 - val_accuracy: 0.9723\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 56s - loss: 0.1179 - accuracy: 0.9495 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19660/1501325532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fd8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f6883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b058ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43530db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99db6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b30013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196759ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250da400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
