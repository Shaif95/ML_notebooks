{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bfd55cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0384eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "#q = discard_random_image(image_parts[15])\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def resize_images(q,target_size=(56, 56)):\n",
    "    img = Image.fromarray(q)\n",
    "    resized_img = img.resize((56,56), Image.ANTIALIAS)\n",
    "    return resized_img\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "def prepare_videoes(image_paths, target_size=(56, 56)):\n",
    "    images = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        # Load image\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # Convert to RGB if not already\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Resize the image\n",
    "        img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Append the processed image to the list\n",
    "        images.append(img_array)\n",
    "    \n",
    "    num_images = len(images)\n",
    "    num_parts = 16\n",
    "    part_length = num_images // num_parts\n",
    "    remaining = num_images % num_parts\n",
    "    \n",
    "    image_parts = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        end_idx = start_idx + part_length + (1 if i < remaining else 0)\n",
    "        image_part = images[start_idx:end_idx]\n",
    "        image_parts.append(image_part)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    processed_parts = [discard_random_image(part) for part in image_parts]\n",
    "    \n",
    "    img_parts = [resize_images(part) for part in processed_parts]\n",
    "    \n",
    "    combined_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    for i, img_part in enumerate(img_parts):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        combined_image[row*56:(row+1)*56, col*56:(col+1)*56, :] = img_part\n",
    "    \n",
    "    return combined_image\n",
    "\n",
    "#combined_image = prepare_videoes(vid)\n",
    "#print(\"Combined image shape:\", combined_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b85bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_12816\\595239305.py:61: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize(target_size, Image.ANTIALIAS)\n",
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_12816\\595239305.py:30: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize((56,56), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "len(tr)\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))\n",
    "\n",
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)\n",
    "\n",
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    #print(\"..........................\")\n",
    "    #print(x)\n",
    "    #print(\"..........................\")\n",
    "    imgdata=prepare_videoes(x)\n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata.astype('float32') / 255.\n",
    "    #print(\"..........................\")\n",
    "    #print(np.shape(X_train))\n",
    "    #print(\"..........................\")\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)\n",
    "\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty arrays to store training and validation data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "batch_size = 16\n",
    "num_train_samples = len(train)  # Number of training samples\n",
    "num_val_samples = len(val)      # Number of validation samples\n",
    "\n",
    "# Initialize your custom batch generators\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "# Load training data\n",
    "for batch_idx in range(num_train_samples // batch_size):\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(batch_idx)\n",
    "    X_train.append(batch_images)\n",
    "    Y_train.append(batch_labels)\n",
    "\n",
    "# Load any remaining training data (if num_train_samples is not a multiple of batch_size)\n",
    "if num_train_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(num_train_samples // batch_size)\n",
    "    X_train.append(batch_images[:num_train_samples % batch_size])\n",
    "    Y_train.append(batch_labels[:num_train_samples % batch_size])\n",
    "\n",
    "# Load validation data\n",
    "for batch_idx in range(num_val_samples // batch_size):\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(batch_idx)\n",
    "    X_val.append(batch_images)\n",
    "    Y_val.append(batch_labels)\n",
    "\n",
    "# Load any remaining validation data (if num_val_samples is not a multiple of batch_size)\n",
    "if num_val_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(num_val_samples // batch_size)\n",
    "    X_val.append(batch_images[:num_val_samples % batch_size])\n",
    "    Y_val.append(batch_labels[:num_val_samples % batch_size])\n",
    "\n",
    "# Concatenate the loaded batches into arrays\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "Y_val = np.concatenate(Y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b760a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "# Width and height of image\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "class Augmentation(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    @tf.function\n",
    "    def random_execute(self, prob: float) -> bool:\n",
    "        return tf.random.uniform([], minval=0, maxval=1) < prob\n",
    "\n",
    "\n",
    "class RandomToGrayscale(Augmentation):\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        if self.random_execute(0.2):\n",
    "            x = tf.image.rgb_to_grayscale(x)\n",
    "            x = tf.tile(x, [1, 1, 3])\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomColorJitter(Augmentation):\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "      \n",
    "\n",
    "        if self.random_execute(0.8):\n",
    "            x = tf.image.random_brightness(x, 0.8)\n",
    "            x = tf.image.random_contrast(x, 0.4, 1.6)\n",
    "            x = tf.image.random_saturation(x, 0.4, 1.6)\n",
    "            x = tf.image.random_hue(x, 0.2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomFlip(Augmentation):\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        if self.random_execute(0.5):\n",
    "            x = tf.image.random_flip_left_right(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomResizedCrop(Augmentation):\n",
    "\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        rand_size = tf.random.uniform(\n",
    "            shape=[],\n",
    "            minval=int(0.75 * self.image_size),\n",
    "            maxval=1 * self.image_size,\n",
    "            dtype=tf.int32,\n",
    "        )\n",
    "\n",
    "        crop = tf.image.random_crop(x, (rand_size, rand_size, 3))\n",
    "        crop_resize = tf.image.resize(crop, (self.image_size, self.image_size))\n",
    "        return crop_resize\n",
    "\n",
    "\n",
    "class RandomSolarize(Augmentation):\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "        if self.random_execute(0.2):\n",
    "            # flips abnormally low pixels to abnormally high pixels\n",
    "            x = tf.where(x < 10, x, 255 - x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RandomBlur(Augmentation):\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "   \n",
    "\n",
    "        if self.random_execute(0.2):\n",
    "            s = np.random.random()\n",
    "            return tfa.image.gaussian_filter2d(image=x, sigma=s)\n",
    "        return x\n",
    "import tensorflow as tf\n",
    "\n",
    "class EdgeIntensityBasedCrop(tf.keras.layers.Layer):\n",
    "    def __init__(self, image_size):\n",
    "        super(EdgeIntensityBasedCrop, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        print(\"Edge\")\n",
    "        \n",
    "        # Calculate edge intensity map (e.g., using Sobel filtering)\n",
    "        gray_image = tf.image.rgb_to_grayscale(x)\n",
    "        sobel_x = tf.image.sobel_edges(gray_image)[0]\n",
    "        sobel_y = tf.image.sobel_edges(gray_image)[1]\n",
    "        edge_intensity_map = tf.sqrt(sobel_x**2 + sobel_y**2)\n",
    "        \n",
    "        d = tf.round (0.5 * self.image_size)\n",
    "         \n",
    "        crop = tf.image.random_crop(x, ( d , d , 3)  )\n",
    "        crop_resize = tf.image.resize(crop, (self.image_size, self.image_size))\n",
    "        \n",
    "        return crop_resize\n",
    "\n",
    "\n",
    "class RandomAugmentor(keras.Model):\n",
    "    \n",
    "\n",
    "    def __init__(self, image_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.random_resized_crop = RandomResizedCrop(image_size)\n",
    "        self.EdgeIntensityBasedCrop = EdgeIntensityBasedCrop(image_size)\n",
    "        self.random_flip = RandomFlip()\n",
    "        self.random_color_jitter = RandomColorJitter()\n",
    "        self.random_blur = RandomBlur()\n",
    "        self.random_to_grayscale = RandomToGrayscale()\n",
    "        self.random_solarize = RandomSolarize()\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        x = self.random_resized_crop(x)\n",
    "        x = self.random_flip(x)\n",
    "        x = self.random_color_jitter(x)\n",
    "        x = self.random_blur(x)\n",
    "        x = self.random_to_grayscale(x)\n",
    "        x = self.random_solarize(x)\n",
    "        x = self.EdgeIntensityBasedCrop(x)\n",
    "\n",
    "        x = tf.clip_by_value(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "bt_augmentor = RandomAugmentor(IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19ab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f18639f",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 44\u001b[0m\n\u001b[0;32m     39\u001b[0m         a2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmented_version(ds)\n\u001b[0;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((a1, a2))\u001b[38;5;241m.\u001b[39mwith_options(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m---> 44\u001b[0m augment_versions \u001b[38;5;241m=\u001b[39m \u001b[43mBTDatasetCreator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbt_augmentor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 38\u001b[0m, in \u001b[0;36mBTDatasetCreator.__call__\u001b[1;34m(self, ds)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[1;32m---> 38\u001b[0m     a1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmented_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     a2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmented_version(ds)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((a1, a2))\u001b[38;5;241m.\u001b[39mwith_options(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m, in \u001b[0;36mBTDatasetCreator.augmented_version\u001b[1;34m(self, ds)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maugmented_version\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 29\u001b[0m         \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentor, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;241m.\u001b[39mwith_options(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m     35\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:809\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    733\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \n\u001b[0;32m    735\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4551\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4550\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4551\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4552\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4553\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:125\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    122\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 125\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1640\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1631\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1632\u001b[0m           _add_error_prefix(\n\u001b[0;32m   1633\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1636\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1637\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m   1639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1640\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1643\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "class BTDatasetCreator:\n",
    "    \"\"\"Barlow twins dataset creator class.\n",
    "\n",
    "    BTDatasetCreator class. Responsible for creating the\n",
    "    barlow twins' dataset.\n",
    "\n",
    "    Attributes:\n",
    "        options: tf.data.Options needed to configure a setting\n",
    "          that may improve performance.\n",
    "        seed: random seed for shuffling. Used to synchronize two\n",
    "          augmented versions.\n",
    "        augmentor: augmentor used for augmentation.\n",
    "\n",
    "    Methods:\n",
    "        __call__: creates barlow dataset.\n",
    "        augmented_version: creates 1 half of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, augmentor: RandomAugmentor, seed: int = 1024):\n",
    "        self.options = tf.data.Options()\n",
    "        self.options.threading.max_intra_op_parallelism = 1\n",
    "        self.seed = seed\n",
    "        self.augmentor = augmentor\n",
    "\n",
    "    def augmented_version(self, ds: list) -> tf.data.Dataset:\n",
    "        return (\n",
    "            tf.data.Dataset.from_tensor_slices(ds)\n",
    "            .shuffle(1000, seed=self.seed)\n",
    "            .map(self.augmentor, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "            .batch(BATCH_SIZE, drop_remainder=True)\n",
    "            .prefetch(tf.data.AUTOTUNE)\n",
    "            .with_options(self.options)\n",
    "        )\n",
    "\n",
    "    def __call__(self, ds: list) -> tf.data.Dataset:\n",
    "        a1 = self.augmented_version(ds)\n",
    "        a2 = self.augmented_version(ds)\n",
    "\n",
    "        return tf.data.Dataset.zip((a1, a2)).with_options(self.options)\n",
    "\n",
    "\n",
    "augment_versions = BTDatasetCreator(bt_augmentor)(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143442e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a502d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_augment_versions = iter(augment_versions)\n",
    "\n",
    "\n",
    "def plot_values(batch: tuple):\n",
    "    fig, axs = plt.subplots(3, 3)\n",
    "    fig1, axs1 = plt.subplots(3, 3)\n",
    "\n",
    "    fig.suptitle(\"Augmentation 1\")\n",
    "    fig1.suptitle(\"Augmentation 2\")\n",
    "\n",
    "    a1, a2 = batch\n",
    "\n",
    "    # plots images on both tables\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # CHANGE(add / 255)\n",
    "            axs[i][j].imshow(a1[2 * i + j])\n",
    "            axs[i][j].axis(\"off\")\n",
    "            axs1[i][j].imshow(a2[2 * i + j])\n",
    "            axs1[i][j].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_values(next(sample_augment_versions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52fc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab08b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowLoss(keras.losses.Loss):\n",
    "    \"\"\"BarlowLoss class.\n",
    "\n",
    "    BarlowLoss class. Creates a loss function based on the cross-correlation\n",
    "    matrix.\n",
    "\n",
    "    Attributes:\n",
    "        batch_size: the batch size of the dataset\n",
    "        lambda_amt: the value for lambda(used in cross_corr_matrix_loss)\n",
    "\n",
    "    Methods:\n",
    "        __init__: gets instance variables\n",
    "        call: gets the loss based on the cross-correlation matrix\n",
    "          make_diag_zeros: Used in calculating off-diagonal section\n",
    "          of loss function; makes diagonals zeros.\n",
    "        cross_corr_matrix_loss: creates loss based on cross correlation\n",
    "          matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size: int):\n",
    "        \"\"\"__init__ method.\n",
    "\n",
    "        Gets the instance variables\n",
    "\n",
    "        Arguments:\n",
    "            batch_size: An integer value representing the batch size of the\n",
    "              dataset. Used for cross correlation matrix calculation.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.lambda_amt = 5e-3\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_off_diag(self, c: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"get_off_diag method.\n",
    "\n",
    "        Makes the diagonals of the cross correlation matrix zeros.\n",
    "        This is used in the off-diagonal portion of the loss function,\n",
    "        where we take the squares of the off-diagonal values and sum them.\n",
    "\n",
    "        Arguments:\n",
    "            c: A tf.tensor that represents the cross correlation\n",
    "              matrix\n",
    "\n",
    "        Returns:\n",
    "            Returns a tf.tensor which represents the cross correlation\n",
    "            matrix with its diagonals as zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        zero_diag = tf.zeros(c.shape[-1])\n",
    "        return tf.linalg.set_diag(c, zero_diag)\n",
    "\n",
    "    def cross_corr_matrix_loss(self, c: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"cross_corr_matrix_loss method.\n",
    "\n",
    "        Gets the loss based on the cross correlation matrix.\n",
    "        We want the diagonals to be 1's and everything else to be\n",
    "        zeros to show that the two augmented images are similar.\n",
    "\n",
    "        Loss function procedure:\n",
    "        take the diagonal of the cross-correlation matrix, subtract by 1,\n",
    "        and square that value so no negatives.\n",
    "\n",
    "        Take the off-diagonal of the cc-matrix(see get_off_diag()),\n",
    "        square those values to get rid of negatives and increase the value,\n",
    "        and multiply it by a lambda to weight it such that it is of equal\n",
    "        value to the optimizer as the diagonal(there are more values off-diag\n",
    "        then on-diag)\n",
    "\n",
    "        Take the sum of the first and second parts and then sum them together.\n",
    "\n",
    "        Arguments:\n",
    "            c: A tf.tensor that represents the cross correlation\n",
    "              matrix\n",
    "\n",
    "        Returns:\n",
    "            Returns a tf.tensor which represents the cross correlation\n",
    "            matrix with its diagonals as zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        # subtracts diagonals by one and squares them(first part)\n",
    "        c_diff = tf.pow(tf.linalg.diag_part(c) - 1, 2)\n",
    "\n",
    "        # takes off diagonal, squares it, multiplies with lambda(second part)\n",
    "        off_diag = tf.pow(self.get_off_diag(c), 2) * self.lambda_amt\n",
    "\n",
    "        # sum first and second parts together\n",
    "        loss = tf.reduce_sum(c_diff) + tf.reduce_sum(off_diag)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize(self, output: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"normalize method.\n",
    "\n",
    "        Normalizes the model prediction.\n",
    "\n",
    "        Arguments:\n",
    "            output: the model prediction.\n",
    "\n",
    "        Returns:\n",
    "            Returns a normalized version of the model prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        return (output - tf.reduce_mean(output, axis=0)) / tf.math.reduce_std(\n",
    "            output, axis=0\n",
    "        )\n",
    "\n",
    "    def cross_corr_matrix(self, z_a_norm: tf.Tensor, z_b_norm: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"cross_corr_matrix method.\n",
    "\n",
    "        Creates a cross correlation matrix from the predictions.\n",
    "        It transposes the first prediction and multiplies this with\n",
    "        the second, creating a matrix with shape (n_dense_units, n_dense_units).\n",
    "        See build_twin() for more info. Then it divides this with the\n",
    "        batch size.\n",
    "\n",
    "        Arguments:\n",
    "            z_a_norm: A normalized version of the first prediction.\n",
    "            z_b_norm: A normalized version of the second prediction.\n",
    "\n",
    "        Returns:\n",
    "            Returns a cross correlation matrix.\n",
    "        \"\"\"\n",
    "        return (tf.transpose(z_a_norm) @ z_b_norm) / self.batch_size\n",
    "\n",
    "    def call(self, z_a: tf.Tensor, z_b: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"call method.\n",
    "\n",
    "        Makes the cross-correlation loss. Uses the CreateCrossCorr\n",
    "        class to make the cross corr matrix, then finds the loss and\n",
    "        returns it(see cross_corr_matrix_loss()).\n",
    "\n",
    "        Arguments:\n",
    "            z_a: The prediction of the first set of augmented data.\n",
    "            z_b: the prediction of the second set of augmented data.\n",
    "\n",
    "        Returns:\n",
    "            Returns a (rank-0) tf.Tensor that represents the loss.\n",
    "        \"\"\"\n",
    "\n",
    "        z_a_norm, z_b_norm = self.normalize(z_a), self.normalize(z_b)\n",
    "        c = self.cross_corr_matrix(z_a_norm, z_b_norm)\n",
    "        loss = self.cross_corr_matrix_loss(c)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7fadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49fa93e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 1000)              350196968 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              2050048   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 352,247,016\n",
      "Trainable params: 2,050,048\n",
      "Non-trainable params: 350,196,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "class ResNet34:\n",
    "\n",
    "    def __call__(self, shape=(224, 224, 3)):\n",
    "        \n",
    "        inputs = Input(shape=shape)\n",
    "        base_model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://www.kaggle.com/models/spsayakpaul/convnext/frameworks/TensorFlow2/variations/xlarge-21k-1k-224/versions/1\", trainable=False)\n",
    "])\n",
    "        x = base_model(inputs)\n",
    "        output = Dense(2048)(x)\n",
    "        new_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "resnet = ResNet34()()\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cf27a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_twin() -> keras.Model:\n",
    "    \"\"\"build_twin method.\n",
    "\n",
    "    Builds a barlow twins model consisting of an encoder(resnet-34)\n",
    "    and a projector, which generates embeddings for the images\n",
    "\n",
    "    Returns:\n",
    "        returns a barlow twins model\n",
    "    \"\"\"\n",
    "\n",
    "    # number of dense neurons in the projector\n",
    "    n_dense_neurons = 5000\n",
    "\n",
    "    # encoder network\n",
    "    resnet = ResNet34()()\n",
    "    last_layer = resnet.layers[-1].output\n",
    "\n",
    "    # intermediate layers of the projector network\n",
    "    n_layers = 2\n",
    "    for i in range(n_layers):\n",
    "        dense = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{i}\")\n",
    "        if i == 0:\n",
    "            x = dense(last_layer)\n",
    "        else:\n",
    "            x = dense(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"projector_bn_{i}\")(x)\n",
    "        x = tf.keras.layers.ReLU(name=f\"projector_relu_{i}\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{n_layers}\")(x)\n",
    "\n",
    "    model = keras.Model(resnet.input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e407131",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowModel(keras.Model):\n",
    "    \"\"\"BarlowModel class.\n",
    "\n",
    "    BarlowModel class. Responsible for making predictions and handling\n",
    "    gradient descent with the optimizer.\n",
    "\n",
    "    Attributes:\n",
    "        model: the barlow model architecture.\n",
    "        loss_tracker: the loss metric.\n",
    "\n",
    "    Methods:\n",
    "        train_step: one train step; do model predictions, loss, and\n",
    "            optimizer step.\n",
    "        metrics: Returns metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = build_twin()\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, batch: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"train_step method.\n",
    "\n",
    "        Do one train step. Make model predictions, find loss, pass loss to\n",
    "        optimizer, and make optimizer apply gradients.\n",
    "\n",
    "        Arguments:\n",
    "            batch: one batch of data to be given to the loss function.\n",
    "\n",
    "        Returns:\n",
    "            Returns a dictionary with the loss metric.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the two augmentations from the batch\n",
    "        y_a, y_b = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get two versions of predictions\n",
    "            z_a, z_b = self.model(y_a, training=True), self.model(y_b, training=True)\n",
    "            loss = self.loss(z_a, z_b)\n",
    "\n",
    "        grads_model = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grads_model, self.model.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc010782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "234/234 [==============================] - 282s 1s/step - loss: 21019.0859\n",
      "Epoch 2/2\n",
      "234/234 [==============================] - 251s 1s/step - loss: 20619.5859\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm = BarlowModel()\n",
    "optimizer = tfa.optimizers.LAMB()\n",
    "loss = BarlowLoss(BATCH_SIZE)\n",
    "bm.compile(optimizer=optimizer, loss=loss)\n",
    "history = bm.fit(augment_versions, epochs=2)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "588eedd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47/47 [==============================] - 20s 243ms/step - loss: 20.4568 - accuracy: 0.0707 - val_loss: 304144640.0000 - val_accuracy: 0.0907\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 9s 190ms/step - loss: 11.2150 - accuracy: 0.0761 - val_loss: 1537544.2500 - val_accuracy: 0.0720\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 9s 195ms/step - loss: 9.4836 - accuracy: 0.0714 - val_loss: 6432.9375 - val_accuracy: 0.0693\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 10s 206ms/step - loss: 6.7765 - accuracy: 0.0734 - val_loss: 8070.9160 - val_accuracy: 0.0640\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 11s 226ms/step - loss: 6.3944 - accuracy: 0.0807 - val_loss: 21861.7891 - val_accuracy: 0.0693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5bea8ec70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D\n",
    "model = bm.model\n",
    "\n",
    "num_classes = 10  # Update with the actual number of classes in your target data\n",
    "model.layers[0].trainable = False\n",
    "x = model.layers[-1].output  # Access the last 4th layer from the end\n",
    "output = Dense(15, activation='softmax')(x)\n",
    "# Create the new model with the updated head\n",
    "new_model = keras.models.Model(inputs=model.input, outputs=output)\n",
    "# Compile the model\n",
    "new_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "new_model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc865f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09c1f5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 106ms/step\n",
      "Test Accuracy: 0.08450704225352113\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have already trained your model and have test_features and y_test ready\n",
    "predictions = new_model.predict(X_val)\n",
    "accuracy = accuracy_score( np.argmax(Y_val, axis = 1) , np.argmax(predictions, axis = 1) )\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7ef99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e814b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827799a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090ec4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b5d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbb61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89200411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee5880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d79f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2babaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f71367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59fc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68222940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c96e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b4894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f51a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cd2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b678a5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      5\u001b[0m     hub\u001b[38;5;241m.\u001b[39mKerasLayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/models/spsayakpaul/convnext/frameworks/TensorFlow2/variations/xlarge-21k-1k-224/versions/1\u001b[39m\u001b[38;5;124m\"\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m ])\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\training.py:2869\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001b[0m\n\u001b[0;32m   2847\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   2848\u001b[0m \n\u001b[0;32m   2849\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2866\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   2867\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m-> 2869\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2870\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2871\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2872\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe model on a batch of data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2873\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\n\u001b[0;32m   2874\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2875\u001b[0m     line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2878\u001b[0m     expand_nested\u001b[38;5;241m=\u001b[39mexpand_nested,\n\u001b[0;32m   2879\u001b[0m     show_trainable\u001b[38;5;241m=\u001b[39mshow_trainable)\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e922a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7ad8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
