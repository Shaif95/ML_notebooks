{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd392b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd4d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3db760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023897409439086914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading spiece.model",
       "rate": null,
       "total": 760289,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9541a7c15e648afb7d642fe65895ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shaif\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0316624641418457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 25,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c575670e8cd5479bb29314b1768ae2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029972314834594727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 684,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b1b2c6cd274bb08a8671139a0697ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.044747114181518555,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading builder script",
       "rate": null,
       "total": 5090,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df735f70be484b3c8a4863f1ac909b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011113643646240234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading readme",
       "rate": null,
       "total": 10630,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dedb48917f446caf52676d17f33471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016922712326049805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355a164b90c44d5da8df036841c60de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015875816345214844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 335858,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b929243816f748eeaaa4597723d63b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/336k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035686492919921875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 23354,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12443d333ca24039b6485d45c7bef726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016979694366455078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 5452,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd90cfaba5b24cb7ac99ead24d9d6fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019753456115722656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 500,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ace52b6d7e84829bd2fd161fa41b33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03202629089355469,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading model.safetensors",
       "rate": null,
       "total": 47372894,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb11e6810d64cedab9f0fdbf8d91185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.4550, Accuracy=0.8465\n",
      "Epoch 2: Loss=0.4757, Accuracy=0.8208\n",
      "Epoch 3: Loss=0.2379, Accuracy=0.9382\n",
      "Test Accuracy: 0.9540\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AlbertModel, AlbertTokenizer, get_scheduler\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load TREC Dataset\n",
    "dataset = load_dataset(\"trec\")\n",
    "labels_list = dataset[\"train\"].features[\"coarse_label\"].names  # Class names\n",
    "NUM_CLASSES = len(labels_list)  # 6 classes\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TrecDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.data = dataset[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"coarse_label\"]\n",
    "        inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(TrecDataset(\"train\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TrecDataset(\"test\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Custom ALBERT Model\n",
    "class CustomAlbertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.albert = AlbertModel.from_pretrained(MODEL_NAME)\n",
    "        self.classifier = nn.Linear(self.albert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.albert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [CLS] token output\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Initialize Model\n",
    "model = CustomAlbertClassifier(NUM_CLASSES).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * EPOCHS)\n",
    "\n",
    "# Training Loop\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Accuracy={correct/total:.4f}\")\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    print(f\"Test Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# Run Training and Evaluation\n",
    "train()\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0e684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680d8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fd8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AlbertTokenizer, AlbertConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load TREC Dataset\n",
    "dataset = load_dataset(\"trec\")\n",
    "NUM_CLASSES = len(dataset[\"train\"].features[\"coarse_label\"].names)\n",
    "\n",
    "# Custom Dataset\n",
    "class TrecDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.data = dataset[split]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx][\"text\"]\n",
    "        label = self.data[idx][\"coarse_label\"]\n",
    "        inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(TrecDataset(\"train\"), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(TrecDataset(\"test\"), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Custom Attention\n",
    "class DynamicEnergyRegulatedTemporalSpatialAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_phi = nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.W_alpha = nn.Linear(d_model, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, E_embed, mask=None):\n",
    "        B, L, _ = x.shape\n",
    "\n",
    "        Q = self.q_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(B, L, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        phi_E = torch.matmul(E_embed, self.W_phi)\n",
    "        alpha = self.sigmoid(self.W_alpha(phi_E)).squeeze(-1)\n",
    "        bias_scalar = phi_E.mean(dim=-1)\n",
    "\n",
    "        bias = (alpha * bias_scalar).unsqueeze(1).unsqueeze(-1)  # (B, 1, L, 1)\n",
    "        attn_scores += bias  # broadcast to (B, heads, L, L)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        out = torch.matmul(attn_weights, V)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "# Encoder Block\n",
    "class CustomAlbertBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = DynamicEnergyRegulatedTemporalSpatialAttention(config.hidden_size, config.num_attention_heads)\n",
    "        self.attn_ln = nn.LayerNorm(config.hidden_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.intermediate_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        )\n",
    "        self.ffn_ln = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, x, E_embed, mask=None):\n",
    "        attn_out = self.attn(x, E_embed, mask)\n",
    "        x = self.attn_ln(x + attn_out)\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ffn_ln(x + ffn_out)\n",
    "        return x\n",
    "\n",
    "# Custom Encoder\n",
    "\n",
    "class CustomAlbertEncoder(nn.Module):\n",
    "    def __init__(self, config, num_layers=12):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        self.blocks = nn.ModuleList([CustomAlbertBlock(config) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, E_embed=None):\n",
    "        B, L = input_ids.size()\n",
    "        position_ids = torch.arange(0, L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        x = self.embeddings(input_ids) + \\\n",
    "            self.position_embeddings(position_ids) + \\\n",
    "            self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if E_embed is None:\n",
    "            E_embed = x\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, E_embed, attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CustomAlbertForClassification(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = CustomAlbertEncoder(config, num_layers=12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.norm = nn.LayerNorm(config.hidden_size)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, E_embed=None):\n",
    "        x = self.encoder(input_ids, token_type_ids, attention_mask, E_embed)\n",
    "        cls_token = x[:, 0]\n",
    "        cls_token = self.norm(cls_token)\n",
    "        cls_token = self.dropout(cls_token)\n",
    "        return self.classifier(cls_token)\n",
    "\n",
    "\n",
    "# Load config and model\n",
    "config = AlbertConfig.from_pretrained(MODEL_NAME)\n",
    "model = CustomAlbertForClassification(config, NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# Optimizer & Loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train():\n",
    "    model.train()\n",
    "    for ep in range(EPOCHS):\n",
    "        total, correct, total_loss = 0, 0, 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {ep+1}: Loss={total_loss/len(train_loader):.4f}, Accuracy={correct/total:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    print(f\"Test Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "# Run training and evaluation\n",
    "train()\n",
    "evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8080734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1901a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3589c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
