{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaif\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "#train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.5, random_state=42 )\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.2, random_state=40 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.02),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 3)     7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_encoder():\n",
    "    resnet = tf.keras.applications.ResNet50V2( include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling=\"avg\" )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        logits = tf.divide( tf.matmul(  \n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)),self.temperature,)\n",
    "        \n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,089,351\n",
      "Trainable params: 24,043,904\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "313/313 [==============================] - 66s 151ms/step - loss: 4.0042\n",
      "Epoch 2/2\n",
      " 22/313 [=>............................] - ETA: 30s - loss: 3.9211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5612\\549349323.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mencoder_with_projection_head\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m history = encoder_with_projection_head.fit(\n\u001b[0m\u001b[0;32m     13\u001b[0m     x=X_train, y=Y_train, batch_size=batch_size, epochs=2)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1221\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m       raise ValueError(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    548\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \"\"\"\n\u001b[0;32m   1148\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1113\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=X_train, y=Y_train, batch_size=batch_size, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "157/157 [==============================] - 13s 47ms/step - loss: nan - sparse_categorical_accuracy: 0.1015\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 10s 62ms/step - loss: nan - sparse_categorical_accuracy: 0.1015\n",
      "157/157 [==============================] - 4s 21ms/step - loss: nan - sparse_categorical_accuracy: 0.1044\n",
      "Test accuracy: 10.44%\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resmodel():\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(32))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model(X,Y,X_test, Y_test,num_epochs):\n",
    "    \n",
    "    data_augmentation.layers[0].adapt(X)\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    \n",
    "    encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                                     loss=SupervisedContrastiveLoss(temperature),)\n",
    "    \n",
    "    \n",
    "    history = encoder_with_projection_head.fit(x=X, y=Y, batch_size=256, epochs=num_epochs)\n",
    "    \n",
    "    \n",
    "    classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "    history = classifier.fit(x=X, y=Y, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    #model.summary()\n",
    "    return encoder,classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.concatenate((X_train, x_unlab))\n",
    "X_all = arr\n",
    "arr = np.concatenate((Y_train, y_unlab))\n",
    "Y_all = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test,num):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    encoder,classifier = create_model(X_train,Y_train,X_test, Y_test,num)\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") \n",
    "    \n",
    "    \n",
    "    return encoder,classifier\n",
    "\n",
    "\n",
    "encoder,classifier = train_full_model(X_all, X_test, Y_all, Y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,classifier = train_full_model(X_train, X_test, Y_train, Y_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "313/313 [==============================] - 862s 3s/step - loss: 4.9117\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 861s 3s/step - loss: 3.7871\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 870s 3s/step - loss: 3.5972\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 872s 3s/step - loss: 3.4477\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 875s 3s/step - loss: 3.3051\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "313/313 [==============================] - 83s 247ms/step - loss: 1.6041 - sparse_categorical_accuracy: 0.4839\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 1.2944 - sparse_categorical_accuracy: 0.5731\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 79s 253ms/step - loss: 1.2170 - sparse_categorical_accuracy: 0.5895\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 78s 250ms/step - loss: 1.1740 - sparse_categorical_accuracy: 0.6022\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 78s 249ms/step - loss: 1.1447 - sparse_categorical_accuracy: 0.6090\n",
      "157/157 [==============================] - 16s 97ms/step - loss: 1.1057 - sparse_categorical_accuracy: 0.6172\n",
      "Acc Test : \n",
      "\n",
      "\n",
      "0.6172000169754028\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "391/391 [==============================] - 68s 171ms/step\n",
      "(25000, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(2817,)\n",
      "(350,)\n",
      "89/89 [==============================] - 8s 91ms/step - loss: 1.2294 - sparse_categorical_accuracy: 0.6773\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.6773163080215454\n",
      "45/45 [==============================] - 8s 159ms/step\n",
      "This\n",
      "(20000, 1)\n",
      "Now\n",
      "(2817, 10)\n",
      "Now2\n",
      "(20350, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1272/1272 [==============================] - 112s 88ms/step - loss: 1.1226 - sparse_categorical_accuracy: 0.6158\n",
      "Epoch 2/5\n",
      "1272/1272 [==============================] - 114s 90ms/step - loss: 1.0950 - sparse_categorical_accuracy: 0.6168\n",
      "Epoch 3/5\n",
      "1272/1272 [==============================] - 115s 91ms/step - loss: 1.0712 - sparse_categorical_accuracy: 0.6269\n",
      "Epoch 4/5\n",
      "1272/1272 [==============================] - 114s 90ms/step - loss: 1.0609 - sparse_categorical_accuracy: 0.6271\n",
      "Epoch 5/5\n",
      "1272/1272 [==============================] - 113s 89ms/step - loss: 1.0504 - sparse_categorical_accuracy: 0.6332\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "347/347 [==============================] - 60s 174ms/step\n",
      "(22182, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(3220,)\n",
      "(350,)\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 0.8914 - sparse_categorical_accuracy: 0.6944\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.694409966468811\n",
      "51/51 [==============================] - 8s 163ms/step\n",
      "This\n",
      "(20350, 1)\n",
      "Now\n",
      "(3220, 10)\n",
      "Now2\n",
      "(20700, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1294/1294 [==============================] - 116s 89ms/step - loss: 1.0357 - sparse_categorical_accuracy: 0.6414\n",
      "Epoch 2/5\n",
      "1294/1294 [==============================] - 116s 90ms/step - loss: 1.0128 - sparse_categorical_accuracy: 0.6437\n",
      "Epoch 3/5\n",
      "1294/1294 [==============================] - 113s 87ms/step - loss: 1.0133 - sparse_categorical_accuracy: 0.6432\n",
      "Epoch 4/5\n",
      "1294/1294 [==============================] - 114s 88ms/step - loss: 1.0096 - sparse_categorical_accuracy: 0.6431\n",
      "Epoch 5/5\n",
      "1294/1294 [==============================] - 114s 88ms/step - loss: 1.0064 - sparse_categorical_accuracy: 0.6467\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "297/297 [==============================] - 51s 171ms/step\n",
      "(18961, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(3084,)\n",
      "(350,)\n",
      "97/97 [==============================] - 9s 92ms/step - loss: 0.9532 - sparse_categorical_accuracy: 0.6793\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.6793125867843628\n",
      "49/49 [==============================] - 8s 171ms/step\n",
      "This\n",
      "(20700, 1)\n",
      "Now\n",
      "(3084, 10)\n",
      "Now2\n",
      "(21050, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1316/1316 [==============================] - 117s 89ms/step - loss: 1.0098 - sparse_categorical_accuracy: 0.6419\n",
      "Epoch 2/5\n",
      "1316/1316 [==============================] - 118s 89ms/step - loss: 0.9805 - sparse_categorical_accuracy: 0.6529\n",
      "Epoch 3/5\n",
      "1316/1316 [==============================] - 116s 88ms/step - loss: 0.9993 - sparse_categorical_accuracy: 0.6505\n",
      "Epoch 4/5\n",
      "1316/1316 [==============================] - 115s 87ms/step - loss: 0.9904 - sparse_categorical_accuracy: 0.6481\n",
      "Epoch 5/5\n",
      "1316/1316 [==============================] - 113s 86ms/step - loss: 0.9860 - sparse_categorical_accuracy: 0.6536\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "249/249 [==============================] - 42s 170ms/step\n",
      "(15876, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(3220,)\n",
      "(350,)\n",
      "101/101 [==============================] - 9s 94ms/step - loss: 0.8821 - sparse_categorical_accuracy: 0.6832\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.6832298040390015\n",
      "51/51 [==============================] - 8s 164ms/step\n",
      "This\n",
      "(21050, 1)\n",
      "Now\n",
      "(3220, 10)\n",
      "Now2\n",
      "(21400, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1338/1338 [==============================] - 116s 87ms/step - loss: 0.9802 - sparse_categorical_accuracy: 0.6538\n",
      "Epoch 2/5\n",
      "1338/1338 [==============================] - 117s 88ms/step - loss: 0.9805 - sparse_categorical_accuracy: 0.6552\n",
      "Epoch 3/5\n",
      "1338/1338 [==============================] - 118s 88ms/step - loss: 0.9735 - sparse_categorical_accuracy: 0.6530\n",
      "Epoch 4/5\n",
      "1338/1338 [==============================] - 118s 88ms/step - loss: 0.9849 - sparse_categorical_accuracy: 0.6556\n",
      "Epoch 5/5\n",
      "1338/1338 [==============================] - 118s 88ms/step - loss: 0.9769 - sparse_categorical_accuracy: 0.6551\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "198/198 [==============================] - 34s 169ms/step\n",
      "(12655, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(3172,)\n",
      "(350,)\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 1.0265 - sparse_categorical_accuracy: 0.6450\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.6450189352035522\n",
      "50/50 [==============================] - 9s 170ms/step\n",
      "This\n",
      "(21400, 1)\n",
      "Now\n",
      "(3172, 10)\n",
      "Now2\n",
      "(21750, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1360/1360 [==============================] - 120s 88ms/step - loss: 0.9733 - sparse_categorical_accuracy: 0.6590\n",
      "Epoch 2/5\n",
      "1360/1360 [==============================] - 119s 88ms/step - loss: 0.9708 - sparse_categorical_accuracy: 0.6547\n",
      "Epoch 3/5\n",
      "1360/1360 [==============================] - 119s 88ms/step - loss: 0.9715 - sparse_categorical_accuracy: 0.6595\n",
      "Epoch 4/5\n",
      "1360/1360 [==============================] - 119s 88ms/step - loss: 0.9687 - sparse_categorical_accuracy: 0.6575\n",
      "Epoch 5/5\n",
      "1360/1360 [==============================] - 119s 88ms/step - loss: 0.9570 - sparse_categorical_accuracy: 0.6606\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "149/149 [==============================] - 25s 169ms/step\n",
      "(9482, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(2909,)\n",
      "(350,)\n",
      "91/91 [==============================] - 9s 96ms/step - loss: 1.1234 - sparse_categorical_accuracy: 0.5785\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.578549325466156\n",
      "46/46 [==============================] - 8s 163ms/step\n",
      "This\n",
      "(21750, 1)\n",
      "Now\n",
      "(2909, 10)\n",
      "Now2\n",
      "(22100, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1382/1382 [==============================] - 122s 88ms/step - loss: 0.9643 - sparse_categorical_accuracy: 0.6588\n",
      "Epoch 2/5\n",
      "1382/1382 [==============================] - 120s 87ms/step - loss: 0.9711 - sparse_categorical_accuracy: 0.6590\n",
      "Epoch 3/5\n",
      "1382/1382 [==============================] - 120s 87ms/step - loss: 0.9743 - sparse_categorical_accuracy: 0.6544\n",
      "Epoch 4/5\n",
      "1382/1382 [==============================] - 122s 88ms/step - loss: 0.9512 - sparse_categorical_accuracy: 0.6646\n",
      "Epoch 5/5\n",
      "1382/1382 [==============================] - 120s 87ms/step - loss: 0.9647 - sparse_categorical_accuracy: 0.6574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "103/103 [==============================] - 18s 171ms/step\n",
      "(6572, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "350\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3220\n",
      "\n",
      "\n",
      "(2787,)\n",
      "(350,)\n",
      "88/88 [==============================] - 9s 96ms/step - loss: 1.3101 - sparse_categorical_accuracy: 0.5353\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.5353426337242126\n",
      "44/44 [==============================] - 8s 174ms/step\n",
      "This\n",
      "(22100, 1)\n",
      "Now\n",
      "(2787, 10)\n",
      "Now2\n",
      "(22450, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1404/1404 [==============================] - 124s 88ms/step - loss: 0.9683 - sparse_categorical_accuracy: 0.6570\n",
      "Epoch 2/5\n",
      "1404/1404 [==============================] - 124s 88ms/step - loss: 0.9569 - sparse_categorical_accuracy: 0.6630\n",
      "Epoch 3/5\n",
      "1404/1404 [==============================] - 125s 89ms/step - loss: 0.9518 - sparse_categorical_accuracy: 0.6604\n",
      "Epoch 4/5\n",
      "1404/1404 [==============================] - 124s 88ms/step - loss: 0.9495 - sparse_categorical_accuracy: 0.6605\n",
      "Epoch 5/5\n",
      "1404/1404 [==============================] - 124s 88ms/step - loss: 0.9549 - sparse_categorical_accuracy: 0.6626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "    \n",
    "    print(\"Acc Test : \")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy)\n",
    "    print(\"\\n\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    #x = int(np.round( l/d ))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "  \n",
    "        x_ulb = encoder.predict(x_unlab, batch_size=batch_size)\n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 10\n",
    "        num_points_per_class = int ( np.round (d/10)   )\n",
    "        \n",
    "        budget =  int(num_points_per_class /10 )\n",
    "        num_points_per_class = num_points_per_class - budget\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "        print(\"\\n\")\n",
    "        print(\"Chosen in each iter : \")\n",
    "        print(num_points_per_class*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        closest_points_indices = []\n",
    "        annotate_indices = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            closest_indices = cluster_indices[np.argsort(distances)[:num_points_per_class]]\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]]\n",
    "            \n",
    "            closest_points_indices.extend(closest_indices)\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "\n",
    "        chosen_indices = closest_points_indices\n",
    "    \n",
    "        print(np.shape(chosen_indices))\n",
    "        print(np.shape(annotate_indices))\n",
    "        \n",
    "        rnd = chosen_indices\n",
    "        \n",
    "        annt = annotate_indices\n",
    "        \n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(rnd) - set(annt))\n",
    "        \n",
    "        new_lab = x_unlab[rnd]\n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train, new_annt))\n",
    "        X_train = arr\n",
    "\n",
    "        \n",
    "        accuracy = classifier.evaluate(new_lab, y_unlab[rnd], batch_size=32)[1]\n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "        \n",
    "        new_y = np.round(classifier.predict(new_lab, batch_size=64))\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        print(\"This\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"Now\")\n",
    "        print(np.shape(new_y))\n",
    "        \n",
    "        new_yy = np.argmax(new_y, axis=1)\n",
    "        \n",
    "        new_yy = new_yy.reshape(len(new_y),1)\n",
    "        \n",
    "        \n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        #history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=1)\n",
    "        \n",
    "        print(\"Now2\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        history = classifier.fit(x=X_train, y=Y_train, batch_size=16, epochs=5) \n",
    "   \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "#train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.5, random_state=42 )\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.2, random_state=40 )\n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=64, epochs=5)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=64, epochs=5) \n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:22:59.202783\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "    \n",
    "    \n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    #x = int(np.round( l/d ))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "  \n",
    "        try :\n",
    "            x_ulb = encoder.predict(x_unlab, batch_size=batch_size)\n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 10\n",
    "        num_points_per_class = int ( np.round (d/10)   )\n",
    "        \n",
    "        budget =  int(num_points_per_class /10 )\n",
    "        num_points_per_class = num_points_per_class - budget\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "        print(\"\\n\")\n",
    "        print(\"Chosen in each iter : \")\n",
    "        print(num_points_per_class*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        closest_points_indices = []\n",
    "        annotate_indices = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            #closest_indices = cluster_indices[np.argsort(distances)[:num_points_per_class]]\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]] \n",
    "            \n",
    "            #closest_points_indices.extend(closest_indices)\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "\n",
    "        #chosen_indices = closest_points_indices\n",
    "    \n",
    "        #print(np.shape(chosen_indices))\n",
    "        print(np.shape(annotate_indices))\n",
    "        \n",
    "        #rnd = chosen_indices\n",
    "        \n",
    "        annt = annotate_indices\n",
    "        \n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(annt))\n",
    "        \n",
    "        #add those index to from unlablled set to training set\n",
    "        #new_lab = x_unlab[rnd]\n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train,new_annt))\n",
    "        X_train = arr\n",
    "\n",
    "        \n",
    "        try :\n",
    "            accuracy = classifier.evaluate(new_lab, y_unlab[rnd], batch_size=8)[1]\n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        print(\"This\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"Now\")\n",
    "        #print(np.shape(new_y))\n",
    "        \n",
    "        #new_yy = np.argmax(new_y, axis=1)\n",
    "        \n",
    "        #new_yy = new_yy.reshape(len(new_y),1)\n",
    "        \n",
    "        \n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        #try :\n",
    "            #history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=5)\n",
    "        #except :\n",
    "            #return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Now2\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try :\n",
    "            history = classifier.fit(x=X_train, y=Y_train, batch_size=4, epochs=5) \n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        #test on data\n",
    "    \n",
    "    #accuracy = classifier.evaluate(X_test, Y_test, batch_size=4)[1]\n",
    "   \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "\n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [7],\n",
       "       [8],\n",
       "       ...,\n",
       "       [3],\n",
       "       [1],\n",
       "       [3]], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =np.argmax(Y_train)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5784\\240059899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
