{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1da4b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2887fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78cd1f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d7f4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n",
      "GPU device name: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "    print('GPU device name: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92787667",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255\n",
    "\n",
    "X_train, x_unlab, Y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.2, random_state=42 )\n",
    "X_test = test_images\n",
    "Y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b2735a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288579b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.02),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dee2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 3)     7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_encoder():\n",
    "    resnet = tf.keras.applications.ResNet50V2( include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling=\"avg\" )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd752bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1da07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a83df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34764d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00fa1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        logits = tf.divide( tf.matmul(  \n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)),self.temperature,)\n",
    "        \n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb06c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,089,351\n",
      "Trainable params: 24,043,904\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 31s 92ms/step - loss: 6.3427\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 9s 57ms/step - loss: 4.6191\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=X_train, y=Y_train, batch_size=batch_size, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200f5288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035d07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e88fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "157/157 [==============================] - 6s 21ms/step - loss: 2.3388 - sparse_categorical_accuracy: 0.2248\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 3s 22ms/step - loss: 2.0065 - sparse_categorical_accuracy: 0.3101\n",
      "156/313 [=============>................] - ETA: 3s - loss: 1.7065 - sparse_categorical_accuracy: 0.4193"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc025ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cba6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a9e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resmodel():\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(32))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cdcefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model(X,Y,X_test, Y_test,num_epochs):\n",
    "    \n",
    "    data_augmentation.layers[0].adapt(X)\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    \n",
    "    encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                                     loss=SupervisedContrastiveLoss(temperature),)\n",
    "    \n",
    "    \n",
    "    history = encoder_with_projection_head.fit(x=X, y=Y, batch_size=256, epochs=num_epochs)\n",
    "    \n",
    "    \n",
    "    classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "    history = classifier.fit(x=X, y=Y, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return encoder,classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c844e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.concatenate((X_train, x_unlab))\n",
    "X_all = arr\n",
    "arr = np.concatenate((Y_train, y_unlab))\n",
    "Y_all = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test,num):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    encoder,classifier = create_model(X_train,Y_train,X_test, Y_test,num)\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") \n",
    "    \n",
    "    \n",
    "    return encoder,classifier\n",
    "\n",
    "\n",
    "encoder,classifier = train_full_model(X_all, X_test, Y_all, Y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,classifier = train_full_model(X_train, X_test, Y_train, Y_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d02db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4750e604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe19c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "176/176 [==============================] - 44s 137ms/step - loss: 6.5763\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 16s 88ms/step - loss: 5.1327\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 13s 73ms/step - loss: 4.9044\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 13s 72ms/step - loss: 4.7240\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 13s 75ms/step - loss: 4.5654\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 15s 88ms/step - loss: 4.4456\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 13s 74ms/step - loss: 4.3605\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 14s 77ms/step - loss: 4.2801\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 14s 77ms/step - loss: 4.2147\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 14s 81ms/step - loss: 4.1538\n",
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 36s 23ms/step - loss: 0.8177 - sparse_categorical_accuracy: 0.7533\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.6570 - sparse_categorical_accuracy: 0.7942\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.6238 - sparse_categorical_accuracy: 0.8015\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 36s 25ms/step - loss: 0.6065 - sparse_categorical_accuracy: 0.8035\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 36s 25ms/step - loss: 0.5960 - sparse_categorical_accuracy: 0.8056\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 0.5872 - sparse_categorical_accuracy: 0.8064\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 0.5862 - sparse_categorical_accuracy: 0.8076\n",
      "Epoch 8/10\n",
      " 461/1407 [========>.....................] - ETA: 20s - loss: 0.5781 - sparse_categorical_accuracy: 0.8088"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "  \n",
    "        try :\n",
    "            x_ulb = encoder.predict(x_unlab, batch_size=128)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 10\n",
    "        \n",
    "        budget =  200\n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        annotate_indices = []\n",
    "        \n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]]\n",
    "            \n",
    "            pts = cluster_indices[np.argsort(distances,-1)[:1]]\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "            \n",
    "\n",
    "        print(np.shape(annotate_indices))\n",
    "\n",
    "        annt = annotate_indices      \n",
    "        \n",
    "        ante = x_ulb[annt]\n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(annt))\n",
    "        \n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train, new_annt))\n",
    "        X_train = arr\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        try :\n",
    "            history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=10)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "      \n",
    "\n",
    "        try:\n",
    "            history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "\n",
    "        try :\n",
    "            accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "       \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train, y_train),( X_test, Y_test )= cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "X_test = X_test / 255 \n",
    "\n",
    "X_train, x_unlab, Y_train, y_unlab = train_test_split( x_train,y_train, test_size=0.1, random_state=42 )\n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=10)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "\n",
    "try :\n",
    "  accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"Acc : \")\n",
    "print(\"\\n\")\n",
    "print(accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354509b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa050921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2efda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f528cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42d186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226dab0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ee8c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "176/176 [==============================] - 14s 64ms/step - loss: 6.4512\n",
      "Epoch 2/10\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 5.1610\n",
      "Epoch 3/10\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 4.9008\n",
      "Epoch 4/10\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 4.7020\n",
      "Epoch 5/10\n",
      "176/176 [==============================] - 11s 65ms/step - loss: 4.5554\n",
      "Epoch 6/10\n",
      "176/176 [==============================] - 11s 65ms/step - loss: 4.4240\n",
      "Epoch 7/10\n",
      "176/176 [==============================] - 11s 64ms/step - loss: 4.3308\n",
      "Epoch 8/10\n",
      "176/176 [==============================] - 12s 65ms/step - loss: 4.2625\n",
      "Epoch 9/10\n",
      "176/176 [==============================] - 13s 76ms/step - loss: 4.2132\n",
      "Epoch 10/10\n",
      "176/176 [==============================] - 11s 63ms/step - loss: 4.1625\n",
      "Epoch 1/10\n",
      "1407/1407 [==============================] - 23s 15ms/step - loss: 0.7945 - sparse_categorical_accuracy: 0.7591\n",
      "Epoch 2/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.6550 - sparse_categorical_accuracy: 0.7929\n",
      "Epoch 3/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.8015\n",
      "Epoch 4/10\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 0.5943 - sparse_categorical_accuracy: 0.8065\n",
      "Epoch 5/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5974 - sparse_categorical_accuracy: 0.8056\n",
      "Epoch 6/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5746 - sparse_categorical_accuracy: 0.8102\n",
      "Epoch 7/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5731 - sparse_categorical_accuracy: 0.8104\n",
      "Epoch 8/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5675 - sparse_categorical_accuracy: 0.8135\n",
      "Epoch 9/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5619 - sparse_categorical_accuracy: 0.8139\n",
      "Epoch 10/10\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.5618 - sparse_categorical_accuracy: 0.8149\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7444 - sparse_categorical_accuracy: 0.7622\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7621999979019165\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "(5000, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "184/184 [==============================] - 13s 73ms/step - loss: 4.0914\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 13s 72ms/step - loss: 4.0446\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 4.0032\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 12s 66ms/step - loss: 3.9514\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 3.9360\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 12s 65ms/step - loss: 3.8912\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 3.8542\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 12s 63ms/step - loss: 3.8396\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 3.8136\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 12s 64ms/step - loss: 3.7831\n",
      "Epoch 1/10\n",
      "1469/1469 [==============================] - 23s 15ms/step - loss: 0.3393 - sparse_categorical_accuracy: 0.8936\n",
      "Epoch 2/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3320 - sparse_categorical_accuracy: 0.8953\n",
      "Epoch 3/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3209 - sparse_categorical_accuracy: 0.8989\n",
      "Epoch 4/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3163 - sparse_categorical_accuracy: 0.9001\n",
      "Epoch 5/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3160 - sparse_categorical_accuracy: 0.9002\n",
      "Epoch 6/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3053 - sparse_categorical_accuracy: 0.9019\n",
      "Epoch 7/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3125 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 8/10\n",
      "1469/1469 [==============================] - 23s 15ms/step - loss: 0.3117 - sparse_categorical_accuracy: 0.9008\n",
      "Epoch 9/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3099 - sparse_categorical_accuracy: 0.9004\n",
      "Epoch 10/10\n",
      "1469/1469 [==============================] - 22s 15ms/step - loss: 0.3130 - sparse_categorical_accuracy: 0.9002\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7086 - sparse_categorical_accuracy: 0.7903\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7903000116348267\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "(3868, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "192/192 [==============================] - 13s 69ms/step - loss: 3.7802\n",
      "Epoch 2/10\n",
      "192/192 [==============================] - 14s 71ms/step - loss: 3.7484\n",
      "Epoch 3/10\n",
      "192/192 [==============================] - 12s 64ms/step - loss: 3.7021\n",
      "Epoch 4/10\n",
      "192/192 [==============================] - 12s 65ms/step - loss: 3.6959\n",
      "Epoch 5/10\n",
      "192/192 [==============================] - 12s 63ms/step - loss: 3.6833\n",
      "Epoch 6/10\n",
      "192/192 [==============================] - 12s 64ms/step - loss: 3.6582\n",
      "Epoch 7/10\n",
      "192/192 [==============================] - 14s 71ms/step - loss: 3.6480\n",
      "Epoch 8/10\n",
      "192/192 [==============================] - 12s 64ms/step - loss: 3.6314\n",
      "Epoch 9/10\n",
      "192/192 [==============================] - 12s 64ms/step - loss: 3.6100\n",
      "Epoch 10/10\n",
      "192/192 [==============================] - 12s 65ms/step - loss: 3.6017\n",
      "Epoch 1/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2225 - sparse_categorical_accuracy: 0.9331\n",
      "Epoch 2/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2151 - sparse_categorical_accuracy: 0.9338\n",
      "Epoch 3/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2148 - sparse_categorical_accuracy: 0.9340\n",
      "Epoch 4/10\n",
      "1532/1532 [==============================] - 22s 15ms/step - loss: 0.2109 - sparse_categorical_accuracy: 0.9358\n",
      "Epoch 5/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2041 - sparse_categorical_accuracy: 0.9358\n",
      "Epoch 6/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2013 - sparse_categorical_accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2006 - sparse_categorical_accuracy: 0.9384\n",
      "Epoch 8/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.1989 - sparse_categorical_accuracy: 0.9381\n",
      "Epoch 9/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.9374\n",
      "Epoch 10/10\n",
      "1532/1532 [==============================] - 23s 15ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9363\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.7872 - sparse_categorical_accuracy: 0.8036\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.803600013256073\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "(2607, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 14s 69ms/step - loss: 3.5952\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 14s 68ms/step - loss: 3.5712\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 3.5689\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 3.5504\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 14s 70ms/step - loss: 3.5473\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 3.5371\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 13s 67ms/step - loss: 3.5163\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 3.5182\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 13s 64ms/step - loss: 3.5017\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 13s 65ms/step - loss: 3.4952\n",
      "Epoch 1/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1558 - sparse_categorical_accuracy: 0.9541\n",
      "Epoch 2/10\n",
      "1594/1594 [==============================] - 23s 14ms/step - loss: 0.1400 - sparse_categorical_accuracy: 0.9572\n",
      "Epoch 3/10\n",
      "1594/1594 [==============================] - 23s 14ms/step - loss: 0.1378 - sparse_categorical_accuracy: 0.9577\n",
      "Epoch 4/10\n",
      "1594/1594 [==============================] - 23s 15ms/step - loss: 0.1388 - sparse_categorical_accuracy: 0.9589\n",
      "Epoch 5/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1328 - sparse_categorical_accuracy: 0.9593\n",
      "Epoch 6/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1386 - sparse_categorical_accuracy: 0.9585\n",
      "Epoch 7/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1424 - sparse_categorical_accuracy: 0.9571\n",
      "Epoch 8/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1368 - sparse_categorical_accuracy: 0.9583\n",
      "Epoch 9/10\n",
      "1594/1594 [==============================] - 23s 15ms/step - loss: 0.1366 - sparse_categorical_accuracy: 0.9587\n",
      "Epoch 10/10\n",
      "1594/1594 [==============================] - 24s 15ms/step - loss: 0.1359 - sparse_categorical_accuracy: 0.9592\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.9107 - sparse_categorical_accuracy: 0.8096\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8095999956130981\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "(1703, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "208/208 [==============================] - 14s 66ms/step - loss: 3.5002\n",
      "Epoch 2/10\n",
      "208/208 [==============================] - 14s 67ms/step - loss: 3.5095\n",
      "Epoch 3/10\n",
      "208/208 [==============================] - 13s 65ms/step - loss: 3.4865\n",
      "Epoch 4/10\n",
      "208/208 [==============================] - 14s 67ms/step - loss: 3.4744\n",
      "Epoch 5/10\n",
      "208/208 [==============================] - 13s 64ms/step - loss: 3.4756\n",
      "Epoch 6/10\n",
      "208/208 [==============================] - 13s 64ms/step - loss: 3.4611\n",
      "Epoch 7/10\n",
      "208/208 [==============================] - 13s 65ms/step - loss: 3.4558\n",
      "Epoch 8/10\n",
      "208/208 [==============================] - 13s 65ms/step - loss: 3.4545\n",
      "Epoch 9/10\n",
      "208/208 [==============================] - 14s 66ms/step - loss: 3.4403\n",
      "Epoch 10/10\n",
      "208/208 [==============================] - 13s 64ms/step - loss: 3.4678\n",
      "Epoch 1/10\n",
      "1657/1657 [==============================] - 25s 15ms/step - loss: 0.1283 - sparse_categorical_accuracy: 0.9638\n",
      "Epoch 2/10\n",
      "1657/1657 [==============================] - 25s 15ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9659\n",
      "Epoch 3/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1186 - sparse_categorical_accuracy: 0.9652\n",
      "Epoch 4/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1128 - sparse_categorical_accuracy: 0.9668\n",
      "Epoch 5/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1145 - sparse_categorical_accuracy: 0.9657\n",
      "Epoch 6/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9666\n",
      "Epoch 7/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1084 - sparse_categorical_accuracy: 0.9683\n",
      "Epoch 8/10\n",
      "1657/1657 [==============================] - 24s 15ms/step - loss: 0.1108 - sparse_categorical_accuracy: 0.9672\n",
      "Epoch 9/10\n",
      "1657/1657 [==============================] - 25s 15ms/step - loss: 0.1107 - sparse_categorical_accuracy: 0.9678\n",
      "Epoch 10/10\n",
      "1657/1657 [==============================] - 25s 15ms/step - loss: 0.1103 - sparse_categorical_accuracy: 0.9672\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.9830 - sparse_categorical_accuracy: 0.8097\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8097000122070312\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "(1025, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "215/215 [==============================] - 16s 74ms/step - loss: 3.4676\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 3.4411\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 16s 73ms/step - loss: 3.4175\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 3.4184\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 3.4267\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 3.4204\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 15s 72ms/step - loss: 3.4151\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 14s 64ms/step - loss: 3.4093\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 3.3966\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 14s 65ms/step - loss: 3.4036\n",
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0994 - sparse_categorical_accuracy: 0.9715\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0990 - sparse_categorical_accuracy: 0.9721\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0939 - sparse_categorical_accuracy: 0.9726\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0910 - sparse_categorical_accuracy: 0.9734\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.0931 - sparse_categorical_accuracy: 0.9730\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0912 - sparse_categorical_accuracy: 0.9742\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.0908 - sparse_categorical_accuracy: 0.9731\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0908 - sparse_categorical_accuracy: 0.9741\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 25s 14ms/step - loss: 0.0898 - sparse_categorical_accuracy: 0.9732\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.0935 - sparse_categorical_accuracy: 0.9728\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.1180 - sparse_categorical_accuracy: 0.8152\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8151999711990356\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "(582, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "223/223 [==============================] - 16s 72ms/step - loss: 3.4222\n",
      "Epoch 2/10\n",
      "223/223 [==============================] - 16s 73ms/step - loss: 3.4047\n",
      "Epoch 3/10\n",
      "223/223 [==============================] - 16s 71ms/step - loss: 3.4067\n",
      "Epoch 4/10\n",
      "223/223 [==============================] - 14s 64ms/step - loss: 3.3960\n",
      "Epoch 5/10\n",
      "223/223 [==============================] - 14s 62ms/step - loss: 3.4073\n",
      "Epoch 6/10\n",
      "223/223 [==============================] - 15s 67ms/step - loss: 3.3975\n",
      "Epoch 7/10\n",
      "223/223 [==============================] - 15s 66ms/step - loss: 3.3797\n",
      "Epoch 8/10\n",
      "223/223 [==============================] - 14s 65ms/step - loss: 3.3851\n",
      "Epoch 9/10\n",
      "223/223 [==============================] - 14s 63ms/step - loss: 3.3894\n",
      "Epoch 10/10\n",
      "223/223 [==============================] - 14s 64ms/step - loss: 3.3829\n",
      "Epoch 1/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0853 - sparse_categorical_accuracy: 0.9739\n",
      "Epoch 2/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0810 - sparse_categorical_accuracy: 0.9758\n",
      "Epoch 3/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0797 - sparse_categorical_accuracy: 0.9766\n",
      "Epoch 4/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0782 - sparse_categorical_accuracy: 0.9768\n",
      "Epoch 5/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0791 - sparse_categorical_accuracy: 0.9768\n",
      "Epoch 6/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0788 - sparse_categorical_accuracy: 0.9770\n",
      "Epoch 7/10\n",
      "1782/1782 [==============================] - 27s 15ms/step - loss: 0.0765 - sparse_categorical_accuracy: 0.9784\n",
      "Epoch 8/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9774\n",
      "Epoch 9/10\n",
      "1782/1782 [==============================] - 26s 15ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9775\n",
      "Epoch 10/10\n",
      "1782/1782 [==============================] - 27s 15ms/step - loss: 0.0749 - sparse_categorical_accuracy: 0.9783\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.2550 - sparse_categorical_accuracy: 0.8174\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8173999786376953\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "(297, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "231/231 [==============================] - 16s 69ms/step - loss: 3.4101\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 16s 70ms/step - loss: 3.3820\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 14s 62ms/step - loss: 3.3906\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 15s 64ms/step - loss: 3.3831\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 15s 65ms/step - loss: 3.3791\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 15s 64ms/step - loss: 3.3861\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 15s 65ms/step - loss: 3.3694\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 16s 69ms/step - loss: 3.3718\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 15s 64ms/step - loss: 3.3615\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 15s 66ms/step - loss: 3.3616\n",
      "Epoch 1/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0854 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 2/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0786 - sparse_categorical_accuracy: 0.9774\n",
      "Epoch 3/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0806 - sparse_categorical_accuracy: 0.9773\n",
      "Epoch 4/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0809 - sparse_categorical_accuracy: 0.9771\n",
      "Epoch 5/10\n",
      "1844/1844 [==============================] - 26s 14ms/step - loss: 0.0760 - sparse_categorical_accuracy: 0.9781\n",
      "Epoch 6/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9789\n",
      "Epoch 7/10\n",
      "1844/1844 [==============================] - 28s 15ms/step - loss: 0.0768 - sparse_categorical_accuracy: 0.9777\n",
      "Epoch 8/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0730 - sparse_categorical_accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0752 - sparse_categorical_accuracy: 0.9779\n",
      "Epoch 10/10\n",
      "1844/1844 [==============================] - 27s 15ms/step - loss: 0.0781 - sparse_categorical_accuracy: 0.9777\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.3092 - sparse_categorical_accuracy: 0.8202\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8202000260353088\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "(59, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n",
      "(2000,)\n",
      "Epoch 1/10\n",
      "239/239 [==============================] - 16s 68ms/step - loss: 3.3779\n",
      "Epoch 2/10\n",
      "239/239 [==============================] - 16s 68ms/step - loss: 3.3604\n",
      "Epoch 3/10\n",
      "239/239 [==============================] - 15s 64ms/step - loss: 3.3639\n",
      "Epoch 4/10\n",
      "239/239 [==============================] - 16s 69ms/step - loss: 3.3576\n",
      "Epoch 5/10\n",
      "239/239 [==============================] - 15s 64ms/step - loss: 3.3593\n",
      "Epoch 6/10\n",
      "239/239 [==============================] - 16s 69ms/step - loss: 3.3578\n",
      "Epoch 7/10\n",
      "239/239 [==============================] - 15s 63ms/step - loss: 3.3541\n",
      "Epoch 8/10\n",
      "239/239 [==============================] - 15s 64ms/step - loss: 3.3582\n",
      "Epoch 9/10\n",
      "239/239 [==============================] - 16s 65ms/step - loss: 3.3515\n",
      "Epoch 10/10\n",
      "239/239 [==============================] - 16s 65ms/step - loss: 3.3493\n",
      "Epoch 1/10\n",
      "1907/1907 [==============================] - 32s 17ms/step - loss: 0.0789 - sparse_categorical_accuracy: 0.9776\n",
      "Epoch 2/10\n",
      "1907/1907 [==============================] - 29s 15ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9792\n",
      "Epoch 3/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0696 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 4/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0721 - sparse_categorical_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0687 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 6/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0694 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 7/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0701 - sparse_categorical_accuracy: 0.9799\n",
      "Epoch 8/10\n",
      "1907/1907 [==============================] - 28s 15ms/step - loss: 0.0682 - sparse_categorical_accuracy: 0.9810\n",
      "Epoch 9/10\n",
      "1907/1907 [==============================] - 29s 15ms/step - loss: 0.0636 - sparse_categorical_accuracy: 0.9813\n",
      "Epoch 10/10\n",
      "1907/1907 [==============================] - 29s 15ms/step - loss: 0.0697 - sparse_categorical_accuracy: 0.9813\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 1.2161 - sparse_categorical_accuracy: 0.8189\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8188999891281128\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "(1, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "2000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=10.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 151\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 151\u001b[0m encoder, classifier,X_train,Y_train,x_unlab,y_unlab \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_active_learning_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_unlab\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_unlab\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 56\u001b[0m, in \u001b[0;36mtrain_active_learning_models\u001b[1;34m(encoder, classifier, X_train, Y_train, x_unlab, y_unlab, X_test, Y_test, num_iterations, num_epochs)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotated in each iter : \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(budget\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mk-means++\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ulb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m annotate_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1480\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1446\u001b[0m \n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1472\u001b[0m     X,\n\u001b[0;32m   1473\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1477\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1478\u001b[0m )\n\u001b[1;32m-> 1480\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1482\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n\u001b[0;32m   1483\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412\u001b[0m, in \u001b[0;36mKMeans._check_params_vs_input\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m-> 1412\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_n_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_algorithm \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:866\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[1;34m(self, X, default_n_init)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, default_n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;66;03m# n_clusters\u001b[39;00m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters:\n\u001b[1;32m--> 866\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    867\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be >= n_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    868\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# tol\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol \u001b[38;5;241m=\u001b[39m _tolerance(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol)\n",
      "\u001b[1;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=10."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def probability_sampling(distances, budget):\n",
    "    # Convert squared distances to probabilities using inverse\n",
    "    # since larger squared distance corresponds to smaller probability\n",
    "    probabilities = 1 / (1 + distances**2)\n",
    "\n",
    "    # Normalize probabilities to sum up to 1\n",
    "    probabilities /= np.sum(probabilities)\n",
    "\n",
    "    # Sample 'budget' initial centroids based on probabilities\n",
    "    sampled_indices = np.random.choice(len(distances), size=budget, p=probabilities)\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "  \n",
    "        try :\n",
    "            x_ulb = encoder.predict(x_unlab, batch_size=128)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 10\n",
    "        \n",
    "        budget =  200\n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        annotate_indices = []\n",
    "        \n",
    "        \n",
    "        for i in range(10):\n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            sampled_indices = probability_sampling(distances, budget)\n",
    "            annotate_indices.extend(cluster_indices[sampled_indices])\n",
    "            \n",
    "\n",
    "        print(np.shape(annotate_indices))\n",
    "\n",
    "        annt = annotate_indices      \n",
    "        \n",
    "        ante = x_ulb[annt]\n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(annt))\n",
    "        \n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train, new_annt))\n",
    "        X_train = arr\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        try :\n",
    "            history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=10)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "      \n",
    "\n",
    "        try:\n",
    "            history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "\n",
    "        try :\n",
    "            accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab\n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "       \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train, y_train),( X_test, Y_test )= cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "X_test = X_test / 255 \n",
    "\n",
    "X_train, x_unlab, Y_train, y_unlab = train_test_split( x_train,y_train, test_size=0.1, random_state=42 )\n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=10)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "\n",
    "try :\n",
    "  accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"Acc : \")\n",
    "print(\"\\n\")\n",
    "print(accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48a682c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12811f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5853d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cb7b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c7791d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591012d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0facb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1fb3d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69c8b298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67b50e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcaceb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3d9fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398211b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b8b46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a70f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68431a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b3940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90bd329c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0d773bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3fb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52762b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712a28bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bd48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fea79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b95d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
