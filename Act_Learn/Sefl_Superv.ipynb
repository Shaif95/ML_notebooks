{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea172b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 2048\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7b78a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a46715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e72bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)\n",
      "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "# Load the train and test data splits\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Display shapes of train and test datasets\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "637a4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.02),\n",
    "        layers.RandomWidth(0.2),\n",
    "        layers.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57bbb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 3)     7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_encoder():\n",
    "    resnet = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787b310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6e5be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,619,025\n",
      "Trainable params: 24,573,578\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "189/189 [==============================] - 1952s 10s/step - loss: 1.9393 - sparse_categorical_accuracy: 0.2857\n",
      "313/313 [==============================] - 23s 70ms/step - loss: 1.8147 - sparse_categorical_accuracy: 0.3608\n",
      "Test accuracy: 36.08%\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b58c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5928bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d957feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               262272    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,827,079\n",
      "Trainable params: 23,781,632\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/90\n",
      "391/391 [==============================] - 58s 137ms/step - loss: 4.5706\n",
      "Epoch 2/90\n",
      "391/391 [==============================] - 54s 139ms/step - loss: 4.3485\n",
      "Epoch 3/90\n",
      "391/391 [==============================] - 124s 317ms/step - loss: 4.2205\n",
      "Epoch 4/90\n",
      "391/391 [==============================] - 161s 413ms/step - loss: 4.1125\n",
      "Epoch 5/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 4.0204\n",
      "Epoch 6/90\n",
      "391/391 [==============================] - 161s 410ms/step - loss: 3.9452\n",
      "Epoch 7/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.8861\n",
      "Epoch 8/90\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 3.8225\n",
      "Epoch 9/90\n",
      "391/391 [==============================] - 158s 403ms/step - loss: 3.7795\n",
      "Epoch 10/90\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 3.7224\n",
      "Epoch 11/90\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 3.6848\n",
      "Epoch 12/90\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 3.6489\n",
      "Epoch 13/90\n",
      "391/391 [==============================] - 160s 408ms/step - loss: 3.6155\n",
      "Epoch 14/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.5906\n",
      "Epoch 15/90\n",
      "391/391 [==============================] - 164s 420ms/step - loss: 3.5440\n",
      "Epoch 16/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.5277\n",
      "Epoch 17/90\n",
      "391/391 [==============================] - 164s 420ms/step - loss: 3.5016\n",
      "Epoch 18/90\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 3.4765\n",
      "Epoch 19/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 3.4475\n",
      "Epoch 20/90\n",
      "391/391 [==============================] - 162s 416ms/step - loss: 3.4252\n",
      "Epoch 21/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.4077\n",
      "Epoch 22/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.3856\n",
      "Epoch 23/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.3666\n",
      "Epoch 24/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 3.3455\n",
      "Epoch 25/90\n",
      "391/391 [==============================] - 165s 421ms/step - loss: 3.3207\n",
      "Epoch 26/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.3182\n",
      "Epoch 27/90\n",
      "391/391 [==============================] - 160s 408ms/step - loss: 3.2957\n",
      "Epoch 28/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.2817\n",
      "Epoch 29/90\n",
      "391/391 [==============================] - 158s 405ms/step - loss: 3.2696\n",
      "Epoch 30/90\n",
      "391/391 [==============================] - 157s 403ms/step - loss: 3.2487\n",
      "Epoch 31/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.2311\n",
      "Epoch 32/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.2175\n",
      "Epoch 33/90\n",
      "391/391 [==============================] - 159s 407ms/step - loss: 3.2004\n",
      "Epoch 34/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.2004\n",
      "Epoch 35/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 3.1764\n",
      "Epoch 36/90\n",
      "391/391 [==============================] - 161s 411ms/step - loss: 3.1677\n",
      "Epoch 37/90\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 3.1508\n",
      "Epoch 38/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.1494\n",
      "Epoch 39/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 3.1306\n",
      "Epoch 40/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.1199\n",
      "Epoch 41/90\n",
      "391/391 [==============================] - 163s 417ms/step - loss: 3.1116\n",
      "Epoch 42/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 3.1058\n",
      "Epoch 43/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.0889\n",
      "Epoch 44/90\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 3.0790\n",
      "Epoch 45/90\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 3.0706\n",
      "Epoch 46/90\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 3.0712\n",
      "Epoch 47/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 3.0524\n",
      "Epoch 48/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 3.0483\n",
      "Epoch 49/90\n",
      "391/391 [==============================] - 164s 419ms/step - loss: 3.0351\n",
      "Epoch 50/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 3.0300\n",
      "Epoch 51/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.0193\n",
      "Epoch 52/90\n",
      "391/391 [==============================] - 166s 425ms/step - loss: 3.0156\n",
      "Epoch 53/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 3.0007\n",
      "Epoch 54/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 3.0032\n",
      "Epoch 55/90\n",
      "391/391 [==============================] - 164s 419ms/step - loss: 2.9866\n",
      "Epoch 56/90\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 2.9979\n",
      "Epoch 57/90\n",
      "391/391 [==============================] - 159s 405ms/step - loss: 2.9866\n",
      "Epoch 58/90\n",
      "391/391 [==============================] - 163s 418ms/step - loss: 2.9685\n",
      "Epoch 59/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 2.9679\n",
      "Epoch 60/90\n",
      "391/391 [==============================] - 160s 409ms/step - loss: 2.9544\n",
      "Epoch 61/90\n",
      "391/391 [==============================] - 164s 419ms/step - loss: 2.9506\n",
      "Epoch 62/90\n",
      "391/391 [==============================] - 164s 420ms/step - loss: 2.9525\n",
      "Epoch 63/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 2.9393\n",
      "Epoch 64/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 2.9344\n",
      "Epoch 65/90\n",
      "391/391 [==============================] - 161s 410ms/step - loss: 2.9283\n",
      "Epoch 66/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 2.9291\n",
      "Epoch 67/90\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 2.9148\n",
      "Epoch 68/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 2.9165\n",
      "Epoch 69/90\n",
      "391/391 [==============================] - 164s 418ms/step - loss: 2.9149\n",
      "Epoch 70/90\n",
      "391/391 [==============================] - 161s 412ms/step - loss: 2.9076\n",
      "Epoch 71/90\n",
      "391/391 [==============================] - 159s 407ms/step - loss: 2.8848\n",
      "Epoch 72/90\n",
      "391/391 [==============================] - 162s 415ms/step - loss: 2.8942\n",
      "Epoch 73/90\n",
      "391/391 [==============================] - 161s 413ms/step - loss: 2.8911\n",
      "Epoch 74/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 2.8796\n",
      "Epoch 75/90\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 2.8804\n",
      "Epoch 76/90\n",
      "391/391 [==============================] - 163s 417ms/step - loss: 2.8736\n",
      "Epoch 77/90\n",
      "391/391 [==============================] - 162s 414ms/step - loss: 2.8670\n",
      "Epoch 78/90\n",
      "391/391 [==============================] - 163s 416ms/step - loss: 2.8724\n",
      "Epoch 79/90\n",
      "391/391 [==============================] - 159s 407ms/step - loss: 2.8692\n",
      "Epoch 80/90\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 2.8498\n",
      "Epoch 81/90\n",
      "391/391 [==============================] - 12466s 32s/step - loss: 2.8617\n",
      "Epoch 82/90\n",
      "391/391 [==============================] - 49s 126ms/step - loss: 2.8517\n",
      "Epoch 83/90\n",
      "391/391 [==============================] - 51s 129ms/step - loss: 2.8504\n",
      "Epoch 84/90\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 2.8511\n",
      "Epoch 85/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 116s 297ms/step - loss: 2.8379\n",
      "Epoch 86/90\n",
      "391/391 [==============================] - 160s 410ms/step - loss: 2.8397\n",
      "Epoch 87/90\n",
      "391/391 [==============================] - 172s 440ms/step - loss: 2.8313\n",
      "Epoch 88/90\n",
      "391/391 [==============================] - 170s 435ms/step - loss: 2.8349\n",
      "Epoch 89/90\n",
      "391/391 [==============================] - 182s 466ms/step - loss: 2.8294\n",
      "Epoch 90/90\n",
      "391/391 [==============================] - 178s 454ms/step - loss: 2.8313\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=x_train, y=y_train, batch_size=batch_size, epochs=90\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684924cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90\n",
      "391/391 [==============================] - 80s 164ms/step - loss: 0.2148 - sparse_categorical_accuracy: 0.9453\n",
      "Epoch 2/90\n",
      "391/391 [==============================] - 58s 147ms/step - loss: 0.1796 - sparse_categorical_accuracy: 0.9494\n",
      "Epoch 3/90\n",
      "391/391 [==============================] - 52s 133ms/step - loss: 0.1647 - sparse_categorical_accuracy: 0.9527\n",
      "Epoch 4/90\n",
      "391/391 [==============================] - 56s 143ms/step - loss: 0.1692 - sparse_categorical_accuracy: 0.9512\n",
      "Epoch 5/90\n",
      "391/391 [==============================] - 55s 140ms/step - loss: 0.1652 - sparse_categorical_accuracy: 0.9516\n",
      "Epoch 6/90\n",
      "391/391 [==============================] - 56s 142ms/step - loss: 0.1581 - sparse_categorical_accuracy: 0.9528\n",
      "Epoch 7/90\n",
      "368/391 [===========================>..] - ETA: 2s - loss: 0.1596 - sparse_categorical_accuracy: 0.9527"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False)\n",
    "\n",
    "history = classifier.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=90)\n",
    "\n",
    "accuracy = classifier.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier2 = create_classifier(encoder)\n",
    "classifier2.summary()\n",
    "\n",
    "history = classifier2.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=10)\n",
    "\n",
    "accuracy = classifier2.evaluate(x_test, y_test)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ba347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ac229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4f8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55536b60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
