{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13beafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R\n",
      "From (redirected): https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R&confirm=t&uuid=358c192f-2ba0-44e9-81d7-bd3d9a054de6\n",
      "To: F:\\ML_notebooks\\z.Gen_vs_Contrastive\\Contrastive\\semi_super_augPipe.py\n",
      "\n",
      "  0%|          | 0.00/12.2k [00:00<?, ?B/s]\n",
      "100%|##########| 12.2k/12.2k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "! gdown https://drive.google.com/uc?id=1pGDcwitiFXy-CY9vO3wH3XprwjWgsQ8R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10b5795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    " \n",
    "# Distribute it to train and test set\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbad7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name :  MobileNetV1_L2_10_6_warmUp_BYOL_batchNorm_colorJ_0.1_op_1024_random_max_pool_minObjCov_0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version :  2.8.0\n"
     ]
    }
   ],
   "source": [
    "IMG_H, IMG_W = (32,32)\n",
    "input_shape = (IMG_H,IMG_W,3)\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "# The reason to reduce the aggresive nature of random crop is to get the full picture idea & not the background\n",
    "min_obj_cov_value = 0.7\n",
    "color_jitter_value = 0.1\n",
    "\n",
    "# Model Name for reference\n",
    "modelNameStr = \"MobileNetV1_L2_10_6_warmUp_BYOL_batchNorm_colorJ_\" + str(color_jitter_value) + \"_op_1024_random_max_pool_minObjCov_\"+str(min_obj_cov_value)\n",
    "print(\"Model Name : \",modelNameStr)\n",
    "\n",
    "# Basic Imports\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import functools\n",
    "from semi_super_augPipe import preprocess_image\n",
    "from typing import Callable\n",
    "import shutil\n",
    "\n",
    "# Loss plot\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import random \n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Random seed fixation for experiment result repitition\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# Directory Setup\n",
    "import os\n",
    "import tempfile\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "print(\"Tensorflow version : \",tf.__version__)\n",
    "\n",
    "# In-order run function decorators in tf2.0\n",
    "tf.config.run_functions_eagerly(False)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace1d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing utils\n",
    "@tf.function\n",
    "def train_parse_images(image):\n",
    "\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # IMG_H & IMG_W are constants that will be saved in the tf.graph during the tracing step (i.e. the first call)\n",
    "    image = tf.image.resize(image, size=[IMG_H, IMG_W])\n",
    "\n",
    "    aug_image_1 = preprocess_image(image = image, \n",
    "                                   height = IMG_H, \n",
    "                                   width  = IMG_W, \n",
    "                                   cjs = color_jitter_value,\n",
    "                                   m_obj_cov = min_obj_cov_value,\n",
    "                                   a_range = (min_obj_cov_value, 1.0))\n",
    "  \n",
    "    \n",
    "    aug_image_2 = preprocess_image(image = image, \n",
    "                                   height = IMG_H, \n",
    "                                   width  = IMG_W, \n",
    "                                   cjs = color_jitter_value,\n",
    "                                   m_obj_cov = min_obj_cov_value,\n",
    "                                   a_range = (min_obj_cov_value, 1.0))\n",
    "    \n",
    "    return aug_image_1, aug_image_2\n",
    "train_tensor = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds_shuffled = train_tensor.shuffle(len(train_tensor))\n",
    "\n",
    "final_train_ds = (train_ds_shuffled\n",
    "                .map(train_parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9255ff04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding l2 reg\n",
    "def add_regularization(model, regularizer = tf.keras.regularizers.l2(0.0001)):\n",
    "    \"\"\"\n",
    "    Function to add l2 regularisation penalty to each layer of a built in keras model.\n",
    "    Args:\n",
    "        model (tf.keras.models) : Input model whose layer's you want to regularize\n",
    "        regularizer ( tf.keras.regularizers.l2) : object indicating l2 penalty to be applied.\n",
    "    \n",
    "    Returns:\n",
    "        l2 regularised keras model\n",
    "    \"\"\"\n",
    "\n",
    "    # Regularizer check \n",
    "    if not isinstance(regularizer, tf.keras.regularizers.Regularizer):\n",
    "        print(\"Regularizer must be a subclass of tf.keras.regularizers.Regularizer\")\n",
    "        return model\n",
    "    \n",
    "    # Apply l2 reg in each layer in which we can apply the penalty\n",
    "    for layer in model.layers:\n",
    "        for attr in ['kernel_regularizer']:\n",
    "            if hasattr(layer, attr):\n",
    "                setattr(layer, attr, regularizer)\n",
    "\n",
    "    # When we change the layers attributes, the change only happens in the model config file\n",
    "    model_json = model.to_json()\n",
    "    \n",
    "    # Save the weights before reloading the model.\n",
    "    # Stragtegy helps us to save pretrained weights if the passed input model has any\n",
    "    tmp_weights_path = os.path.join(tempfile.gettempdir(), 'tmp_weights.h5')\n",
    "    model.save_weights(tmp_weights_path)\n",
    "\n",
    "    # load the model from the config\n",
    "    model = tf.keras.models.model_from_json(model_json)\n",
    "    \n",
    "    # Reload the model weights\n",
    "    model.load_weights(tmp_weights_path, by_name=True)\n",
    "    return model\n",
    "def predictor(input_dim, \n",
    "              l2_reg_penalty = 1.5*10e-6, \n",
    "              hid_1 = 512, \n",
    "              hid_2 = 256):\n",
    "    \"\"\"\n",
    "    Function to build the predictor newtork on top of one the arms of the BYOL framework\n",
    "    Args:\n",
    "        input_dim (int) : Input vector dimensionality\n",
    "        l2_reg_penalty (float) : L2 penalty to be applied.\n",
    "        hid_1, hid_2 (int) : Hidden layer dimensionality of the predictor n/w\n",
    "    Returns:\n",
    "        The keras model of the network\n",
    "    \"\"\"\n",
    "  \n",
    "    regularizer = tf.keras.regularizers.l2(l2_reg_penalty)\n",
    "    \n",
    "    inputs = Input(input_dim)\n",
    "  \n",
    "    # Non linear prediction layer for the task to increasing the mutual information between 2 views\n",
    "    prediction_1 = Dense(hid_1, kernel_regularizer = regularizer)(inputs)\n",
    "    prediction_1 = tf.keras.layers.BatchNormalization()(prediction_1)\n",
    "    prediction_1 = Activation(\"relu\")(prediction_1)\n",
    "    prediction_2 = Dense(hid_2, kernel_regularizer = regularizer)(prediction_1)\n",
    "\n",
    "    return Model(inputs,prediction_2)\n",
    "\n",
    "def encoder_projector(l2_reg_penalty = 1.5*10e-6, \n",
    "                      input_shape = (224,224,3), \n",
    "                      hid_1 = 512, \n",
    "                      hid_2 = 256):\n",
    "    \"\"\"\n",
    "    Function to build the symmetric part of (online & offline n/w) the BYOL f/w\n",
    "    Args:\n",
    "        l2_reg_penalty (float) : L2 penalty to be applied.\n",
    "        input_shape (tuple) : input image shape of the model.\n",
    "        hid_1, hid_2 (int) : Hidden layer dimensionality of the projector n/w only\n",
    "    \"\"\"\n",
    "\n",
    "    # None itself is random initalisation that ensures on each \n",
    "    # call the model gets alloted different random weights.\n",
    "    # Both the online & target branch need to alloted different random weights.\n",
    "    base_model = tf.keras.applications.MobileNet(include_top = False, \n",
    "                                                weights = None, \n",
    "                                                input_shape = input_shape)\n",
    "  \n",
    "    regularizer = tf.keras.regularizers.l2(l2_reg_penalty)\n",
    "\n",
    "    # If we use imagenet weights on a pretrained model \n",
    "    # we have to use this helper to add l2 penalty in each layer\n",
    "    reg_base_model = add_regularization(base_model, regularizer)\n",
    "    reg_base_model.trainable = True\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "    h = reg_base_model(inputs, training=True)\n",
    "    embed_vector = GlobalMaxPooling2D()(h)\n",
    "\n",
    "    # since architecturally projector & predictor are the same\n",
    "    projector_model = predictor(embed_vector.shape[-1], hid_1 = hid_1, hid_2 = hid_2)\n",
    "\n",
    "    op = projector_model(embed_vector)\n",
    "    enc_prec = Model(inputs, op)\n",
    "    return enc_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0acff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'byol_logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'byol_logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "def byol_loss(p, z):\n",
    "    p = tf.math.l2_normalize(p, axis=1)  # (2*bs, 128)\n",
    "    z = tf.math.l2_normalize(z, axis=1)  # (2*bs, 128)\n",
    "\n",
    "    similarities = tf.reduce_sum(tf.multiply(p, z), axis=1)\n",
    "    return 2 - 2 * tf.reduce_mean(similarities)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x1, \n",
    "               x2,\n",
    "               online_enc_proj,\n",
    "               target_enc_proj,\n",
    "               online_predictor,\n",
    "               optimizer,\n",
    "               loss_func):\n",
    "        \n",
    "    #Forward pass\n",
    "    z_target_1 = target_enc_proj(x1, training=True)\n",
    "    z_target_2 = target_enc_proj(x2, training=True)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        z_online_1 = online_enc_proj(x1, training=True)\n",
    "        p_online_1 = online_predictor(z_online_1, training = True)\n",
    "\n",
    "        z_online_2 = online_enc_proj(x2, training=True)\n",
    "        p_online_2 = online_predictor(z_online_2, training = True)\n",
    "\n",
    "        # Trying to make the loss symmetric in nature\n",
    "        p_online = tf.concat([p_online_1, p_online_2], axis=0)\n",
    "        z_target = tf.concat([z_target_2, z_target_1], axis=0)\n",
    "        loss = loss_func(p_online, z_target)\n",
    "\n",
    "    # Backward pass (update online networks)\n",
    "    # encoder + projector update\n",
    "    grads_1 = tape.gradient(loss, online_enc_proj.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads_1, online_enc_proj.trainable_variables))\n",
    " \n",
    "    # predictor update\n",
    "    grads_2 = tape.gradient(loss, online_predictor.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads_2, online_predictor.trainable_variables))\n",
    "    del tape\n",
    "\n",
    "    return loss, grads_1\n",
    "\n",
    "def train_byol(online_enc_proj,\n",
    "               online_predictor,\n",
    "               target_enc_proj, \n",
    "               train_dataset, \n",
    "               optimizer, \n",
    "               criterion,\n",
    "               lr_epoch,\n",
    "               beta_base = 0.996,\n",
    "               epochs=100,\n",
    "               num_train_samples_viz = 5,\n",
    "               num_test_samples_viz = 2):\n",
    "    \n",
    "    #################################################\n",
    "    # Setting up epoch steps & time taken  \n",
    "    t_start = time.time()\n",
    "    epoch_wise_loss = []\n",
    "    print(\"Number of steps : \",len(train_dataset))\n",
    "    \n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        \n",
    "        cnt = 0\n",
    "        \n",
    "        # Reset loss collection each step\n",
    "        step_wise_loss = []\n",
    "        num_train_steps = len(train_dataset) \n",
    "        \n",
    "        # Tensor board visualisations \n",
    "        random_batches_train = random.sample(range(len(train_dataset)),num_train_samples_viz)\n",
    "        cnt = 0\n",
    "        random_collection_train_sample_1 = []\n",
    "        random_collection_train_sample_2 = []\n",
    "        gradArray = None\n",
    "        loss = None \n",
    "\n",
    "        #################################################\n",
    "        # Train STEP\n",
    "        for step, image_batch in enumerate(tqdm(train_dataset)):\n",
    "            a = image_batch[0]\n",
    "            b = image_batch[1]\n",
    "\n",
    "            loss, gradArray = train_step(a, \n",
    "                                         b, \n",
    "                                         online_enc_proj, \n",
    "                                         target_enc_proj, \n",
    "                                         online_predictor, \n",
    "                                         optimizer, \n",
    "                                         criterion)\n",
    "            step_wise_loss.append(loss)\n",
    "\n",
    "            # slow update of the target n/w\n",
    "            f_target_weights = target_enc_proj.get_weights()\n",
    "            f_online_weights = online_enc_proj.get_weights()\n",
    "\n",
    "            # Beta weight uodates ....\n",
    "            beta_final =  1 - (1 - beta_base)*((np.cos((np.pi*(step+1))/(num_train_steps)) + 1)/2)\n",
    "            for i in range(len(f_online_weights)):\n",
    "                f_target_weights[i] = beta_final * f_target_weights[i] + (1 - beta_final) * f_online_weights[i]\n",
    "            \n",
    "            # Setting updates weights .....\n",
    "            target_enc_proj.set_weights(f_target_weights)\n",
    "\n",
    "\n",
    "            # Logging images for tensorboard:\n",
    "            if cnt in random_batches_train:\n",
    "                random_collection_train_sample_1.append(image_batch[0][0])\n",
    "                random_collection_train_sample_2.append(image_batch[1][0])\n",
    "            cnt+=1\n",
    "                \n",
    "        # Average loss throughout the whole process\n",
    "        if not len(epoch_wise_loss):\n",
    "            epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "        else:\n",
    "            # Adding the mean of previous ones\n",
    "            mean_value = (np.sum(step_wise_loss) + epoch_wise_loss[-1]*(epoch)*num_train_steps)/((epoch+1)*num_train_steps)\n",
    "            epoch_wise_loss.append(mean_value)\n",
    "        \n",
    "      \n",
    "        #################################################\n",
    "        # Lr tracking\n",
    "        lr_epoch.append(optimizer._decayed_lr(tf.float32).numpy())\n",
    "        \n",
    "        #################################################\n",
    "        # Console progress\n",
    "        print(\"\\n epoch: {} | train loss: {:.8f} | lr : {} | {:.4f} mins\"\n",
    "              .format(epoch + 1,\n",
    "                      epoch_wise_loss[-1],\n",
    "                      optimizer._decayed_lr(tf.float32).numpy(), \n",
    "                      (time.time()-t_start)/60.0))\n",
    "        \n",
    "        #################################################\n",
    "        # Model Save, you can add an heuristic to save if you want\n",
    "        if not os.path.isdir(modelNameStr):\n",
    "            os.mkdir(modelNameStr)\n",
    "                \n",
    "        online_enc_proj.save(\"./\" + modelNameStr + \"/online_encoder_projector\" + \".h5\")\n",
    "        online_predictor.save(\"./\" + modelNameStr + \"/online_predictor\" +\".h5\")\n",
    "        target_enc_proj.save(\"./\" + modelNameStr + \"/target_encoder_projector\" +\".h5\")\n",
    "        \n",
    "        #################################################\n",
    "        # Tensorboard logging\n",
    "        tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', epoch_wise_loss[-1], step = epoch)\n",
    "            tf.summary.scalar('learningRate', lr_epoch[-1], step = epoch)\n",
    "            tf.summary.image('View 1', random_collection_train_sample_1, step = epoch)\n",
    "            tf.summary.image('View 2', random_collection_train_sample_2, step = epoch)\n",
    "          \n",
    "            # global variable defined later\n",
    "            # Observe the gradients distributions ..\n",
    "            for name in layer_names:\n",
    "                tf.summary.histogram(name + \"_gradients\", gradArray[layer_to_index[name]])\n",
    "                \n",
    "            # Observe model weights \n",
    "            for layer in online_enc_proj.layers:\n",
    "                for tl in layer.trainable_weights:\n",
    "                    if tl.name in layer_names:\n",
    "                        tf.summary.histogram(tl.name+\"_weights\", tl.numpy())\n",
    "        gc.collect()\n",
    "\n",
    "    return epoch_wise_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1ee973e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV1_L2_10_6_warmUp_BYOL_batchNorm_colorJ_0.1_op_1024_random_max_pool_minObjCov_0.7\n",
      "Decay Steps :  390000\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " mobilenet_1.00_32 (Function  (None, 1, 1, 1024)       3228864   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_max_pooling2d_2 (Glo  (None, 1024)             0         \n",
      " balMaxPooling2D)                                                \n",
      "                                                                 \n",
      " model_5 (Functional)        (None, 256)               658176    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,887,040\n",
      "Trainable params: 3,864,128\n",
      "Non-trainable params: 22,912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "               initial_learning_rate: float,\n",
    "               decay_schedule_fn: Callable,\n",
    "               warmup_steps: int,\n",
    "               power: float = 1.0,\n",
    "               name: str = None,):\n",
    "\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.power = power\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"WarmUp\") as name:\n",
    "            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n",
    "            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "            global_step_float = tf.cast(step, tf.float32)\n",
    "            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_percent_done = global_step_float / warmup_steps_float\n",
    "            warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n",
    "            return tf.cond(\n",
    "                global_step_float < warmup_steps_float,\n",
    "                lambda: warmup_learning_rate,\n",
    "                lambda: self.decay_schedule_fn(step - self.warmup_steps),\n",
    "                name=name,\n",
    "            )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"initial_learning_rate\": self.initial_learning_rate,\n",
    "                \"decay_schedule_fn\": self.decay_schedule_fn,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"power\": self.power,\n",
    "                \"name\": self.name,\n",
    "                }\n",
    "    \n",
    "criterion = byol_loss\n",
    "decay_steps = (len(final_train_ds))*1000\n",
    "warmup_steps = (len(final_train_ds))*10\n",
    "print(modelNameStr)\n",
    "saving_model_weights = \"\"\n",
    "l2_reg_penalty = 1.5*10e-6\n",
    "\n",
    "# parameters for the hidden layer of projector & predictior (scaled down for fast execution)\n",
    "hidden_layer_1 = 512 \n",
    "hidden_layer_2 = 256   \n",
    "\n",
    "initial_lr = 5e-4 # base lr\n",
    "\n",
    "\n",
    "online_enc_proj  = encoder_projector(l2_reg_penalty = l2_reg_penalty, input_shape = (IMG_H,IMG_W,3), hid_1 = hidden_layer_1, hid_2 = hidden_layer_2)\n",
    "target_enc_proj  = encoder_projector(l2_reg_penalty = l2_reg_penalty, input_shape = (IMG_H,IMG_W,3), hid_1 = hidden_layer_1, hid_2 = hidden_layer_2)\n",
    "online_predictor = predictor(input_dim = hidden_layer_2, l2_reg_penalty = l2_reg_penalty, hid_1 = hidden_layer_1, hid_2 = hidden_layer_2)\n",
    "\n",
    "epoch_wise_loss = []\n",
    "valid_epoch_loss = []\n",
    "lr_epoch = []\n",
    "\n",
    "\n",
    "print(\"Decay Steps : \",decay_steps)\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = initial_lr, \n",
    "                                                decay_steps = decay_steps)\n",
    "\n",
    "cosine_with_warmUp = WarmUp(initial_learning_rate = initial_lr,\n",
    "                          decay_schedule_fn = lr_decayed_fn,\n",
    "                          warmup_steps = warmup_steps)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(cosine_with_warmUp)\n",
    "\n",
    "online_enc_proj.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e19bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of steps :  390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 390/390 [02:22<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " epoch: 1 | train loss: 1.23066950 | lr : 0.00010000001202570274 | 2.3788 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2306695]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "layer_names = [\"conv_pw_12/kernel:0\", \"conv_pw_13/kernel:0\"]\n",
    "index_to_layer = {}\n",
    "for i in online_enc_proj.layers:\n",
    "    for j in i.trainable_weights:\n",
    "        index_to_layer[cnt] = j.name\n",
    "        cnt+=1\n",
    "layer_to_index = {j:i for i,j in index_to_layer.items()}\n",
    "\n",
    "train_byol(online_enc_proj,\n",
    "           online_predictor,\n",
    "           target_enc_proj,\n",
    "           final_train_ds,\n",
    "           optimizer,\n",
    "           criterion,\n",
    "           lr_epoch,\n",
    "           epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fd8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2a3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5b1418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71be8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320e6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
