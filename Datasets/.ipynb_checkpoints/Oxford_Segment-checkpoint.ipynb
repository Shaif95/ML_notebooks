{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a475f3bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\google\\protobuf\\json_format.py:544\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[1;34m(self, js, message, path)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 544\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\n\u001b[0;32m    545\u001b[0m       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has no field named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    546\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Available Fields(except extensions): \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    547\u001b[0m            message_descriptor\u001b[38;5;241m.\u001b[39mfull_name, name, path,\n\u001b[0;32m    548\u001b[0m            [f\u001b[38;5;241m.\u001b[39mjson_name \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m message_descriptor\u001b[38;5;241m.\u001b[39mfields]))\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n",
      "\u001b[1;31mParseError\u001b[0m: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mParseError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load Oxford Pets dataset from TFDS\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m (train_ds, test_ds), info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moxford_iiit_pet:3.*.*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Define preprocessing functions\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(data):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:330\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m   builder_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 330\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_gcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_gcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    332\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:168\u001b[0m, in \u001b[0;36mbuilder\u001b[1;34m(name, try_gcs, **builder_kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _try_load_from_files_first(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs):\n\u001b[0;32m    167\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[43mread_only_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n\u001b[0;32m    170\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m registered\u001b[38;5;241m.\u001b[39mDatasetNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\read_only_builder.py:162\u001b[0m, in \u001b[0;36mbuilder_from_files\u001b[1;34m(name, **builder_kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m builder_dir \u001b[38;5;241m=\u001b[39m _find_builder_dir(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuilder_kwargs)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# A generated dataset was found on disk\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m   data_dirs \u001b[38;5;241m=\u001b[39m constants\u001b[38;5;241m.\u001b[39mlist_data_dirs(\n\u001b[0;32m    165\u001b[0m       given_data_dir\u001b[38;5;241m=\u001b[39mbuilder_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    166\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\read_only_builder.py:131\u001b[0m, in \u001b[0;36mbuilder_from_directory\u001b[1;34m(builder_dir)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuilder_from_directory\u001b[39m(builder_dir: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dataset_builder\u001b[38;5;241m.\u001b[39mDatasetBuilder:\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a `tfds.core.DatasetBuilder` from the given generated dataset path.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m  This function reconstruct the `tfds.core.DatasetBuilder` without\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    builder: `tf.core.DatasetBuilder`, builder for dataset at the given path.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mReadOnlyBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilder_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuilder_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\read_only_builder.py:57\u001b[0m, in \u001b[0;36mReadOnlyBuilder.__init__\u001b[1;34m(self, builder_dir)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m     53\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not load `ReadOnlyBuilder`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exists.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     54\u001b[0m   )\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Restore name, config, info\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m info_proto \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m info_proto\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mVERSION \u001b[38;5;241m=\u001b[39m version_lib\u001b[38;5;241m.\u001b[39mVersion(info_proto\u001b[38;5;241m.\u001b[39mversion)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_info.py:558\u001b[0m, in \u001b[0;36mread_from_json\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    556\u001b[0m json_str \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mas_path(path)\u001b[38;5;241m.\u001b[39mread_text()\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Parse it back into a proto.\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m parsed_proto \u001b[38;5;241m=\u001b[39m \u001b[43mjson_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_info_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDatasetInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed_proto\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\google\\protobuf\\json_format.py:436\u001b[0m, in \u001b[0;36mParse\u001b[1;34m(text, message, ignore_unknown_fields, descriptor_pool, max_recursion_depth)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    435\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to load JSON: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(e)))\n\u001b[1;32m--> 436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParseDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_unknown_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptor_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_recursion_depth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\google\\protobuf\\json_format.py:461\u001b[0m, in \u001b[0;36mParseDict\u001b[1;34m(js_dict, message, ignore_unknown_fields, descriptor_pool, max_recursion_depth)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parses a JSON dictionary representation into a message.\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m  The same message passed as argument.\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m parser \u001b[38;5;241m=\u001b[39m _Parser(ignore_unknown_fields, descriptor_pool, max_recursion_depth)\n\u001b[1;32m--> 461\u001b[0m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvertMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\google\\protobuf\\json_format.py:502\u001b[0m, in \u001b[0;36m_Parser.ConvertMessage\u001b[1;34m(self, value, message, path)\u001b[0m\n\u001b[0;32m    500\u001b[0m   methodcaller(_WKTJSONMETHODS[full_name][\u001b[38;5;241m1\u001b[39m], value, message, path)(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 502\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ConvertFieldValuePair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursion_depth \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\google\\protobuf\\json_format.py:629\u001b[0m, in \u001b[0;36m_Parser._ConvertFieldValuePair\u001b[1;34m(self, js, message, path)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n\u001b[0;32m    628\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ParseError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to parse \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m field: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, e))\n",
      "\u001b[1;31mParseError\u001b[0m: Message type \"tensorflow_datasets.DatasetInfo\" has no field named \"fileFormat\" at \"DatasetInfo\".\n Available Fields(except extensions): \"['name', 'description', 'version', 'configName', 'configDescription', 'citation', 'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums', 'schema', 'splits', 'supervisedKeys', 'redistributionInfo', 'moduleName', 'disableShuffling']\""
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Load Oxford Pets dataset from TFDS\n",
    "(train_ds, test_ds), info = tfds.load('oxford_iiit_pet:3.*.*', split=['train', 'test'], with_info=True)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def preprocess_image(data):\n",
    "    image = tf.image.resize(data['image'], (32, 32))  # Resize image\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    label = tf.image.resize(data['segmentation_mask'], (32, 32))  # Resize label\n",
    "    label = tf.cast(label, tf.int32)  # Ensure labels are integers\n",
    "    \n",
    "    # Convert labels to one-hot categorical\n",
    "    num_classes = info.features['label'].num_classes\n",
    "    label = tf.one_hot(label, depth=num_classes)\n",
    "    \n",
    "    # Reshape labels\n",
    "    label = tf.reshape(label, (32, 32, num_classes))  # Reshape to (32, 32, num_classes)\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350eb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Label shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d1c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9744cafa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Adjust input shape as per your dataset\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_classes\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m create_segmentation_model(input_shape, num_classes)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'info' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_segmentation_model(input_shape, num_classes):\n",
    "    # Load ResNet50 backbone\n",
    "    resnet50_base = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Add segmentation-specific layers on top of ResNet50\n",
    "    x = resnet50_base.output\n",
    "    x = tf.keras.layers.UpSampling2D(size=(32, 32))(x)  # Upsample to match original image size\n",
    "    x = tf.keras.layers.Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(x)\n",
    "\n",
    "    # Create segmentation model\n",
    "    model = tf.keras.models.Model(inputs=resnet50_base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = (32, 32, 3)  # Adjust input shape as per your dataset\n",
    "num_classes = info.features['label'].num_classes\n",
    "model = create_segmentation_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5c0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=10, monitor='loss', verbose=1)\n",
    "model.fit(train_ds.batch(32), epochs=100,callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eaeab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0541199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(test_ds.batch(32))\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ad4de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f67d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "\n",
    "def create_segmentation_model(input_shape, num_classes):\n",
    "    # Load ResNet50 backbone\n",
    "    resnet50_base = keras.models.load_model(r\"F:\\Pre-Trained_Models\\Pre-Trained_Models\\oxf_32_model.h5\")\n",
    "    \n",
    "    # Add segmentation-specific layers on top of ResNet50\n",
    "    x = resnet50_base.layers[-4].output\n",
    "    x = tf.keras.layers.UpSampling2D(size=(32, 32))(x)  # Upsample to match original image size\n",
    "    x = tf.keras.layers.Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(x)\n",
    "\n",
    "    # Create segmentation model\n",
    "    model = tf.keras.models.Model(inputs=resnet50_base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = (32, 32, 3)  # Adjust input shape as per your dataset\n",
    "num_classes = info.features['label'].num_classes\n",
    "model = create_segmentation_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca03709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20538483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "115/115 [==============================] - 16s 89ms/step - loss: 1.0784 - accuracy: 0.5690\n",
      "Epoch 2/100\n",
      "115/115 [==============================] - 9s 82ms/step - loss: 0.9179 - accuracy: 0.5964\n",
      "Epoch 3/100\n",
      "115/115 [==============================] - 10s 91ms/step - loss: 0.9278 - accuracy: 0.5877\n",
      "Epoch 4/100\n",
      "115/115 [==============================] - 10s 90ms/step - loss: 0.9309 - accuracy: 0.5859\n",
      "Epoch 5/100\n",
      "115/115 [==============================] - 11s 89ms/step - loss: 0.9345 - accuracy: 0.5864\n",
      "Epoch 6/100\n",
      "115/115 [==============================] - 10s 88ms/step - loss: 0.9178 - accuracy: 0.5887\n",
      "Epoch 7/100\n",
      "115/115 [==============================] - 10s 90ms/step - loss: 0.9044 - accuracy: 0.5953\n",
      "Epoch 8/100\n",
      "115/115 [==============================] - 11s 91ms/step - loss: 0.9038 - accuracy: 0.5984\n",
      "Epoch 9/100\n",
      "115/115 [==============================] - 10s 88ms/step - loss: 0.9064 - accuracy: 0.5970\n",
      "Epoch 10/100\n",
      "115/115 [==============================] - 11s 94ms/step - loss: 0.9200 - accuracy: 0.5936\n",
      "Epoch 11/100\n",
      "115/115 [==============================] - 11s 90ms/step - loss: 0.9442 - accuracy: 0.5833\n",
      "Epoch 12/100\n",
      "115/115 [==============================] - 11s 90ms/step - loss: 0.9323 - accuracy: 0.5882\n",
      "Epoch 13/100\n",
      "115/115 [==============================] - 11s 93ms/step - loss: 0.9290 - accuracy: 0.5870\n",
      "Epoch 14/100\n",
      "115/115 [==============================] - 11s 94ms/step - loss: 0.9306 - accuracy: 0.5893\n",
      "Epoch 15/100\n",
      "115/115 [==============================] - 11s 96ms/step - loss: 0.9217 - accuracy: 0.5902\n",
      "Epoch 16/100\n",
      "115/115 [==============================] - 11s 92ms/step - loss: 0.9256 - accuracy: 0.5906\n",
      "Epoch 17/100\n",
      "115/115 [==============================] - 11s 91ms/step - loss: 0.9092 - accuracy: 0.5943\n",
      "Epoch 18/100\n",
      "115/115 [==============================] - 11s 95ms/step - loss: 0.9112 - accuracy: 0.5946\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d579ab17f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_monitor = EarlyStopping(patience=10, monitor='loss', verbose=1)\n",
    "model.fit(train_ds.batch(32), epochs=100,callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462272ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496dfc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 8s 62ms/step - loss: 0.9797 - accuracy: 0.5866\n",
      "Test Loss: 0.9797009229660034\n",
      "Test Accuracy: 0.5866425633430481\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(test_ds.batch(32))\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9b9b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd487aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import keras\n",
    "\n",
    "def create_segmentation_model(input_shape, num_classes):\n",
    "    # Load ResNet50 backbone\n",
    "    resnet50_base = keras.models.load_model(r\"F:\\Pre-Trained_Models\\Pre-Trained_Models\\rnd_32_model.h5\")\n",
    "    \n",
    "    # Add segmentation-specific layers on top of ResNet50\n",
    "    x = resnet50_base.layers[-4].output\n",
    "    x = tf.keras.layers.UpSampling2D(size=(32, 32))(x)  # Upsample to match original image size\n",
    "    x = tf.keras.layers.Conv2D(num_classes, kernel_size=(1, 1), activation='softmax')(x)\n",
    "\n",
    "    # Create segmentation model\n",
    "    model = tf.keras.models.Model(inputs=resnet50_base.input, outputs=x)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = (32, 32, 3)  # Adjust input shape as per your dataset\n",
    "num_classes = info.features['label'].num_classes\n",
    "model = create_segmentation_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e76b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74e859cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "115/115 [==============================] - 17s 94ms/step - loss: 1.1083 - accuracy: 0.5700\n",
      "Epoch 2/100\n",
      "115/115 [==============================] - 10s 86ms/step - loss: 0.8959 - accuracy: 0.6016\n",
      "Epoch 3/100\n",
      "115/115 [==============================] - 11s 96ms/step - loss: 0.8879 - accuracy: 0.6113\n",
      "Epoch 4/100\n",
      "115/115 [==============================] - 11s 92ms/step - loss: 0.8825 - accuracy: 0.6198\n",
      "Epoch 5/100\n",
      "115/115 [==============================] - 10s 91ms/step - loss: 0.9115 - accuracy: 0.6138\n",
      "Epoch 6/100\n",
      "115/115 [==============================] - 11s 91ms/step - loss: 0.9273 - accuracy: 0.5876\n",
      "Epoch 7/100\n",
      "115/115 [==============================] - 10s 89ms/step - loss: 0.9318 - accuracy: 0.5852\n",
      "Epoch 8/100\n",
      "115/115 [==============================] - 10s 89ms/step - loss: 0.9087 - accuracy: 0.5868\n",
      "Epoch 9/100\n",
      "115/115 [==============================] - 10s 90ms/step - loss: 0.9417 - accuracy: 0.5861\n",
      "Epoch 10/100\n",
      "115/115 [==============================] - 10s 86ms/step - loss: 0.9408 - accuracy: 0.5865\n",
      "Epoch 11/100\n",
      "115/115 [==============================] - 11s 92ms/step - loss: 0.9104 - accuracy: 0.5886\n",
      "Epoch 12/100\n",
      "115/115 [==============================] - 12s 100ms/step - loss: 0.9107 - accuracy: 0.5939\n",
      "Epoch 13/100\n",
      "115/115 [==============================] - 11s 97ms/step - loss: 0.9086 - accuracy: 0.5898\n",
      "Epoch 14/100\n",
      "115/115 [==============================] - 11s 96ms/step - loss: 0.9036 - accuracy: 0.5921\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d57f9a6b20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_monitor = EarlyStopping(patience=10, monitor='loss', verbose=1)\n",
    "model.fit(train_ds.batch(32), epochs=100,callbacks=[early_stopping_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0586a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 7s 55ms/step - loss: 0.9835 - accuracy: 0.5769\n",
      "Test Loss: 0.9834929704666138\n",
      "Test Accuracy: 0.5769003629684448\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(test_ds.batch(32))\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d046a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928444d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a7272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ac19c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb799c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cb67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
