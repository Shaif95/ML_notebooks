{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4044afab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import tensorflow as tf  # framework\n",
    "from tensorflow import keras  # for tf.keras\n",
    "import tensorflow_addons as tfa  # LAMB optimizer and gaussian_blur_2d function\n",
    "import numpy as np  # np.random.random\n",
    "import matplotlib.pyplot as plt  # graphs\n",
    "import datetime  # tensorboard logs naming\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed4dfc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f8e92fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_22592\\706252393.py:32: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((32, 32), Image.ANTIALIAS)  # Resize to 16x16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def list_lowest_level_subdirectories(directory_path):\n",
    "    lowest_level_subdirectories = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for dir in dirs:\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            # Check if the current directory doesn't contain any subdirectories\n",
    "            if not any(os.path.isdir(os.path.join(dir_path, sub_dir)) for sub_dir in os.listdir(dir_path)):\n",
    "                lowest_level_subdirectories.append(dir_path)\n",
    "\n",
    "    return lowest_level_subdirectories\n",
    "\n",
    "def collect_images_and_lengths(directory_paths):\n",
    "    images_list = []\n",
    "    lengths_list = []\n",
    "    train_features = []\n",
    "    for directory_path in directory_paths:\n",
    "        image_paths = [os.path.join(directory_path, file) for file in os.listdir(directory_path) if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "\n",
    "        image = 0\n",
    "\n",
    "        for image_path in image_paths:\n",
    "            try:\n",
    "                with Image.open(image_path) as img:\n",
    "                    # Check if the image is not in RGB mode\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    img = img.resize((32, 32), Image.ANTIALIAS)  # Resize to 16x16\n",
    "                    img = np.array(img).astype('float32') / 255.0  # Convert to float and normalize\n",
    "                    images_list.append(img)\n",
    "                    if(image == 0 ):\n",
    "                        train_features.append(img)\n",
    "                    image= image + 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading or converting image '{image_path}': {e}\")\n",
    "        lengths_list.append((image))\n",
    "        \n",
    "    return images_list, lengths_list\n",
    "\n",
    "# Specify the directory path\n",
    "start_directory = r'E:\\invasive-aquatic-species-data'\n",
    "\n",
    "# Get a list of lowest-level subdirectories\n",
    "lowest_level_subdirectories = list_lowest_level_subdirectories(start_directory)\n",
    "\n",
    "# Collect images and lengths for subdirectories\n",
    "images_list, lengths_list = collect_images_and_lengths(lowest_level_subdirectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.shape(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55037c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.sum(lengths_list)\n",
    "#a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b154664",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "# Width and height of image\n",
    "IMAGE_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23774f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cabe2cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112788, 2, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "\n",
    "for i in range(0,len(lengths_list)):\n",
    "    a = int(np.sum(lengths_list[:i])) #0,  27, 65, 95\n",
    "    \n",
    "    b = a+lengths_list[i]    # 27, 65, 95\n",
    "    lst = images_list[a:b]\n",
    "    \n",
    "    #print(np.shape(lst))\n",
    "    \n",
    "    for k in lst:\n",
    "        pairs = []\n",
    "        augment = random.choice(lst)\n",
    "        pairs.append(k)\n",
    "        pairs.append(augment)\n",
    "        dataset.append(pairs)\n",
    "        \n",
    "print(np.shape(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(dataset[150][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ec392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea59887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class BTDatasetCreator:\n",
    "    def __init__(self, seed: int = 1024):\n",
    "        self.seed = seed\n",
    "\n",
    "    def split_pairs(self, ds: list) -> tf.data.Dataset:\n",
    "        # Split pairs into a1 and a2\n",
    "        a1 = [pair[0] for pair in ds]\n",
    "        a2 = [pair[1] for pair in ds]\n",
    "\n",
    "        return (tf.data.Dataset.from_tensor_slices((a1, a2))\n",
    "                .shuffle(1000, seed=self.seed)\n",
    "                .batch(BATCH_SIZE, drop_remainder=True)\n",
    "                .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "    def __call__(self, ds: list) -> tf.data.Dataset:\n",
    "        return self.split_pairs(ds)\n",
    "\n",
    "bt_creator = BTDatasetCreator()\n",
    "augment_versions = bt_creator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a26b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ac8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_augment_versions = iter(augment_versions)\n",
    "\n",
    "\n",
    "def plot_values(batch: tuple):\n",
    "    fig, axs = plt.subplots(3, 3)\n",
    "    fig1, axs1 = plt.subplots(3, 3)\n",
    "\n",
    "    fig.suptitle(\"Augmentation 1\")\n",
    "    fig1.suptitle(\"Augmentation 2\")\n",
    "\n",
    "    a1, a2 = batch\n",
    "\n",
    "    # plots images on both tables\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            # CHANGE(add / 255)\n",
    "            axs[i][j].imshow(a1[3 * i + j])\n",
    "            axs[i][j].axis(\"off\")\n",
    "            axs1[i][j].imshow(a2[3 * i + j])\n",
    "            axs1[i][j].axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "#for _ in range(2):  # Plot 5 different batches\n",
    "    #batch = next(sample_augment_versions)\n",
    "    #plot_values(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726395e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d2951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ae6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowLoss(keras.losses.Loss):\n",
    "\n",
    "\n",
    "    def __init__(self, batch_size: int):\n",
    "\n",
    "        super().__init__()\n",
    "        self.lambda_amt = 5e-3\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_off_diag(self, c: tf.Tensor) -> tf.Tensor:\n",
    "   \n",
    "        zero_diag = tf.zeros(c.shape[-1])\n",
    "        return tf.linalg.set_diag(c, zero_diag)\n",
    "\n",
    "    def cross_corr_matrix_loss(self, c: tf.Tensor) -> tf.Tensor:\n",
    "     \n",
    "        # subtracts diagonals by one and squares them(first part)\n",
    "        c_diff = tf.pow(tf.linalg.diag_part(c) - 1, 2)\n",
    "\n",
    "        # takes off diagonal, squares it, multiplies with lambda(second part)\n",
    "        off_diag = tf.pow(self.get_off_diag(c), 2) * self.lambda_amt\n",
    "\n",
    "        # sum first and second parts together\n",
    "        loss = tf.reduce_sum(c_diff) + tf.reduce_sum(off_diag)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def normalize(self, output: tf.Tensor) -> tf.Tensor:\n",
    "    \n",
    "\n",
    "        return (output - tf.reduce_mean(output, axis=0)) / tf.math.reduce_std(\n",
    "            output, axis=0\n",
    "        )\n",
    "\n",
    "    def cross_corr_matrix(self, z_a_norm: tf.Tensor, z_b_norm: tf.Tensor) -> tf.Tensor:\n",
    "        \n",
    "        return (tf.transpose(z_a_norm) @ z_b_norm) / self.batch_size\n",
    "\n",
    "    def call(self, z_a: tf.Tensor, z_b: tf.Tensor) -> tf.Tensor:\n",
    "        \n",
    "        z_a_norm, z_b_norm = self.normalize(z_a), self.normalize(z_b)\n",
    "        c = self.cross_corr_matrix(z_a_norm, z_b_norm)\n",
    "        loss = self.cross_corr_matrix_loss(c)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d7dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "\n",
    "class ResNet34:\n",
    "\n",
    "    def __call__(self, shape=(32, 32, 3)):\n",
    "        \n",
    "        inputs = Input(shape=shape)\n",
    "        base_model = ResNet50(include_top=False, weights=\"imagenet\", input_shape=shape, pooling=\"avg\")\n",
    "        x = base_model(inputs)\n",
    "        output = Dense(2048)(x)\n",
    "        new_model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "        return new_model\n",
    "\n",
    "resnet = ResNet34()()\n",
    "#resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e2e7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_twin() -> keras.Model:\n",
    "\n",
    "\n",
    "    # number of dense neurons in the projector\n",
    "    n_dense_neurons = 5000\n",
    "\n",
    "    # encoder network\n",
    "    resnet = ResNet34()()\n",
    "    last_layer = resnet.layers[-1].output\n",
    "\n",
    "    # intermediate layers of the projector network\n",
    "    n_layers = 2\n",
    "    for i in range(n_layers):\n",
    "        dense = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{i}\")\n",
    "        if i == 0:\n",
    "            x = dense(last_layer)\n",
    "        else:\n",
    "            x = dense(x)\n",
    "        x = tf.keras.layers.BatchNormalization(name=f\"projector_bn_{i}\")(x)\n",
    "        x = tf.keras.layers.ReLU(name=f\"projector_relu_{i}\")(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(n_dense_neurons, name=f\"projector_dense_{n_layers}\")(x)\n",
    "\n",
    "    model = keras.Model(resnet.input, x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95a32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BarlowModel(keras.Model):\n",
    "   \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = build_twin()\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def train_step(self, batch: tf.Tensor) -> tf.Tensor:\n",
    "       \n",
    "        # get the two augmentations from the batch\n",
    "        y_a, y_b = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get two versions of predictions\n",
    "            z_a, z_b = self.model(y_a, training=True), self.model(y_b, training=True)\n",
    "            loss = self.loss(z_a, z_b)\n",
    "\n",
    "        grads_model = tape.gradient(loss, self.model.trainable_variables)\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grads_model, self.model.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45528b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7049/7049 [==============================] - 2617s 363ms/step - loss: 13345.6670\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sets up model, optimizer, loss\n",
    "\n",
    "bm = BarlowModel()\n",
    "# chose the LAMB optimizer due to high batch sizes. Converged MUCH faster\n",
    "# than ADAM or SGD\n",
    "optimizer = tfa.optimizers.LAMB()\n",
    "loss = BarlowLoss(BATCH_SIZE)\n",
    "\n",
    "bm.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "# Expected training time: 1 hours 30 min\n",
    "\n",
    "history = bm.fit(augment_versions, epochs=1)\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a5e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1b388cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_22592\\1045525047.py:33: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((32, 32), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def list_subdirectories(directory_path):\n",
    "    try:\n",
    "        subdirectories = [f.path for f in os.scandir(directory_path) if f.is_dir()]\n",
    "        return subdirectories\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def list_all_subdirectories_with_images(parent_directory):\n",
    "    subdirectories = list_subdirectories(parent_directory)\n",
    "    XT = []  # List to store all images\n",
    "    YT = []  # List to store labels\n",
    "    YT_count = []\n",
    "    #print(subdirectories)\n",
    "    \n",
    "    label= 0\n",
    "    length_list = []\n",
    "    for s in subdirectories:\n",
    "        \n",
    "        sub = [f.path for f in os.scandir(s) if f.is_dir()]\n",
    "        #print(sub)\n",
    "        for f in sub:\n",
    "            count = 0\n",
    "            for img_path in os.listdir(f):\n",
    "            \n",
    "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "                    img = Image.open(os.path.join(f, img_path))\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    img = img.resize((32, 32), Image.ANTIALIAS)\n",
    "                    img = np.array(img).astype('float32') / 255.0\n",
    "                    XT.append(img)\n",
    "                    count = count + 1\n",
    "                    YT.append(label)\n",
    "                #print(np.shape(lst))\n",
    "            length_list.append(count)\n",
    "            YT_count.append(label)\n",
    "        label=label+1    \n",
    "                \n",
    "        \n",
    "    \n",
    "    return XT, YT, YT_count, length_list\n",
    "\n",
    "# Example usage:\n",
    "directory_path = r'C:\\Users\\shaif\\Downloads\\Compressed\\Labeled Baylor 8-30-23\\Labeled Baylor 8-30-23\\Zebra Umbo 1a Image1a'\n",
    "images_list, YT, YT_count, lengths_list = list_all_subdirectories_with_images(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cb154f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "yt = to_categorical(YT)\n",
    "ytc = to_categorical(YT_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "26a12583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of samples for testing\n",
    "num_test_samples = np.sum(lengths_list[75:])\n",
    "# Split the lists\n",
    "X_test = images_list[:num_test_samples]\n",
    "Y_test = yt[:num_test_samples]\n",
    "X_train = images_list[num_test_samples:]\n",
    "Y_train = yt[num_test_samples:]\n",
    "y_train = ytc[num_test_samples:]\n",
    "y_test = ytc[:num_test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05f54924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "99488fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "75/75 [==============================] - 14s 76ms/step - loss: 0.0501 - accuracy: 0.9958 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "75/75 [==============================] - 4s 52ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "75/75 [==============================] - 4s 53ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279411eb850>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D\n",
    "model = bm.model\n",
    "    \n",
    "num_classes = 10  # Update with the actual number of classes in your target data\n",
    "model.layers[0].trainable = False\n",
    "x = model.layers[-1].output  # Access the last 4th layer from the end\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Create the new model with the updated head\n",
    "new_model = keras.models.Model(inputs=model.input, outputs=output)\n",
    "# Compile the model\n",
    "new_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "new_model.fit(np.array(X_train).astype('float32'),Y_train, batch_size=32, epochs=3, validation_split = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edce96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ca09f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 2s 15ms/step\n",
      "Test Accuracy: 0.20242576327896278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = new_model.predict(np.array(X_test).astype('float32'))\n",
    "accuracy = accuracy_score(Y_test,predictions)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e84da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc5b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
