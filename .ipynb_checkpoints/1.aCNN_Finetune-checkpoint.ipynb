{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de27a733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.1 Requires-Python <3.13,>=3.11; 0.2 Requires-Python <3.13,>=3.11; 0.3 Requires-Python <3.13,>=3.11\n",
      "ERROR: Could not find a version that satisfies the requirement tfds (from versions: none)\n",
      "ERROR: No matching distribution found for tfds\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee87ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tfds-nightly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58194922",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5fb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2ace50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (4.9.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: array-record in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (0.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (8.1.3)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (1.24.4)\n",
      "Requirement already satisfied: promise in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (5.9.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (1.13.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: toml in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (4.62.3)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (1.12.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-datasets) (5.12.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from promise->tensorflow-datasets) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-metadata->tensorflow-datasets) (1.59.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce89756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb7915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15e2711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.24.4)\n",
      "Requirement already satisfied: six~=1.14 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0c5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06fc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47503c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-model-optimization in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (0.8.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tensorflow-probability in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: absl-py~=1.2 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.4.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (0.1.8)\n",
      "Requirement already satisfied: numpy~=1.23 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.24.4)\n",
      "Requirement already satisfied: six~=1.14 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-model-optimization) (1.15.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-probability) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-probability) (2.2.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-probability) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0 in c:\\users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages (from tensorflow-probability) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-model-optimization tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0edfc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c11495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c0c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\shaif\\tensorflow_datasets\\caltech101\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021280765533447266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Dl Completed...",
       "rate": null,
       "total": 0,
       "unit": " url",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35b62171ec34a149971a1aece05c610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0161440372467041,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Dl Size...",
       "rate": null,
       "total": 0,
       "unit": " MiB",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de711133f4b43fda8796f7df8f0157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020771026611328125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Extraction completed...",
       "rate": null,
       "total": 0,
       "unit": " file",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837858b54c941a090456a302e809df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Artifact https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp, downloaded to C:\\Users\\shaif\\tensorflow_datasets\\downloads\\ucexport_download_id_137RyRjvTBkBiIfeYBNZ_EwspXlUxYffGj8eiXhr_4rE3NXcywQLRalS5KkycsuJfYfM.tmp.297958418ca740799c3742076c706bc0\\download, has wrong checksum:\n* Expected: UrlInfo(size=125.64 MiB, checksum='af6ece2f339791ca20f855943d8b55dd60892c0a25105fcd631ee3d6430f9926', filename='view')\n* Got: UrlInfo(size=2.38 KiB, checksum='ac6edd3aba75d3907df430b851471ab1789d0fc40a693efd172fe1e094c34c18', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     18\u001b[0m builder \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mbuilder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaltech101\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Delete old data (Optional: only do this if you want to start fresh)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# import shutil\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# shutil.rmtree(\"C:/Users/shaif/tensorflow_datasets/caltech101/3.0.1\", ignore_errors=True)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDownloadConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify_ssl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m info \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m     29\u001b[0m dataset_train, dataset_test \u001b[38;5;241m=\u001b[39m builder\u001b[38;5;241m.\u001b[39mas_dataset(\n\u001b[0;32m     30\u001b[0m     split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[:80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain[80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:]\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m     31\u001b[0m     as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1546\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1544\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1545\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1546\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1550\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1553\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1554\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1555\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1556\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\datasets\\caltech101\\caltech101_dataset_builder.py:55\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m---> 55\u001b[0m   path \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     57\u001b[0m       tfds\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mSplitGenerator(\n\u001b[0;32m     58\u001b[0m           name\u001b[38;5;241m=\u001b[39mtfds\u001b[38;5;241m.\u001b[39mSplit\u001b[38;5;241m.\u001b[39mTRAIN,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m       ),\n\u001b[0;32m     71\u001b[0m   ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:688\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m    687\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 831\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:832\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m    831\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[0;32m    833\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\six.py:703\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:87\u001b[0m, in \u001b[0;36mtry_catch\u001b[1;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_catch\u001b[39m(handler, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     89\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:408\u001b[0m, in \u001b[0;36mDownloadManager._download.<locals>.<lambda>\u001b[1;34m(dl_result)\u001b[0m\n\u001b[0;32m    402\u001b[0m   future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[0;32m    403\u001b[0m       url, download_tmp_dir, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_ssl\n\u001b[0;32m    404\u001b[0m   )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;66;03m# Post-process the result\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mthen(\n\u001b[1;32m--> 408\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m dl_result: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_or_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:465\u001b[0m, in \u001b[0;36mDownloadManager._register_or_validate_checksums\u001b[1;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[0;32m    454\u001b[0m   checksum_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dl_path(url, computed_url_info\u001b[38;5;241m.\u001b[39mchecksum)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;66;03m# Eventually validate checksums\u001b[39;00m\n\u001b[0;32m    457\u001b[0m   \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m   \u001b[38;5;66;03m#   download). This is expected as it might mean the downloaded file\u001b[39;00m\n\u001b[0;32m    464\u001b[0m   \u001b[38;5;66;03m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m   \u001b[43m_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputed_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_checksums_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_checksums_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rename_and_get_final_dl_path(\n\u001b[0;32m    474\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    475\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m     url_path\u001b[38;5;241m=\u001b[39murl_path,\n\u001b[0;32m    480\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:809\u001b[0m, in \u001b[0;36m_validate_checksums\u001b[1;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    798\u001b[0m     expected_url_info\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m computed_url_info\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m expected_url_info \u001b[38;5;241m!=\u001b[39m computed_url_info\n\u001b[0;32m    801\u001b[0m ):\n\u001b[0;32m    802\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    803\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtifact \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, downloaded to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, has wrong checksum:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    804\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_url_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    808\u001b[0m   )\n\u001b[1;32m--> 809\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m NonMatchingChecksumError(msg)\n",
      "\u001b[1;31mNonMatchingChecksumError\u001b[0m: Artifact https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp, downloaded to C:\\Users\\shaif\\tensorflow_datasets\\downloads\\ucexport_download_id_137RyRjvTBkBiIfeYBNZ_EwspXlUxYffGj8eiXhr_4rE3NXcywQLRalS5KkycsuJfYfM.tmp.297958418ca740799c3742076c706bc0\\download, has wrong checksum:\n* Expected: UrlInfo(size=125.64 MiB, checksum='af6ece2f339791ca20f855943d8b55dd60892c0a25105fcd631ee3d6430f9926', filename='view')\n* Got: UrlInfo(size=2.38 KiB, checksum='ac6edd3aba75d3907df430b851471ab1789d0fc40a693efd172fe1e094c34c18', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet50,\n",
    "    EfficientNetB0,\n",
    "    resnet50,\n",
    "    efficientnet\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Load the tf_flowers Dataset\n",
    "# ---------------------------------------------------\n",
    "# Create your own train/test split: 80% / 20%\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Number of classes in tf_flowers\n",
    "num_classes = info.features['label'].num_classes\n",
    "nums = num_classes  # keep both variables as you did before\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Convert to NumPy arrays (Optionally Resize)\n",
    "# ---------------------------------------------------\n",
    "IMG_SIZE = 128\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    # Resize to reduce memory usage\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "x_train = np.stack(x_train_list).astype(\"float32\")\n",
    "y_train = np.array(y_train_list)\n",
    "x_test  = np.stack(x_test_list).astype(\"float32\")\n",
    "y_test  = np.array(y_test_list)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Normalize & One-Hot Encode Labels\n",
    "# ---------------------------------------------------\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train /= 255.0\n",
    "x_test  /= 255.0\n",
    "\n",
    "# Convert integer labels to one-hot vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test  shape: {x_test.shape}\")\n",
    "print(f\"y_test  shape: {y_test.shape}\")\n",
    "print(f\"num_classes: {num_classes}, nums: {nums}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee5ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8492c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning ResNet50 on CIFAR-100 ---\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 8s 155ms/step - loss: 4.8596 - accuracy: 0.0061 - val_loss: 4.6268 - val_accuracy: 0.0098\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 2s 85ms/step - loss: 4.6213 - accuracy: 0.0098 - val_loss: 4.6422 - val_accuracy: 0.0049\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 3s 105ms/step - loss: 4.6259 - accuracy: 0.0110 - val_loss: 4.6214 - val_accuracy: 0.0098\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 3s 115ms/step - loss: 4.6236 - accuracy: 0.0110 - val_loss: 4.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 3s 115ms/step - loss: 4.6233 - accuracy: 0.0098 - val_loss: 4.6236 - val_accuracy: 0.0147\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 3s 117ms/step - loss: 4.6212 - accuracy: 0.0147 - val_loss: 4.6251 - val_accuracy: 0.0049\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 3s 126ms/step - loss: 4.6230 - accuracy: 0.0159 - val_loss: 4.6259 - val_accuracy: 0.0049\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 3s 130ms/step - loss: 4.6223 - accuracy: 0.0159 - val_loss: 4.6290 - val_accuracy: 0.0049\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 3s 103ms/step - loss: 4.6185 - accuracy: 0.0135 - val_loss: 4.6257 - val_accuracy: 0.0049\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 2s 89ms/step - loss: 4.6196 - accuracy: 0.0098 - val_loss: 4.6256 - val_accuracy: 0.0049\n",
      "ResNet50 - CIFAR-100 Accuracy: 0.0049\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll define two separate pipelines:\n",
    "#   - One for ResNet50\n",
    "#   - One for EfficientNetB0\n",
    "\n",
    "# ------------------------------\n",
    "# 2. tf.data Pipeline for ResNet\n",
    "# ------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    # Resize and apply ResNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = resnet50.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Create training dataset\n",
    "train_ds_resnet = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_resnet = train_ds_resnet.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# Create validation (test) dataset\n",
    "val_ds_resnet = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_resnet = val_ds_resnet.map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Define Baseline Models to Fine-Tune\n",
    "# ------------------------------------------\n",
    "def create_resnet50_finetune(input_shape, num_classes):\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5. Fine-Tune ResNet50 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "resnet_model = create_resnet50_finetune((224, 224, 3), num_classes)\n",
    "resnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning ResNet50 on CIFAR-100 ---\")\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_ds_resnet,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds_resnet,\n",
    "    verbose=1\n",
    ")\n",
    "resnet_loss, resnet_acc = resnet_model.evaluate(val_ds_resnet, verbose=0)\n",
    "print(f\"ResNet50 - CIFAR-100 Accuracy: {resnet_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb5f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "26/26 [==============================] - 10s 169ms/step - loss: 4.8515 - accuracy: 0.0061 - val_loss: 4.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "26/26 [==============================] - 2s 76ms/step - loss: 4.6252 - accuracy: 0.0123 - val_loss: 4.6252 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 4.6249 - accuracy: 0.0061 - val_loss: 4.6254 - val_accuracy: 0.0000e+00\n",
      "EfficientNetB0 - CIFAR-100 Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 3. tf.data Pipeline for EfficientNet\n",
    "# ------------------------------\n",
    "def preprocess_efficientnet(image, label):\n",
    "    # Resize and apply EfficientNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds_eff = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_eff = train_ds_eff.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_eff = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_eff = val_ds_eff.map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "def create_efficientnet_finetune(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6. Fine-Tune EfficientNetB0 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "eff_model = create_efficientnet_finetune((224, 224, 3), num_classes)\n",
    "eff_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\")\n",
    "history_eff = eff_model.fit(\n",
    "    train_ds_eff,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds_eff,\n",
    "    verbose=1\n",
    ")\n",
    "eff_loss, eff_acc = eff_model.evaluate(val_ds_eff, verbose=0)\n",
    "print(f\"EfficientNetB0 - CIFAR-100 Accuracy: {eff_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbd444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f129143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eda9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Structured Pruning via Group Regularization\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd89cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Structured Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_structured_pruned_efficientnet(input_shape, num_classes, l1_lambda=1e-4, l2_lambda=1e-4):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "\n",
    "    # Add Group Lasso Regularization (L1 for sparsity, L2 for structure)\n",
    "    x = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l1_l2(l1=l1_lambda, l2=l2_lambda)  # Group regularization\n",
    "    )(x)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Train the Structured Pruned Model\n",
    "# ---------------------------------------------\n",
    "structured_pruned_model = create_structured_pruned_efficientnet((224, 224, 3), num_classes, l1_lambda=1e-4, l2_lambda=1e-4)\n",
    "\n",
    "structured_pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Structured Pruned EfficientNetB0 on TF Flowers ---\")\n",
    "\n",
    "history_structured = structured_pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Evaluate the Structured Pruned Model\n",
    "# ---------------------------------------------\n",
    "structured_loss, structured_acc = structured_pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Structured Pruned EfficientNetB0 - TF Flowers Accuracy: {structured_acc:.4f}\")\n",
    "\n",
    "# Save the final pruned model\n",
    "structured_pruned_model.save(\"efficientnet_structured_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7afceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33faee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Meta Learning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset and Preprocess\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Define a Custom MetaDense Layer with Learnable Gates\n",
    "# ---------------------------------------------\n",
    "class MetaDense(layers.Layer):\n",
    "    def __init__(self, units, l1_reg=1e-4, l2_reg=1e-4, **kwargs):\n",
    "        super(MetaDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.l1_reg = l1_reg\n",
    "        self.l2_reg = l2_reg\n",
    "        self.dense = layers.Dense(units, activation='relu')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # The dense layer will build its own weights.\n",
    "        # We add a learnable gating vector to modulate each output neuron.\n",
    "        self.gate = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='gate'\n",
    "        )\n",
    "        super(MetaDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense(inputs)\n",
    "        # Multiply the dense output by the learned gate (element-wise)\n",
    "        x = x * self.gate\n",
    "        # Add regularization loss to encourage sparsity in the gate parameters\n",
    "        self.add_loss(self.l1_reg * tf.reduce_sum(tf.abs(self.gate)) +\n",
    "                      self.l2_reg * tf.reduce_sum(tf.square(self.gate)))\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define the Meta-Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_meta_pruned_efficientnet(input_shape, num_classes, l1_reg=1e-4, l2_reg=1e-4):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    # Use the custom MetaDense layer instead of a plain Dense layer.\n",
    "    meta_dense = MetaDense(256, l1_reg=l1_reg, l2_reg=l2_reg, name=\"meta_dense\")\n",
    "    x = meta_dense(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model, meta_dense\n",
    "\n",
    "model, meta_dense_layer = create_meta_pruned_efficientnet((224, 224, 3), num_classes, l1_reg=1e-4, l2_reg=1e-4)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Set Up Optimizers for Model Weights and Meta-Parameters\n",
    "# ---------------------------------------------\n",
    "model_optimizer = Adam(learning_rate=1e-4)\n",
    "meta_optimizer = Adam(learning_rate=1e-4)  # Optimizer for the meta (gate) parameters\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Custom Training Loop with Meta-Learning Update\n",
    "# ---------------------------------------------\n",
    "epochs = 5  # Adjust as needed\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        # Include any regularization losses added in the call (e.g., from MetaDense)\n",
    "        loss += sum(model.losses)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    model_optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def meta_update_step(images, labels):\n",
    "    # Compute validation loss for meta-update on the meta parameters (gate)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=False)\n",
    "        meta_loss = loss_fn(labels, predictions)\n",
    "        meta_loss += sum(model.losses)\n",
    "    # Only update the gate parameters from the MetaDense layer\n",
    "    meta_vars = [meta_dense_layer.gate]\n",
    "    meta_gradients = tape.gradient(meta_loss, meta_vars)\n",
    "    meta_optimizer.apply_gradients(zip(meta_gradients, meta_vars))\n",
    "    val_metric.update_state(labels, predictions)\n",
    "    return meta_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    # Reset metrics at the start of each epoch\n",
    "    train_metric.reset_states()\n",
    "    val_metric.reset_states()\n",
    "\n",
    "    # ----- Inner Loop: Training on the Training Set -----\n",
    "    for batch_images, batch_labels in train_ds:\n",
    "        loss_value = train_step(batch_images, batch_labels)\n",
    "\n",
    "    # ----- Outer Loop: Meta-Update using the Validation Set -----\n",
    "    # For demonstration, we run a meta-update over the entire validation set once per epoch.\n",
    "    for val_images, val_labels in val_ds:\n",
    "        meta_loss_value = meta_update_step(val_images, val_labels)\n",
    "\n",
    "    print(f\"Training Accuracy: {train_metric.result().numpy():.4f}, \" \n",
    "          f\"Validation Accuracy: {val_metric.result().numpy():.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Final Evaluation and Save the Model\n",
    "# ---------------------------------------------\n",
    "eval_loss = 0.0\n",
    "eval_steps = 0\n",
    "for val_images, val_labels in val_ds:\n",
    "    preds = model(val_images, training=False)\n",
    "    loss = loss_fn(val_labels, preds)\n",
    "    eval_loss += loss\n",
    "    eval_steps += 1\n",
    "eval_loss /= eval_steps\n",
    "\n",
    "final_accuracy = val_metric.result().numpy()\n",
    "print(f\"\\nMeta-Pruned EfficientNetB0 - TF Flowers Accuracy: {final_accuracy:.4f}, Loss: {eval_loss:.4f}\")\n",
    "\n",
    "model.save(\"efficientnet_meta_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f39854",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Graph Learning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcdd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import networkx as nx  # For graph operations\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset and Preprocess\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Define the Graph-Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_graph_pruned_efficientnet(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "    \n",
    "    # Build the top layers with explicit layer names\n",
    "    x = layers.Flatten(name=\"flatten\")(base_model.output)\n",
    "    # This dense layer will be our candidate for graph-based pruning\n",
    "    x = layers.Dense(256, activation='relu', name='graph_dense')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = create_graph_pruned_efficientnet((224, 224, 3), num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Pre-Training EfficientNetB0 on TF Flowers ---\")\n",
    "history = model.fit(train_ds, epochs=3, validation_data=val_ds, verbose=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Graph-Theoretic Pruning Function\n",
    "# ---------------------------------------------\n",
    "def graph_theoretic_prune_dense_layer(model, layer_name='graph_dense', prune_ratio=0.2, sim_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Prunes the specified Dense layer in the model using a graph-based strategy.\n",
    "    Each neuron (column in the kernel) is treated as a node; edges are added\n",
    "    if cosine similarity exceeds sim_threshold. Neurons with the lowest eigenvector\n",
    "    centrality are pruned.\n",
    "    \n",
    "    Parameters:\n",
    "        model: Keras model containing the target dense layer.\n",
    "        layer_name: Name of the dense layer to prune.\n",
    "        prune_ratio: Fraction of neurons to remove.\n",
    "        sim_threshold: Minimum cosine similarity to create an edge.\n",
    "    \n",
    "    Returns:\n",
    "        A new Keras model with the pruned dense layer.\n",
    "    \"\"\"\n",
    "    # Extract the dense layer weights\n",
    "    dense_layer = model.get_layer(layer_name)\n",
    "    kernel, bias = dense_layer.get_weights()  # kernel shape: (input_dim, units)\n",
    "    input_dim, num_units = kernel.shape\n",
    "\n",
    "    # Normalize each neuron's weight vector (columns) for cosine similarity\n",
    "    norms = np.linalg.norm(kernel, axis=0, keepdims=True) + 1e-8\n",
    "    kernel_norm = kernel / norms\n",
    "    \n",
    "    # Compute pairwise cosine similarity matrix among neurons\n",
    "    similarity_matrix = np.dot(kernel_norm.T, kernel_norm)  # shape: (units, units)\n",
    "    \n",
    "    # Build a graph where each neuron is a node\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_units))\n",
    "    \n",
    "    # Add edges for neuron pairs with similarity above the threshold\n",
    "    for i in range(num_units):\n",
    "        for j in range(i+1, num_units):\n",
    "            sim = similarity_matrix[i, j]\n",
    "            if sim > sim_threshold:\n",
    "                G.add_edge(i, j, weight=sim)\n",
    "    \n",
    "    # Compute eigenvector centrality for the graph nodes\n",
    "    # For isolated nodes, eigenvector_centrality_numpy returns a value; otherwise,\n",
    "    # this measure indicates influence in the network.\n",
    "    centrality = nx.eigenvector_centrality_numpy(G, weight='weight')\n",
    "    \n",
    "    # Sort neurons by centrality (lowest first)\n",
    "    sorted_units = sorted(centrality.items(), key=lambda x: x[1])\n",
    "    num_prune = int(prune_ratio * num_units)\n",
    "    pruned_units = [unit for unit, cent in sorted_units[:num_prune]]\n",
    "    keep_units = [i for i in range(num_units) if i not in pruned_units]\n",
    "    \n",
    "    print(f\"Pruning {num_prune} out of {num_units} neurons from layer '{layer_name}' based on centrality.\")\n",
    "    \n",
    "    # Create new weights by keeping only the selected neurons\n",
    "    new_kernel = kernel[:, keep_units]\n",
    "    new_bias = bias[keep_units]\n",
    "    \n",
    "    # Create a new Dense layer with the reduced number of units\n",
    "    new_dense_layer = layers.Dense(len(keep_units), activation='relu', name='graph_dense_pruned')\n",
    "    new_dense_layer.build((None, input_dim))\n",
    "    new_dense_layer.set_weights([new_kernel, new_bias])\n",
    "    \n",
    "    # Rebuild the model: assume the architecture is\n",
    "    # Input -> base_model -> Flatten -> [graph_dense] -> Dropout -> Final Dense.\n",
    "    base_input = model.input\n",
    "    flatten_output = model.get_layer('flatten').output\n",
    "    \n",
    "    # Replace the pruned dense layer\n",
    "    x = new_dense_layer(flatten_output)\n",
    "    \n",
    "    # Re-use the remaining layers (dropout and predictions) by forwarding x through them\n",
    "    dropout_layer = model.get_layer('dropout')\n",
    "    x = dropout_layer(x)\n",
    "    final_dense = model.get_layer('predictions')\n",
    "    outputs = final_dense(x)\n",
    "    \n",
    "    new_model = models.Model(inputs=base_input, outputs=outputs)\n",
    "    return new_model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Apply Graph-Theoretic Pruning and Fine-Tune\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- Applying Graph-Theoretic Pruning on Dense Layer ---\")\n",
    "pruned_model = graph_theoretic_prune_dense_layer(model, layer_name='graph_dense', prune_ratio=0.2, sim_threshold=0.5)\n",
    "\n",
    "pruned_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "\n",
    "print(\"\\n--- Fine-Tuning the Pruned Model on TF Flowers ---\")\n",
    "history_pruned = pruned_model.fit(train_ds, epochs=2, validation_data=val_ds, verbose=1)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Final Evaluation and Save the Pruned Model\n",
    "# ---------------------------------------------\n",
    "pruned_loss, pruned_acc = pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"\\nGraph-Theoretically Pruned EfficientNetB0 - TF Flowers Accuracy: {pruned_acc:.4f}\")\n",
    "\n",
    "pruned_model.save(\"efficientnet_graph_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52cce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61abb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#NAS\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd65eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157799d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset and Preprocess\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Define a Custom JDASPDense Layer for Joint DASP\n",
    "# ---------------------------------------------\n",
    "class JDASPDense(layers.Layer):\n",
    "    def __init__(self, units, temperature=1.0, l1_lambda=1e-4, **kwargs):\n",
    "        \"\"\"\n",
    "        units: Number of neurons.\n",
    "        temperature: Temperature parameter to control the sharpness of the sigmoid.\n",
    "        l1_lambda: Regularization coefficient to encourage sparsity.\n",
    "        \"\"\"\n",
    "        super(JDASPDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.temperature = temperature\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.dense = layers.Dense(units, activation='relu')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Architecture parameter (one per neuron) to learn a continuous sparsity mask.\n",
    "        self.alpha = self.add_weight(\n",
    "            name='alpha',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True)\n",
    "        super(JDASPDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Standard dense output.\n",
    "        x = self.dense(inputs)\n",
    "        # Compute a differentiable mask via a sigmoid function.\n",
    "        mask = tf.sigmoid(self.alpha / self.temperature)\n",
    "        # Multiply each neuron's output by its mask value (broadcasted across the batch).\n",
    "        x = x * mask\n",
    "        # Add an L1 penalty on the mask values to encourage pruning.\n",
    "        self.add_loss(self.l1_lambda * tf.reduce_sum(tf.abs(mask)))\n",
    "        return x\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define the J-DASP EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_jdasp_efficientnet(input_shape, num_classes, temperature=1.0, l1_lambda=1e-4):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze the base\n",
    "\n",
    "    x = layers.Flatten(name='flatten')(base_model.output)\n",
    "    # Use JDASPDense for joint architecture search and pruning.\n",
    "    jdasp_layer = JDASPDense(256, temperature=temperature, l1_lambda=l1_lambda, name='jdasp_dense')\n",
    "    x = jdasp_layer(x)\n",
    "    x = layers.Dropout(0.3, name='dropout')(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model, jdasp_layer\n",
    "\n",
    "model, jdasp_layer = create_jdasp_efficientnet((224, 224, 3), num_classes, temperature=1.0, l1_lambda=1e-4)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Set Up Optimizers for Weights and Architecture Parameters\n",
    "# ---------------------------------------------\n",
    "# We use separate optimizers: one for standard weights and one for the architecture parameters (alpha).\n",
    "weights_optimizer = Adam(learning_rate=1e-4)\n",
    "arch_optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "# Separate variables: architecture variables (alpha) and all other weight variables.\n",
    "arch_vars = [jdasp_layer.alpha]\n",
    "weight_vars = [v for v in model.trainable_variables if 'alpha' not in v.name]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Custom Training Loop for Joint Differentiable Architecture Search with Pruning\n",
    "# ---------------------------------------------\n",
    "epochs = 5\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    # Update standard weights on the training set.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        # Incorporate additional losses (e.g., L1 regularization from JDASPDense).\n",
    "        loss += sum(model.losses)\n",
    "    gradients = tape.gradient(loss, weight_vars)\n",
    "    weights_optimizer.apply_gradients(zip(gradients, weight_vars))\n",
    "    train_metric.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def arch_update_step(images, labels):\n",
    "    # Update architecture parameters (mask) using validation loss.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        arch_loss = loss_fn(labels, predictions)\n",
    "        arch_loss += sum(model.losses)\n",
    "    gradients = tape.gradient(arch_loss, arch_vars)\n",
    "    arch_optimizer.apply_gradients(zip(gradients, arch_vars))\n",
    "    val_metric.update_state(labels, predictions)\n",
    "    return arch_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_metric.reset_states()\n",
    "    val_metric.reset_states()\n",
    "\n",
    "    # Inner loop: update network weights on training data.\n",
    "    for images, labels in train_ds:\n",
    "        loss_value = train_step(images, labels)\n",
    "\n",
    "    # Outer loop: update architecture parameters on validation data.\n",
    "    for images, labels in val_ds:\n",
    "        arch_loss_value = arch_update_step(images, labels)\n",
    "\n",
    "    print(f\"Training Accuracy: {train_metric.result().numpy():.4f}, \" \n",
    "          f\"Validation Accuracy: {val_metric.result().numpy():.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Final Evaluation and Save the J-DASP Model\n",
    "# ---------------------------------------------\n",
    "eval_loss = 0.0\n",
    "eval_steps = 0\n",
    "for images, labels in val_ds:\n",
    "    preds = model(images, training=False)\n",
    "    loss = loss_fn(labels, preds)\n",
    "    eval_loss += loss\n",
    "    eval_steps += 1\n",
    "eval_loss /= eval_steps\n",
    "\n",
    "final_accuracy = val_metric.result().numpy()\n",
    "print(f\"\\nJ-DASP EfficientNetB0 - TF Flowers Accuracy: {final_accuracy:.4f}, Loss: {eval_loss:.4f}\")\n",
    "\n",
    "model.save(\"efficientnet_jdasp.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31ac0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344c0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8c290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee374bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc85e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3c07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa5a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fdbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
