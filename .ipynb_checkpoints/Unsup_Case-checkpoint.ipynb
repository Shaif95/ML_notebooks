{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c816a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602c030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrames: 100%|████████████████████████████████████████████████████████████| 4/4 [23:47<00:00, 356.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_paths = [\n",
    "    \"G:/image_features.csv\",\n",
    "    \"G:/image_features1.csv\",\n",
    "    \"G:/image_features2.csv\",\n",
    "    \"G:/image_features3.csv\"\n",
    "]\n",
    "\n",
    "# Load the CSV files into Dask DataFrames\n",
    "dfs = [dd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Function to get 25,000 random indexes and retrieve the full rows\n",
    "def get_random_rows(df, n=25000):\n",
    "    # Convert the Dask DataFrame to a Pandas DataFrame\n",
    "    pandas_df = df.compute()\n",
    "    \n",
    "    # Get the total number of rows in the DataFrame\n",
    "    total_rows = len(pandas_df)\n",
    "    \n",
    "    # Generate 25,000 random indexes\n",
    "    random_indexes = np.random.choice(total_rows, n, replace=False)\n",
    "    \n",
    "    # Retrieve the full rows for these random indexes\n",
    "    random_rows = pandas_df.iloc[random_indexes].values.tolist()\n",
    "    \n",
    "    return random_rows\n",
    "\n",
    "# Initialize an empty list to store all rows\n",
    "all_random_rows = []\n",
    "\n",
    "# Get the rows from each DataFrame\n",
    "for df in tqdm(dfs, desc=\"Processing DataFrames\"):\n",
    "    rows = get_random_rows(df)\n",
    "    all_random_rows.extend(rows)\n",
    "\n",
    "# Print the total number of rows collected\n",
    "print(len(all_random_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cfa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59381467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the first two items from each element in all_random_rows\n",
    "new_all_random_rows = [row[2:] for row in all_random_rows]\n",
    "np.shape(new_all_random_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9caf10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cluster centers: (1000, 2048)\n",
      "Cluster centers have been saved to 'cluster_centers.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Assuming new_all_random_rows has been populated as described\n",
    "\n",
    "# Convert new_all_random_rows to a NumPy array\n",
    "data = np.array(new_all_random_rows)\n",
    "\n",
    "batch_size = 10000\n",
    "max_iter = 100\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = MiniBatchKMeans(n_clusters=1000, batch_size=batch_size, max_iter=max_iter, random_state=42)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Store the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Print the shape of the cluster centers\n",
    "print(f\"Shape of the cluster centers: {centers.shape}\")\n",
    "\n",
    "# Save the cluster centers to a pickle file\n",
    "with open('cluster_centers.pkl', 'wb') as file:\n",
    "    pickle.dump(centers, file)\n",
    "\n",
    "print(\"Cluster centers have been saved to 'cluster_centers.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cd015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bba5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the pixel values between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 100)\n",
    "y_test = keras.utils.to_categorical(y_test, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10d8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "import glob\n",
    "\n",
    "target_size = (32, 32)  # Change the values as per your requirement\n",
    "# Load the pre-trained ResNet50 model with modified input shape\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "ft = model.predict(np.array(x_train).astype(\"float32\"))\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 50\n",
    "batch_size = 100\n",
    "max_iter = 100\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter)\n",
    "kmeans.fit(ft)\n",
    "# Retrieve the cluster centers\n",
    "ct = kmeans.cluster_centers_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9d0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4179a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:11<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the distance array: (50, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a list to store the distances\n",
    "tot_dist = []\n",
    "\n",
    "# Calculate L2 distances\n",
    "for i in tqdm(range(len(ct))):\n",
    "    distances = []\n",
    "    for j in range(len(centers)):\n",
    "        distance = np.linalg.norm(ct[i] - centers[j])\n",
    "        distances.append(distance)\n",
    "    tot_dist.append(distances)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "tot_dist = np.array(tot_dist)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(f\"Shape of the distance array: {tot_dist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01589f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:14<00:00, 66.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost = 3638.9521911759684\n",
      "\n",
      "Cluster 10 assigned to Class 17. Cost: 75.50213928844722\n",
      "Cluster 8 assigned to Class 22. Cost: 71.50815167056071\n",
      "Cluster 41 assigned to Class 29. Cost: 68.05460841307695\n",
      "Cluster 29 assigned to Class 41. Cost: 77.67070634692358\n",
      "Cluster 17 assigned to Class 52. Cost: 73.99451104476611\n",
      "Cluster 5 assigned to Class 90. Cost: 76.2307438638435\n",
      "Cluster 35 assigned to Class 116. Cost: 77.00536911646675\n",
      "Cluster 7 assigned to Class 120. Cost: 71.4266037679404\n",
      "Cluster 23 assigned to Class 122. Cost: 78.1596907027721\n",
      "Cluster 27 assigned to Class 125. Cost: 78.57004664411149\n",
      "Cluster 39 assigned to Class 139. Cost: 70.68291781056807\n",
      "Cluster 24 assigned to Class 181. Cost: 69.40027219762153\n",
      "Cluster 43 assigned to Class 183. Cost: 77.27309836656994\n",
      "Cluster 15 assigned to Class 185. Cost: 71.33526795851941\n",
      "Cluster 30 assigned to Class 192. Cost: 70.68278349498357\n",
      "Cluster 13 assigned to Class 202. Cost: 66.6732404337907\n",
      "Cluster 4 assigned to Class 206. Cost: 74.70699463506227\n",
      "Cluster 42 assigned to Class 219. Cost: 69.90740733254961\n",
      "Cluster 0 assigned to Class 226. Cost: 77.27470164970865\n",
      "Cluster 20 assigned to Class 262. Cost: 71.51491454980287\n",
      "Cluster 46 assigned to Class 263. Cost: 76.13243720810938\n",
      "Cluster 32 assigned to Class 271. Cost: 73.58622066388381\n",
      "Cluster 3 assigned to Class 291. Cost: 76.29244863680024\n",
      "Cluster 49 assigned to Class 326. Cost: 64.04955741294485\n",
      "Cluster 40 assigned to Class 350. Cost: 77.6206598330144\n",
      "Cluster 48 assigned to Class 371. Cost: 64.90190204556662\n",
      "Cluster 36 assigned to Class 401. Cost: 75.79336884870528\n",
      "Cluster 16 assigned to Class 408. Cost: 75.78930272632424\n",
      "Cluster 26 assigned to Class 410. Cost: 74.01673864112263\n",
      "Cluster 1 assigned to Class 413. Cost: 76.74537268383752\n",
      "Cluster 44 assigned to Class 485. Cost: 72.21979993576215\n",
      "Cluster 31 assigned to Class 527. Cost: 67.071669410816\n",
      "Cluster 28 assigned to Class 545. Cost: 75.38953716889665\n",
      "Cluster 38 assigned to Class 590. Cost: 71.53088323197134\n",
      "Cluster 33 assigned to Class 640. Cost: 75.7781170234767\n",
      "Cluster 19 assigned to Class 670. Cost: 75.99165158232178\n",
      "Cluster 22 assigned to Class 745. Cost: 67.78977378303408\n",
      "Cluster 47 assigned to Class 750. Cost: 77.54454037166433\n",
      "Cluster 25 assigned to Class 783. Cost: 75.2010702617995\n",
      "Cluster 14 assigned to Class 790. Cost: 77.25762097993807\n",
      "Cluster 2 assigned to Class 799. Cost: 56.26471647787335\n",
      "Cluster 9 assigned to Class 831. Cost: 76.50451437977604\n",
      "Cluster 12 assigned to Class 846. Cost: 77.19120822976919\n",
      "Cluster 37 assigned to Class 866. Cost: 75.82563695508127\n",
      "Cluster 34 assigned to Class 867. Cost: 69.91312347952248\n",
      "Cluster 21 assigned to Class 868. Cost: 75.22345857171315\n",
      "Cluster 11 assigned to Class 928. Cost: 68.75129229052605\n",
      "Cluster 6 assigned to Class 929. Cost: 71.97045952913807\n",
      "Cluster 18 assigned to Class 933. Cost: 56.05058385490678\n",
      "Cluster 45 assigned to Class 950. Cost: 72.98035566958656\n"
     ]
    }
   ],
   "source": [
    "from ortools.linear_solver import pywraplp\n",
    "dist_list =  np.transpose(tot_dist, (1, 0))\n",
    "from tqdm import tqdm\n",
    "\n",
    "costs = dist_list\n",
    "\n",
    "num_workers = len(costs)\n",
    "num_tasks = len(costs[0])\n",
    "# Create the mip solver with the SCIP backend.\n",
    "solver = pywraplp.Solver.CreateSolver(\"SCIP\")\n",
    "\n",
    "# x[i, j] is an array of 0-1 variables, which will be 1\n",
    "# if worker i is assigned to task j.\n",
    "x = {}\n",
    "for i in range(num_workers):\n",
    "    for j in range(num_tasks):\n",
    "        x[i, j] = solver.IntVar(0, 1, \"\")\n",
    "        \n",
    "# Each worker is assigned to at most 1 task.\n",
    "for i in range(num_workers):\n",
    "    solver.Add(solver.Sum([x[i, j] for j in range(num_tasks)]) <= 1)\n",
    "\n",
    "# Each task is assigned to exactly one worker.\n",
    "for j in range(num_tasks):\n",
    "    solver.Add(solver.Sum([x[i, j] for i in range(num_workers)]) == 1)\n",
    "    \n",
    "objective_terms = []\n",
    "for i in tqdm(range(num_workers)):\n",
    "    for j in range(num_tasks):\n",
    "        objective_terms.append(costs[i][j] * x[i, j])\n",
    "solver.Minimize(solver.Sum(objective_terms))\n",
    "\n",
    "status = solver.Solve()\n",
    "\n",
    "sol_indexes = []\n",
    "if status == pywraplp.Solver.OPTIMAL or status == pywraplp.Solver.FEASIBLE:\n",
    "    print(f\"Total cost = {solver.Objective().Value()}\\n\")\n",
    "    for i in range(num_workers):\n",
    "        for j in range(num_tasks):\n",
    "            # Test if x[i,j] is 1 (with tolerance for floating point arithmetic).\n",
    "            if x[i, j].solution_value() > 0.1:\n",
    "                print(f\"Cluster {j} assigned to Class {i}.\" + f\" Cost: {costs[i][j]}\")\n",
    "                sol_indexes.append(i)\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e8b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sol_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c015e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes have been saved to 'cvfinal.pkl'.\n"
     ]
    }
   ],
   "source": [
    "with open('cvfinal.pkl', 'wb') as file:\n",
    "    pickle.dump(sol_indexes, file)\n",
    "\n",
    "print(\"Indexes have been saved to 'cvfinal.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118f0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6783421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.6200830936431885 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1575fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrames:   0%|                                                      | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                         | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                        \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 15.91 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  25%|███████████▎                                 | 1/4 [13:32<40:38, 812.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res 68291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                        | 0/101 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                        \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.20 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  50%|██████████████████████                      | 2/4 [42:45<45:30, 1365.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res 169856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                         | 0/57 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                        \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.48 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  50%|████████████████████                    | 2/4 [1:18:20<1:18:20, 2350.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 4705.43381357193 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from dask.distributed import Client\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "\n",
    "# Initialize Dask client\n",
    "client = Client()\n",
    "\n",
    "# Function to compute closest indexes\n",
    "def compute_closest_indexes(chunk, centers, index_set):\n",
    "    result_list = []\n",
    "    for idx, row in chunk.iterrows():\n",
    "        features = np.array(row[2:], dtype=float).reshape(1, -1)\n",
    "        distances = cdist(features, centers, metric='euclidean')\n",
    "        closest_center_idx = np.argmin(distances)\n",
    "        if closest_center_idx in index_set:\n",
    "            result_list.append(row[1])\n",
    "    return result_list\n",
    "\n",
    "# Function to process elements of all DataFrames until the result_list reaches 500,000\n",
    "def process_elements(dfs, centers, index_set, max_len=200000):\n",
    "    result_list = []\n",
    "    delayed_results = []\n",
    "\n",
    "    for df in tqdm(dfs, desc=\"Processing DataFrames\"):\n",
    "        print(f\"Res {len(result_list)}\")\n",
    "        # Iterate over chunks of the DataFrame\n",
    "        for chunk in tqdm(df.to_delayed(), desc=\"Processing chunks\", leave=False):\n",
    "            # Process each chunk in parallel\n",
    "            delayed_result = dask.delayed(compute_closest_indexes)(chunk, centers, index_set)\n",
    "            delayed_results.append(delayed_result)\n",
    "            # Check if we have reached the max length\n",
    "            if len(result_list) >= max_len:\n",
    "                break\n",
    "\n",
    "        # Compute and collect results\n",
    "        if delayed_results:\n",
    "            chunk_results = dask.compute(*delayed_results)\n",
    "            for chunk_result in chunk_results:\n",
    "                result_list.extend(chunk_result)\n",
    "                if len(result_list) >= max_len:\n",
    "                    return result_list[:max_len]\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Measure the execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the pickle files\n",
    "with open(\"F:/ML_notebooks/cvfinal.pkl\", 'rb') as file:\n",
    "    index = pickle.load(file)\n",
    "with open(\"F:/ML_notebooks/cluster_centers.pkl\", 'rb') as file:\n",
    "    centers = pickle.load(file)\n",
    "\n",
    "# Convert index to a set for faster lookup\n",
    "index_set = set(index)\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_paths = [\n",
    "    \"G:/image_features.csv\",\n",
    "    \"G:/image_features1.csv\",\n",
    "    \"G:/image_features2.csv\",\n",
    "    \"G:/image_features3.csv\"\n",
    "]\n",
    "\n",
    "# Load the CSV files into Dask DataFrames\n",
    "dfs = [dd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Call the function to process elements of each DataFrame\n",
    "result_list = process_elements(dfs, centers, index_set)\n",
    "\n",
    "# Save the result_list in a pickle file\n",
    "with open(\"cifar_images.pkl\", \"wb\") as file:\n",
    "    pickle.dump(result_list, file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the execution time\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90056f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d18729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c9cd4-a50e-4b6c-b298-a8d179d3c77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c450d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f43ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
