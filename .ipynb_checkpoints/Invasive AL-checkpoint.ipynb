{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cbe26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab52343",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\__init__.py:134\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rnn_cell\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# TensorFlow Debugger (tfdbg).\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_numerics_callback\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dumping_callback\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_debug_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\debug\\__init__.py:48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-imports\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DebugDumpDir\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DebugTensorDatum\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_inf_or_nan\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\debug\\lib\\debug_data.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m event_pb2\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug_graphs\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:971\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:914\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1407\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1379\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1539\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:156\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:148\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "import glob\n",
    "\n",
    "invasive_dirs = [r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1 Image1\\Baylor Preserved Zebra Umbo 1 Image1\\Sorted Images\\Umbonal'\n",
    "]\n",
    "\n",
    "# Invasive category\n",
    "#invasive_dirs = [\n",
    "    r'D:\\VeligerData\\Baylor 2022-03-21_2\\Veligers',\n",
    "    r'D:\\VeligerData\\invasive',\n",
    "    r'D:\\VeligerData\\Ostracod vs Pedi-Veliger Examples\\Ostracod vs Pedi-Veliger Examples\\Preserved Zebra Ped 1 To Baylor\\Preserved Zebra Ped 1 To Baylor\\Sorted Images\\Pedi-Zebra Veligers',\n",
    "    r'D:\\VeligerData\\Ostracod vs Pedi-Veliger Examples\\Ostracod vs Pedi-Veliger Examples\\Preserved Zebra Ped 1a To Baylor\\Preserved Zebra Ped 1a To Baylor\\Sorted Images\\Preserved Zebra Ped 1a',\n",
    "    r'D:\\VeligerData\\To Baylor 2023-01-30\\To Baylor 2023-01-30\\Zebra Pediveliger Image1a\\Zebra Pediveligers',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1 Image1\\Baylor Preserved Zebra Umbo 1 Image1\\Sorted Images\\Umbonal',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1a Image1\\Baylor Preserved Zebra Umbo 1a Image1\\Sorted Images\\Umbonal',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Preserved Zebra D-Hinge 1 Baylor\\Preserved Zebra D-Hinge 1 Baylor\\Sorted Images\\Zebra D-Hinge',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1 Image1\\Baylor Preserved Zebra Umbo 1 Image1\\Sorted Images\\Umbonal'\n",
    "]\n",
    "\n",
    "# Non-Invasive category\n",
    "#non_invasive_dirs = [\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1 Image1\\Baylor Preserved Zebra Umbo 1 Image1\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\Baylor 2022-03-21_2\\NonVeligers\\Images_001',\n",
    "    r'D:\\VeligerData\\noninvasive',\n",
    "    r'D:\\VeligerData\\Ostracod vs Pedi-Veliger Examples\\Ostracod vs Pedi-Veliger Examples\\Preserved Ostracods 1 To Baylor\\Preserved Ostracods 1 To Baylor\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\Ostracod vs Pedi-Veliger Examples\\Ostracod vs Pedi-Veliger Examples\\Preserved Ostracods 1a To Baylor\\Preserved Ostracods 1a To Baylor\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\Ostracod vs Pedi-Veliger Examples\\Ostracod vs Pedi-Veliger Examples\\Preserved Zebra Ped 1 To Baylor\\Preserved Zebra Ped 1 To Baylor\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Preserved Zebra D-Hinge 1 Baylor\\Preserved Zebra D-Hinge 1 Baylor\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\Ostracods Day 2 Image1 To Baylor\\Not',\n",
    "    r'D:\\VeligerData\\Ostracods Day 2 Image2 To Baylor\\Not',\n",
    "    r'D:\\VeligerData\\Ostracods Day 2 Image12 To Baylor_3\\Ostracods Day 2 Image12 To Baylor\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\To Baylor 2023-01-30\\To Baylor 2023-01-30\\Not Veligers\\O1',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1 Image1\\Baylor Preserved Zebra Umbo 1 Image1\\Sorted Images\\Not',\n",
    "    r'D:\\VeligerData\\USGS Labled Zebra\\USGS Labled Zebra\\Baylor Preserved Zebra Umbo 1a Image1\\Baylor Preserved Zebra Umbo 1a Image1\\Sorted Images\\Not'\n",
    "]\n",
    "\n",
    "\n",
    "# List to store subdirectories\n",
    "invasive_subdirs = []\n",
    "non_invasive_subdirs = []\n",
    "\n",
    "# Collect subdirectories in the invasive category\n",
    "for invasive_dir in invasive_dirs:\n",
    "    invasive_subdirs.extend(glob.glob(invasive_dir))\n",
    "\n",
    "# Collect subdirectories in the non-invasive category\n",
    "for non_invasive_dir in non_invasive_dirs:\n",
    "    non_invasive_subdirs.extend(glob.glob(non_invasive_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3114ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ef8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# List of directories in x\n",
    "x = non_invasive_subdirs\n",
    "\n",
    "# List to store subdirectories\n",
    "y1 = []\n",
    "\n",
    "# Read subdirectories for each directory in x\n",
    "for directory in x:\n",
    "    subdirectories = glob.glob(directory + r'\\*')\n",
    "    y1.extend(subdirectories)\n",
    "\n",
    "import glob\n",
    "\n",
    "# List of directories in x\n",
    "x = invasive_subdirs\n",
    "\n",
    "# List to store subdirectories\n",
    "y2 = []\n",
    "\n",
    "# Read subdirectories for each directory in x\n",
    "for directory in x:\n",
    "    subdirectories = glob.glob(directory + r'\\*')\n",
    "    y2.extend(subdirectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c50cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_images(y1, label_num, target_size=(32, 32)):\n",
    "    # List to store image files\n",
    "    image_files = []\n",
    "    # List to store labels\n",
    "    labels = []\n",
    "\n",
    "    # Retrieve image files and create labels for each directory in y1\n",
    "    for directory in y1:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                # Check if the file has an image extension\n",
    "                if file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    # Add the file path to the image_files list\n",
    "                    image_files.append(os.path.join(root, file))\n",
    "                    # Add the label to the labels list\n",
    "                    \n",
    "\n",
    "    # List to store preprocessed images\n",
    "    images = []\n",
    "\n",
    "    # Preprocess each image\n",
    "    for file in image_files:\n",
    "        try:\n",
    "            # Read the image using PIL\n",
    "            image = Image.open(file)\n",
    "            # Resize the image using tf.image.resize_with_crop_or_pad()\n",
    "            image = tf.image.resize_with_crop_or_pad(\n",
    "                tf.keras.preprocessing.image.img_to_array(image),\n",
    "                target_size[0],\n",
    "                target_size[1]\n",
    "            )\n",
    "            # Normalize the image pixels for ML training\n",
    "            image = image / 255.0\n",
    "            # Add the preprocessed image to the images list\n",
    "            images.append(image)\n",
    "            labels.append(label_num)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {file}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "    # Convert the images and labels lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61753676",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0,Y0 = preprocess_images(y1, label_num = 0)\n",
    "X1,Y1 = preprocess_images(y2, label_num = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc704243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Concatenate the image arrays\n",
    "train_images = np.concatenate((X0, X1), axis=0)\n",
    "\n",
    "# Combine the label arrays\n",
    "train_labels = np.concatenate((Y0, Y1), axis=0)\n",
    "\n",
    "# Convert the labels to categorical\n",
    "#train_labels_categorical = to_categorical(train_labels)\n",
    "train_labels_categorical = train_labels\n",
    "\n",
    "# Set the dtype of train_images to float32\n",
    "train_images = train_images.astype('float32')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Assuming you have the original train_images and train_labels_categorical\n",
    "\n",
    "# Shuffle the data\n",
    "train_images, train_labels_categorical = shuffle(train_images, train_labels_categorical)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(train_images, train_labels_categorical, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b6aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a59d3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, x_unlab, Y_train, y_unlab = train_test_split( X_train, Y_train, test_size=0.8, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c7d9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def test(model, X_test, Y_test):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Test set evaluation: \", model.evaluate( X_test, Y_test , verbose=0, return_dict=True, batch_size = 2000), )\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Convert one-hot encoded predictions to class labels.\n",
    "    y_pred_classes = y_pred.argmax(axis=1)\n",
    "    Y_test_classes = Y_test.argmax(axis=1)\n",
    "\n",
    "    # Calculate the F1 score.\n",
    "    f1 = f1_score(Y_test_classes, y_pred_classes, average='weighted')\n",
    "    \n",
    "    # Calculate the F1 score for class label 1.\n",
    "    class_label = 1\n",
    "    f1_class_label_1 = f1_score(Y_test_classes, y_pred_classes, labels=[class_label], average=None)\n",
    "\n",
    "    print(\"F1 score for class label 1:\", f1_class_label_1[0])\n",
    "    print(\"F1 score on test data:\", f1)\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "def train(model,X_train, Y_train, epoch):\n",
    "    \n",
    "    print(f\"Starting to train with {len(Y_train)} samples\")\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size = 2000, epochs=epoch,validation_split=.20)\n",
    "\n",
    "    return model\n",
    "\n",
    "def print_acc(model, X_test, Y_test):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    # Convert one-hot encoded predictions to class labels.\n",
    "    y_pred_classes = y_pred.argmax(axis=1)\n",
    "    Y_test_classes = Y_test\n",
    "\n",
    "    # Calculate the F1 score.\n",
    "    f1 = f1_score(Y_test_classes, y_pred_classes, average='weighted')\n",
    "    \n",
    "    # Calculate the F1 score for class label 1.\n",
    "    class_label = 1\n",
    "    f1_class_label_1 = f1_score(Y_test_classes, y_pred_classes, labels=[class_label], average=None)\n",
    "\n",
    "    print(\"F1 score for class label 1:\", f1_class_label_1[0])\n",
    "    print(\"F1 score on test data:\", f1)\n",
    "    print(\"-\" * 100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592332d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c514f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee15786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    base_model = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "    # Add custom top layers\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bc3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd16bf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train with 32202 samples\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 12s 346ms/step - loss: 0.3043 - accuracy: 0.8752 - val_loss: 1.4191 - val_accuracy: 0.8309\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 4s 272ms/step - loss: 0.1130 - accuracy: 0.9572 - val_loss: 2.1529 - val_accuracy: 0.6424\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 3s 267ms/step - loss: 0.0845 - accuracy: 0.9663 - val_loss: 1.1104 - val_accuracy: 0.7586\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 4s 271ms/step - loss: 0.0696 - accuracy: 0.9716 - val_loss: 0.8045 - val_accuracy: 0.8427\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 4s 273ms/step - loss: 0.0602 - accuracy: 0.9744 - val_loss: 0.6839 - val_accuracy: 0.8946\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 4s 276ms/step - loss: 0.0531 - accuracy: 0.9785 - val_loss: 1.2566 - val_accuracy: 0.8907\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 4s 274ms/step - loss: 0.0488 - accuracy: 0.9801 - val_loss: 0.4345 - val_accuracy: 0.9421\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.0450 - accuracy: 0.9801 - val_loss: 0.9453 - val_accuracy: 0.9155\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 4s 281ms/step - loss: 0.0369 - accuracy: 0.9845 - val_loss: 0.4495 - val_accuracy: 0.9481\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 4s 279ms/step - loss: 0.0318 - accuracy: 0.9873 - val_loss: 1.1368 - val_accuracy: 0.8966\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 4s 285ms/step - loss: 0.0339 - accuracy: 0.9867 - val_loss: 0.3247 - val_accuracy: 0.9472\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 4s 287ms/step - loss: 0.0303 - accuracy: 0.9884 - val_loss: 0.4795 - val_accuracy: 0.9351\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 4s 282ms/step - loss: 0.0269 - accuracy: 0.9885 - val_loss: 0.5132 - val_accuracy: 0.9357\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 4s 288ms/step - loss: 0.0283 - accuracy: 0.9891 - val_loss: 0.2993 - val_accuracy: 0.9551\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 4s 292ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.3620 - val_accuracy: 0.9533\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 4s 295ms/step - loss: 0.0171 - accuracy: 0.9934 - val_loss: 0.2660 - val_accuracy: 0.9514\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 4s 288ms/step - loss: 0.0233 - accuracy: 0.9916 - val_loss: 0.2191 - val_accuracy: 0.9623\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 4s 290ms/step - loss: 0.0213 - accuracy: 0.9919 - val_loss: 0.2612 - val_accuracy: 0.9489\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 4s 290ms/step - loss: 0.0134 - accuracy: 0.9949 - val_loss: 0.2765 - val_accuracy: 0.9556\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 4s 294ms/step - loss: 0.0105 - accuracy: 0.9959 - val_loss: 0.2703 - val_accuracy: 0.9526\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 4s 290ms/step - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.2518 - val_accuracy: 0.9545\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 4s 288ms/step - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.3186 - val_accuracy: 0.9528\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 4s 289ms/step - loss: 0.0138 - accuracy: 0.9947 - val_loss: 0.2324 - val_accuracy: 0.9589\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 4s 290ms/step - loss: 0.0092 - accuracy: 0.9964 - val_loss: 0.2117 - val_accuracy: 0.9615\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 4s 292ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.2970 - val_accuracy: 0.9544\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 4s 295ms/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.2029 - val_accuracy: 0.9638\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 4s 301ms/step - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.2194 - val_accuracy: 0.9590\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 4s 327ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.2070 - val_accuracy: 0.9638\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 4s 321ms/step - loss: 0.0136 - accuracy: 0.9951 - val_loss: 0.2237 - val_accuracy: 0.9568\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 4s 321ms/step - loss: 0.0141 - accuracy: 0.9946 - val_loss: 0.4014 - val_accuracy: 0.9388\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 4s 327ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.2512 - val_accuracy: 0.9598\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 6s 452ms/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 0.2629 - val_accuracy: 0.9635\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 4s 340ms/step - loss: 0.0057 - accuracy: 0.9975 - val_loss: 0.2199 - val_accuracy: 0.9654\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 5s 396ms/step - loss: 0.0037 - accuracy: 0.9984 - val_loss: 0.2806 - val_accuracy: 0.9623\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 6s 453ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.3545 - val_accuracy: 0.9584\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 7s 565ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.3473 - val_accuracy: 0.9522\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 5s 391ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.2463 - val_accuracy: 0.9610\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 10s 774ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.2347 - val_accuracy: 0.9564\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 7s 519ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.2260 - val_accuracy: 0.9606\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 8s 640ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.2144 - val_accuracy: 0.9640\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 9s 723ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.1876 - val_accuracy: 0.9685\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 10s 726ms/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.1854 - val_accuracy: 0.9638\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 6s 451ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1714 - val_accuracy: 0.9637\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 9s 659ms/step - loss: 0.0040 - accuracy: 0.9983 - val_loss: 0.1716 - val_accuracy: 0.9685\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 8s 642ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.1756 - val_accuracy: 0.9683\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 9s 690ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.1746 - val_accuracy: 0.9637\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 9s 715ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.2476 - val_accuracy: 0.9615\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 9s 666ms/step - loss: 0.0055 - accuracy: 0.9984 - val_loss: 0.1959 - val_accuracy: 0.9674\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 9s 683ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.2667 - val_accuracy: 0.9610\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 8s 661ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.2147 - val_accuracy: 0.9644\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 0.22327367961406708, 'accuracy': 0.9616566896438599}\n",
      "F1 score for class label 1: 0.9130985558652139\n",
      "F1 score on test data: 0.9610751433992732\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "Starting to train with 35422 samples\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 11s 380ms/step - loss: 0.0888 - accuracy: 0.9778 - val_loss: 38.6592 - val_accuracy: 0.8092\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 4s 283ms/step - loss: 0.0409 - accuracy: 0.9855 - val_loss: 31.9128 - val_accuracy: 0.8662\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 4s 281ms/step - loss: 0.0217 - accuracy: 0.9926 - val_loss: 24.2595 - val_accuracy: 0.8282\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 4s 280ms/step - loss: 0.0232 - accuracy: 0.9918 - val_loss: 7.3288 - val_accuracy: 0.9035\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 4s 277ms/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 3.7997 - val_accuracy: 0.9039\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 5s 305ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.6053 - val_accuracy: 0.9450\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 6s 394ms/step - loss: 0.0145 - accuracy: 0.9952 - val_loss: 0.6144 - val_accuracy: 0.9417\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 10s 694ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.4555 - val_accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 9s 600ms/step - loss: 0.0241 - accuracy: 0.9911 - val_loss: 0.6190 - val_accuracy: 0.9218\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 11s 704ms/step - loss: 0.0215 - accuracy: 0.9920 - val_loss: 0.4063 - val_accuracy: 0.9396\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 0.3923397362232208, 'accuracy': 0.9408082365989685}\n",
      "F1 score for class label 1: 0.8787462823152596\n",
      "F1 score on test data: 0.9419309988990534\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "Starting to train with 38642 samples\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 11s 364ms/step - loss: 0.0402 - accuracy: 0.9867 - val_loss: 7.0351 - val_accuracy: 0.8619\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 5s 295ms/step - loss: 0.0237 - accuracy: 0.9923 - val_loss: 50.0322 - val_accuracy: 0.8409\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 5s 294ms/step - loss: 0.0165 - accuracy: 0.9944 - val_loss: 45.7871 - val_accuracy: 0.8534\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 5s 285ms/step - loss: 0.0119 - accuracy: 0.9959 - val_loss: 4.2413 - val_accuracy: 0.9235\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 5s 304ms/step - loss: 0.0112 - accuracy: 0.9958 - val_loss: 4.9123 - val_accuracy: 0.9585\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 5s 346ms/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 2.3005 - val_accuracy: 0.9433\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 12s 725ms/step - loss: 0.0080 - accuracy: 0.9975 - val_loss: 1.0237 - val_accuracy: 0.9631\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 8s 495ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.4273 - val_accuracy: 0.9568\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 12s 772ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 1.3306 - val_accuracy: 0.9520\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 11s 719ms/step - loss: 0.0085 - accuracy: 0.9972 - val_loss: 0.8300 - val_accuracy: 0.9607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 1.3978304862976074, 'accuracy': 0.957352876663208}\n",
      "F1 score for class label 1: 0.9094151212553495\n",
      "F1 score on test data: 0.9577313992681624\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "Starting to train with 41862 samples\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 15s 357ms/step - loss: 0.0328 - accuracy: 0.9902 - val_loss: 6.3545 - val_accuracy: 0.9219\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 5s 292ms/step - loss: 0.0158 - accuracy: 0.9946 - val_loss: 1.2211 - val_accuracy: 0.9532\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 5s 298ms/step - loss: 0.0105 - accuracy: 0.9965 - val_loss: 1.5132 - val_accuracy: 0.9555\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 5s 300ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 0.4826 - val_accuracy: 0.9625\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 5s 320ms/step - loss: 0.0074 - accuracy: 0.9972 - val_loss: 0.3561 - val_accuracy: 0.9571\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 10s 633ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.2652 - val_accuracy: 0.9589\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 6s 376ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.6040 - val_accuracy: 0.9596\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 10s 551ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.4663 - val_accuracy: 0.9600\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 10s 582ms/step - loss: 0.0072 - accuracy: 0.9973 - val_loss: 1.0882 - val_accuracy: 0.9473\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 12s 675ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.4291 - val_accuracy: 0.9470\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 0.46891096234321594, 'accuracy': 0.9446090459823608}\n",
      "F1 score for class label 1: 0.8857142857142856\n",
      "F1 score on test data: 0.9455344259343911\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "Starting to train with 45082 samples\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 11s 331ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.8686 - val_accuracy: 0.9514\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 5s 284ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 6.9840 - val_accuracy: 0.9488\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 5s 286ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 2.4489 - val_accuracy: 0.9401\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 5s 289ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 0.2952 - val_accuracy: 0.9286\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 6s 312ms/step - loss: 0.0152 - accuracy: 0.9941 - val_loss: 0.4129 - val_accuracy: 0.9655\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 6s 327ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 0.3160 - val_accuracy: 0.9550\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 8s 444ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 0.6647 - val_accuracy: 0.9521\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 10s 520ms/step - loss: 0.0295 - accuracy: 0.9894 - val_loss: 0.6355 - val_accuracy: 0.9543\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 12s 645ms/step - loss: 0.0200 - accuracy: 0.9931 - val_loss: 0.3880 - val_accuracy: 0.9386\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 12s 639ms/step - loss: 0.0176 - accuracy: 0.9933 - val_loss: 1.0339 - val_accuracy: 0.9554\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 1.37045419216156, 'accuracy': 0.9523782730102539}\n",
      "F1 score for class label 1: 0.8972594470602439\n",
      "F1 score on test data: 0.9525445382739515\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "Starting to train with 48302 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 15s 334ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.2326 - val_accuracy: 0.9577\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 6s 286ms/step - loss: 0.0185 - accuracy: 0.9952 - val_loss: 0.3164 - val_accuracy: 0.9595\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 6s 293ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.6381 - val_accuracy: 0.9358\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 9s 416ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.1872 - val_accuracy: 0.9645\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 12s 599ms/step - loss: 0.0114 - accuracy: 0.9961 - val_loss: 0.4565 - val_accuracy: 0.9571\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 12s 616ms/step - loss: 0.0074 - accuracy: 0.9975 - val_loss: 0.2598 - val_accuracy: 0.9619\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 15s 776ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.1860 - val_accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 13s 649ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.1874 - val_accuracy: 0.9704\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 13s 669ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.2628 - val_accuracy: 0.9611\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 16s 754ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.2735 - val_accuracy: 0.9607\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test set evaluation:  {'loss': 0.3062928020954132, 'accuracy': 0.9575764536857605}\n",
      "F1 score for class label 1: 0.9055852717999752\n",
      "F1 score on test data: 0.9572222437566031\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INFO:tensorflow:Assets written to: saved_model/my_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n",
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\keras\\saving\\saved_model\\layer_serialization.py:112: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  return generic_utils.serialize_keras_object(obj)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def train_active_learning_models(\n",
    "    model,\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    x_unlab,\n",
    "    y_unlab,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_iterations=5):\n",
    "    \n",
    "    test(model, X_test, Y_test)\n",
    "    \n",
    "    d = 100/num_iterations\n",
    "    l = len(y_unlab)\n",
    "    x = int(len(Y_train)/10)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "\n",
    "        model = tf.keras.models.load_model('saved_model/my_model')\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "         \n",
    "        #generate random number and substract from all numbers\n",
    "        rnd = random.sample(range(0, len(x_unlab)), x)\n",
    "        all_indices = list(range(1, l))\n",
    "        main_list = list(set(all_indices) - set(rnd))\n",
    "        \n",
    "        #add those index to from unlablled set to training set\n",
    "        img_list = ([x_unlab[i] for i in rnd])\n",
    "        new_lab = img_list\n",
    "        arr = np.concatenate((X_train, new_lab))\n",
    "        X_train = arr\n",
    "\n",
    "        #check labels in the set and add to training data\n",
    "        new_y = y_unlab[rnd]\n",
    "        arr = np.concatenate((Y_train, new_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = ([x_unlab[i] for i in main_list])\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        #train on data\n",
    "        model = train(model,X_train, Y_train, 10)\n",
    "        \n",
    "        #test for final time\n",
    "        test(model, X_test, Y_test)\n",
    "\n",
    "        model.save('saved_model/my_model')\n",
    "\n",
    "        del model\n",
    "\n",
    "model = create_model()\n",
    "    \n",
    "model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"Adam\",\n",
    "        metrics='accuracy'   )\n",
    "    \n",
    "model = train(model,X_train, Y_train, 50)\n",
    "\n",
    "model.save('saved_model/my_model')\n",
    "\n",
    "active_learning_model = train_active_learning_models(model,X_train, Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55ec256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.02),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a38b6cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 3)     7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_encoder():\n",
    "    resnet = tf.keras.applications.ResNet50V2( include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling=\"avg\" )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "280d76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        logits = tf.divide( tf.matmul(  \n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)),self.temperature,)\n",
    "        \n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cad60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef0ecd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42735712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - 43s 161ms/step - loss: 7.0981\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 10s 82ms/step - loss: 5.3156\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 9s 73ms/step - loss: 5.2418\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 5.2134\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 5.2016\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 9s 71ms/step - loss: 5.1868\n",
      "Epoch 7/50\n",
      " 34/126 [=======>......................] - ETA: 6s - loss: 5.17"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "  \n",
    "        x_ulb = encoder.predict(x_unlab, batch_size=128)\n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 2\n",
    "        \n",
    "        budget =  2500\n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*2)\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        annotate_indices = []\n",
    "        \n",
    "        \n",
    "        for i in range(nn_clusters):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]]\n",
    "            \n",
    "            pts = cluster_indices[np.argsort(distances,-1)[:1]]\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "            \n",
    "\n",
    "        print(np.shape(annotate_indices))\n",
    "\n",
    "        annt = annotate_indices      \n",
    "        \n",
    "        ante = x_ulb[annt]\n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(annt))\n",
    "        \n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train, new_annt))\n",
    "        X_train = arr\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=10)\n",
    "\n",
    "\n",
    "        history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "\n",
    "\n",
    "        print_acc(classifier, X_test, Y_test)\n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "       \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=256, epochs=50)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=32, epochs=10) \n",
    "\n",
    "try :\n",
    "  accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "\n",
    "print(\"Acc : \")\n",
    "print(\"\\n\")\n",
    "print(accuracy)\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "675b7cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25604"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5403c2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1de25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31083b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be88f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e2ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd07929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
