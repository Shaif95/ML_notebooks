{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5419b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fca053a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn='D:/data/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "273abbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))\n",
    "\n",
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c5440a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daeffb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "798d6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    imgdata=[]\n",
    "    for i in range(0,15):\n",
    "        \n",
    "        a = Image.open(x[i])\n",
    "        b = a.resize((96, 96))\n",
    "        c = np.array(b)\n",
    "        imgdata.append(c.reshape(96, 96,3))\n",
    "        \n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata\n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    #print(np.shape(X_train))\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e352d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b004fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28523f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bded865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(My_Test_Generator(test, batch_size).__getitem__(1))\n",
    "for i in range(2,len(x)):\n",
    "    x = My_Test_Generator(test, batch_size).__getitem__(i)\n",
    "    arr = np.concatenate((arr,x),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ffe4a3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15, 96, 96, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d616e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3eb0f671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_96 (TimeDi  (None, 15, 94, 94, 64)   1792      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_97 (TimeDi  (None, 15, 47, 47, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_98 (TimeDi  (None, 15, 45, 45, 32)   18464     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_99 (TimeDi  (None, 15, 22, 22, 32)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_100 (TimeD  (None, 15, 20, 20, 16)   4624      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_101 (TimeD  (None, 15, 10, 10, 16)   0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_102 (TimeD  (None, 15, 8, 8, 8)      1160      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_103 (TimeD  (None, 15, 4, 4, 8)      0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_104 (TimeD  (None, 15, 2, 2, 4)      292       \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_105 (TimeD  (None, 15, 16)           0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 50)                13400     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 15)                765       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,497\n",
      "Trainable params: 40,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "\n",
    "num_frames = 15\n",
    "frame_height = 96\n",
    "frame_width = 96\n",
    "num_channels = 3\n",
    "num_classes = 15  # Change this to the number of classes in your dataset\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# CNN\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu'), input_shape=(num_frames, frame_height, frame_width, num_channels)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(16, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(8, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(4, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# LSTM\n",
    "model.add(LSTM(50))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81e2a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "optimizer = keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67fd3368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_1528\\789722107.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=my_training_batch_generator, epochs = 100,validation_data = my_validation_batch_generator)\n",
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_1528\\2168406815.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 159s 1s/step - loss: 2.6785 - accuracy: 0.0939 - val_loss: 2.6556 - val_accuracy: 0.0986\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 85s 722ms/step - loss: 2.6049 - accuracy: 0.1254 - val_loss: 2.5272 - val_accuracy: 0.1596\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 83s 705ms/step - loss: 2.5021 - accuracy: 0.1435 - val_loss: 2.4520 - val_accuracy: 0.1502\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 78s 659ms/step - loss: 2.4620 - accuracy: 0.1638 - val_loss: 2.3922 - val_accuracy: 0.1737\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 73s 614ms/step - loss: 2.3938 - accuracy: 0.1798 - val_loss: 2.3723 - val_accuracy: 0.1690\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 76s 651ms/step - loss: 2.3419 - accuracy: 0.1996 - val_loss: 2.3017 - val_accuracy: 0.2113\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 75s 635ms/step - loss: 2.2630 - accuracy: 0.2332 - val_loss: 2.3014 - val_accuracy: 0.1878\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 72s 612ms/step - loss: 2.2197 - accuracy: 0.2657 - val_loss: 2.2442 - val_accuracy: 0.2066\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 76s 643ms/step - loss: 2.1476 - accuracy: 0.2759 - val_loss: 2.2602 - val_accuracy: 0.2066\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 80s 673ms/step - loss: 2.0875 - accuracy: 0.3052 - val_loss: 2.2518 - val_accuracy: 0.2113\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 78s 655ms/step - loss: 2.0594 - accuracy: 0.3058 - val_loss: 2.1809 - val_accuracy: 0.2629\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 80s 677ms/step - loss: 1.9832 - accuracy: 0.3228 - val_loss: 2.1720 - val_accuracy: 0.2629\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 85s 715ms/step - loss: 1.9642 - accuracy: 0.3378 - val_loss: 2.1266 - val_accuracy: 0.2582\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 78s 654ms/step - loss: 1.9071 - accuracy: 0.3570 - val_loss: 2.1974 - val_accuracy: 0.2582\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 85s 714ms/step - loss: 1.8453 - accuracy: 0.3821 - val_loss: 2.1695 - val_accuracy: 0.2582\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 82s 695ms/step - loss: 1.7799 - accuracy: 0.4072 - val_loss: 2.1446 - val_accuracy: 0.2676\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 77s 650ms/step - loss: 1.7442 - accuracy: 0.4088 - val_loss: 2.1817 - val_accuracy: 0.2676\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 85s 715ms/step - loss: 1.7392 - accuracy: 0.4248 - val_loss: 2.1363 - val_accuracy: 0.2958\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 82s 689ms/step - loss: 1.6764 - accuracy: 0.4221 - val_loss: 2.1037 - val_accuracy: 0.2911\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 118s 1s/step - loss: 1.6219 - accuracy: 0.4557 - val_loss: 2.2308 - val_accuracy: 0.2817\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 494s 4s/step - loss: 1.6079 - accuracy: 0.4546 - val_loss: 2.2576 - val_accuracy: 0.2441\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 120s 1s/step - loss: 1.5837 - accuracy: 0.4685 - val_loss: 2.1409 - val_accuracy: 0.3005\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 88s 750ms/step - loss: 1.5193 - accuracy: 0.5016 - val_loss: 2.0868 - val_accuracy: 0.3333\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 89s 758ms/step - loss: 1.4503 - accuracy: 0.5128 - val_loss: 2.1664 - val_accuracy: 0.3146\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 80s 678ms/step - loss: 1.4414 - accuracy: 0.5213 - val_loss: 2.1518 - val_accuracy: 0.3099\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 84s 714ms/step - loss: 1.3544 - accuracy: 0.5464 - val_loss: 2.2243 - val_accuracy: 0.3146\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 77s 652ms/step - loss: 1.3487 - accuracy: 0.5518 - val_loss: 2.1577 - val_accuracy: 0.3005\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 78s 654ms/step - loss: 1.3209 - accuracy: 0.5667 - val_loss: 2.1430 - val_accuracy: 0.3286\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 85s 722ms/step - loss: 1.3258 - accuracy: 0.5582 - val_loss: 2.2574 - val_accuracy: 0.3286\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 75s 635ms/step - loss: 1.2610 - accuracy: 0.5816 - val_loss: 2.2143 - val_accuracy: 0.3333\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 78s 660ms/step - loss: 1.2609 - accuracy: 0.5795 - val_loss: 2.2399 - val_accuracy: 0.3192\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 78s 662ms/step - loss: 1.1790 - accuracy: 0.6297 - val_loss: 2.2060 - val_accuracy: 0.3380\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 73s 614ms/step - loss: 1.1512 - accuracy: 0.6153 - val_loss: 2.2587 - val_accuracy: 0.3380\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 70s 592ms/step - loss: 1.1584 - accuracy: 0.6115 - val_loss: 2.2375 - val_accuracy: 0.3099\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 84s 709ms/step - loss: 1.0963 - accuracy: 0.6430 - val_loss: 2.2094 - val_accuracy: 0.3427\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 75s 638ms/step - loss: 1.0878 - accuracy: 0.6537 - val_loss: 2.1908 - val_accuracy: 0.3192\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 74s 625ms/step - loss: 1.0064 - accuracy: 0.6772 - val_loss: 2.1756 - val_accuracy: 0.3568\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 75s 629ms/step - loss: 0.9967 - accuracy: 0.6836 - val_loss: 2.3261 - val_accuracy: 0.3099\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 72s 612ms/step - loss: 1.0954 - accuracy: 0.6435 - val_loss: 2.3623 - val_accuracy: 0.3192\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 72s 605ms/step - loss: 1.0336 - accuracy: 0.6660 - val_loss: 2.2401 - val_accuracy: 0.3474\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 74s 625ms/step - loss: 0.9391 - accuracy: 0.7060 - val_loss: 2.2906 - val_accuracy: 0.3333\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 75s 635ms/step - loss: 0.9183 - accuracy: 0.7145 - val_loss: 2.3090 - val_accuracy: 0.3192\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 67s 567ms/step - loss: 0.8715 - accuracy: 0.7177 - val_loss: 2.3782 - val_accuracy: 0.3146\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 67s 572ms/step - loss: 0.8864 - accuracy: 0.7134 - val_loss: 2.5058 - val_accuracy: 0.3239\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 79s 666ms/step - loss: 0.8543 - accuracy: 0.7161 - val_loss: 2.2691 - val_accuracy: 0.3662\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 72s 611ms/step - loss: 0.7887 - accuracy: 0.7481 - val_loss: 2.5663 - val_accuracy: 0.3052\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 75s 630ms/step - loss: 0.7656 - accuracy: 0.7588 - val_loss: 2.4721 - val_accuracy: 0.3146\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 78s 658ms/step - loss: 0.8026 - accuracy: 0.7433 - val_loss: 2.5529 - val_accuracy: 0.3099\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 71s 604ms/step - loss: 0.7406 - accuracy: 0.7721 - val_loss: 2.5493 - val_accuracy: 0.3286\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 70s 592ms/step - loss: 0.7103 - accuracy: 0.7844 - val_loss: 2.4080 - val_accuracy: 0.3286\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 73s 620ms/step - loss: 0.7313 - accuracy: 0.7743 - val_loss: 2.6025 - val_accuracy: 0.2911\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 75s 637ms/step - loss: 0.7592 - accuracy: 0.7551 - val_loss: 2.6026 - val_accuracy: 0.3005\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 71s 596ms/step - loss: 0.8260 - accuracy: 0.7327 - val_loss: 2.4789 - val_accuracy: 0.3052\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 73s 619ms/step - loss: 0.7151 - accuracy: 0.7657 - val_loss: 2.4701 - val_accuracy: 0.3286\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 79s 669ms/step - loss: 0.6446 - accuracy: 0.8020 - val_loss: 2.7098 - val_accuracy: 0.2958\n",
      "Epoch 56/100\n",
      "118/118 [==============================] - 71s 595ms/step - loss: 0.5588 - accuracy: 0.8372 - val_loss: 2.6073 - val_accuracy: 0.3380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "118/118 [==============================] - 70s 591ms/step - loss: 0.6081 - accuracy: 0.8191 - val_loss: 2.6377 - val_accuracy: 0.3380\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 76s 644ms/step - loss: 0.6226 - accuracy: 0.8079 - val_loss: 2.7070 - val_accuracy: 0.3239\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 71s 599ms/step - loss: 0.5924 - accuracy: 0.8202 - val_loss: 2.6308 - val_accuracy: 0.3239\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 70s 592ms/step - loss: 0.6718 - accuracy: 0.7892 - val_loss: 2.7810 - val_accuracy: 0.3286\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 74s 629ms/step - loss: 0.5249 - accuracy: 0.8495 - val_loss: 2.7759 - val_accuracy: 0.3099\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 73s 613ms/step - loss: 0.4720 - accuracy: 0.8639 - val_loss: 2.8468 - val_accuracy: 0.3239\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 70s 594ms/step - loss: 0.4514 - accuracy: 0.8751 - val_loss: 2.9157 - val_accuracy: 0.3239\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 72s 611ms/step - loss: 0.4520 - accuracy: 0.8746 - val_loss: 2.9144 - val_accuracy: 0.3052\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 76s 645ms/step - loss: 0.4321 - accuracy: 0.8746 - val_loss: 2.8366 - val_accuracy: 0.3192\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 71s 598ms/step - loss: 0.4141 - accuracy: 0.8933 - val_loss: 2.9830 - val_accuracy: 0.2958\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 70s 591ms/step - loss: 0.5549 - accuracy: 0.8351 - val_loss: 2.8262 - val_accuracy: 0.3568\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 77s 649ms/step - loss: 0.5990 - accuracy: 0.8164 - val_loss: 3.0499 - val_accuracy: 0.3005\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 69s 592ms/step - loss: 0.6546 - accuracy: 0.7940 - val_loss: 3.0494 - val_accuracy: 0.3146\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 70s 595ms/step - loss: 0.4629 - accuracy: 0.8719 - val_loss: 2.9575 - val_accuracy: 0.3146\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 76s 643ms/step - loss: 0.4302 - accuracy: 0.8863 - val_loss: 2.8930 - val_accuracy: 0.3099\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 73s 620ms/step - loss: 0.4450 - accuracy: 0.8757 - val_loss: 3.0603 - val_accuracy: 0.3052\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 71s 602ms/step - loss: 1.1127 - accuracy: 0.6451 - val_loss: 3.0277 - val_accuracy: 0.2911\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 79s 670ms/step - loss: 0.5643 - accuracy: 0.8228 - val_loss: 2.8733 - val_accuracy: 0.2958\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 79s 665ms/step - loss: 0.4192 - accuracy: 0.8949 - val_loss: 3.0793 - val_accuracy: 0.3099\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 76s 643ms/step - loss: 0.3421 - accuracy: 0.9210 - val_loss: 2.9227 - val_accuracy: 0.3474\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 74s 629ms/step - loss: 0.3343 - accuracy: 0.9205 - val_loss: 3.0090 - val_accuracy: 0.3380\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 79s 673ms/step - loss: 0.3103 - accuracy: 0.9269 - val_loss: 3.0341 - val_accuracy: 0.2864\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 74s 629ms/step - loss: 0.2679 - accuracy: 0.9424 - val_loss: 3.1513 - val_accuracy: 0.3192\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 74s 620ms/step - loss: 0.3116 - accuracy: 0.9280 - val_loss: 3.0610 - val_accuracy: 0.3146\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 80s 681ms/step - loss: 0.4835 - accuracy: 0.8597 - val_loss: 3.1352 - val_accuracy: 0.3380\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 72s 610ms/step - loss: 0.3749 - accuracy: 0.8970 - val_loss: 3.0441 - val_accuracy: 0.3333\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 73s 616ms/step - loss: 0.2935 - accuracy: 0.9264 - val_loss: 3.0610 - val_accuracy: 0.3615\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 81s 685ms/step - loss: 0.3756 - accuracy: 0.8981 - val_loss: 3.2602 - val_accuracy: 0.3380\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 74s 627ms/step - loss: 0.3855 - accuracy: 0.8997 - val_loss: 3.3376 - val_accuracy: 0.3239\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 72s 614ms/step - loss: 0.3436 - accuracy: 0.9061 - val_loss: 3.0631 - val_accuracy: 0.3239\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 83s 701ms/step - loss: 0.3248 - accuracy: 0.9200 - val_loss: 3.2053 - val_accuracy: 0.3286\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 75s 633ms/step - loss: 0.2243 - accuracy: 0.9600 - val_loss: 3.3869 - val_accuracy: 0.3239\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 73s 619ms/step - loss: 0.1733 - accuracy: 0.9733 - val_loss: 3.2242 - val_accuracy: 0.3239\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 79s 664ms/step - loss: 0.3667 - accuracy: 0.9018 - val_loss: 3.5627 - val_accuracy: 0.3333\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 74s 629ms/step - loss: 0.5162 - accuracy: 0.8420 - val_loss: 3.5189 - val_accuracy: 0.2864\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 73s 618ms/step - loss: 0.5131 - accuracy: 0.8388 - val_loss: 3.3095 - val_accuracy: 0.3146\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 77s 648ms/step - loss: 0.3884 - accuracy: 0.8906 - val_loss: 3.4656 - val_accuracy: 0.3286\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 78s 659ms/step - loss: 0.3207 - accuracy: 0.9125 - val_loss: 3.4143 - val_accuracy: 0.2958\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 75s 633ms/step - loss: 0.3290 - accuracy: 0.9007 - val_loss: 3.6329 - val_accuracy: 0.3052\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 77s 650ms/step - loss: 0.2414 - accuracy: 0.9434 - val_loss: 3.4567 - val_accuracy: 0.3286\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 76s 643ms/step - loss: 0.2128 - accuracy: 0.9568 - val_loss: 3.5548 - val_accuracy: 0.3474\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 78s 655ms/step - loss: 0.1806 - accuracy: 0.9664 - val_loss: 3.6584 - val_accuracy: 0.2817\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 72s 612ms/step - loss: 0.1661 - accuracy: 0.9723 - val_loss: 3.6848 - val_accuracy: 0.2958\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 78s 660ms/step - loss: 0.1325 - accuracy: 0.9851 - val_loss: 3.5911 - val_accuracy: 0.3192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27a41b7bc10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 100,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460f9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(My_Test_Generator(test, batch_size), verbose=0)\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import Ofrom sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, p)\n",
    "scoreneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "p = np.argmax(predictions, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, p)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ab8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossAndMetrics = model.evaluate(arr, te_y)\n",
    "lossAndMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1c09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69843fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(4, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                (layers.Conv2D(8, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2649eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (10,40,40,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(10, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.3)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "outputs = layers.Dense(units=15, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98506d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\AppData\\Local\\Temp\\ipykernel_788\\1958852391.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=my_training_batch_generator, epochs = 50,validation_data = my_validation_batch_generator)\n",
      "C:\\Users\\chowd\\AppData\\Local\\Temp\\ipykernel_788\\2168406815.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "59/59 [==============================] - 80s 1s/step - loss: 2.7010 - accuracy: 0.0975 - val_loss: 2.5393 - val_accuracy: 0.1495\n",
      "Epoch 2/50\n",
      "59/59 [==============================] - 75s 1s/step - loss: 2.4694 - accuracy: 0.1551 - val_loss: 2.3966 - val_accuracy: 0.1916\n",
      "Epoch 3/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 2.3188 - accuracy: 0.2116 - val_loss: 2.2126 - val_accuracy: 0.2383\n",
      "Epoch 4/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 2.1786 - accuracy: 0.2425 - val_loss: 2.0953 - val_accuracy: 0.2850\n",
      "Epoch 5/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 2.0074 - accuracy: 0.3177 - val_loss: 2.0630 - val_accuracy: 0.3131\n",
      "Epoch 6/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 1.8937 - accuracy: 0.3609 - val_loss: 2.0963 - val_accuracy: 0.3551\n",
      "Epoch 7/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 1.7027 - accuracy: 0.4270 - val_loss: 2.1092 - val_accuracy: 0.3598\n",
      "Epoch 8/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 1.6063 - accuracy: 0.4659 - val_loss: 2.0904 - val_accuracy: 0.3411\n",
      "Epoch 9/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 1.4988 - accuracy: 0.5075 - val_loss: 2.1328 - val_accuracy: 0.3271\n",
      "Epoch 10/50\n",
      "59/59 [==============================] - 75s 1s/step - loss: 1.3796 - accuracy: 0.5384 - val_loss: 2.1468 - val_accuracy: 0.3692\n",
      "Epoch 11/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 1.2801 - accuracy: 0.5821 - val_loss: 2.1217 - val_accuracy: 0.4159\n",
      "Epoch 12/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 1.1926 - accuracy: 0.5991 - val_loss: 2.2305 - val_accuracy: 0.3925\n",
      "Epoch 13/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 1.0973 - accuracy: 0.6343 - val_loss: 2.1840 - val_accuracy: 0.3972\n",
      "Epoch 14/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.9823 - accuracy: 0.6818 - val_loss: 2.2423 - val_accuracy: 0.4206\n",
      "Epoch 15/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.8933 - accuracy: 0.7111 - val_loss: 2.3906 - val_accuracy: 0.4019\n",
      "Epoch 16/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.8420 - accuracy: 0.7233 - val_loss: 2.3831 - val_accuracy: 0.4393\n",
      "Epoch 17/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.7419 - accuracy: 0.7607 - val_loss: 2.4700 - val_accuracy: 0.4299\n",
      "Epoch 18/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.6882 - accuracy: 0.7788 - val_loss: 2.4809 - val_accuracy: 0.4019\n",
      "Epoch 19/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.6102 - accuracy: 0.8001 - val_loss: 2.6649 - val_accuracy: 0.3832\n",
      "Epoch 20/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.6006 - accuracy: 0.7894 - val_loss: 2.5325 - val_accuracy: 0.4159\n",
      "Epoch 21/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.4936 - accuracy: 0.8406 - val_loss: 2.6374 - val_accuracy: 0.4252\n",
      "Epoch 22/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.4315 - accuracy: 0.8625 - val_loss: 2.8971 - val_accuracy: 0.3832\n",
      "Epoch 23/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.3825 - accuracy: 0.8785 - val_loss: 2.9358 - val_accuracy: 0.4159\n",
      "Epoch 24/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.3450 - accuracy: 0.8854 - val_loss: 2.9100 - val_accuracy: 0.3972\n",
      "Epoch 25/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.3824 - accuracy: 0.8662 - val_loss: 3.0404 - val_accuracy: 0.4065\n",
      "Epoch 26/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.3293 - accuracy: 0.8902 - val_loss: 2.8838 - val_accuracy: 0.4299\n",
      "Epoch 27/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.2496 - accuracy: 0.9334 - val_loss: 3.1626 - val_accuracy: 0.4112\n",
      "Epoch 28/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.2344 - accuracy: 0.9243 - val_loss: 3.0807 - val_accuracy: 0.4065\n",
      "Epoch 29/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.2187 - accuracy: 0.9259 - val_loss: 3.2106 - val_accuracy: 0.4206\n",
      "Epoch 30/50\n",
      "59/59 [==============================] - 73s 1s/step - loss: 0.2633 - accuracy: 0.9088 - val_loss: 3.1247 - val_accuracy: 0.4019\n",
      "Epoch 31/50\n",
      "59/59 [==============================] - 74s 1s/step - loss: 0.2184 - accuracy: 0.9307 - val_loss: 3.2864 - val_accuracy: 0.3925\n",
      "Epoch 32/50\n",
      "59/59 [==============================] - 86s 1s/step - loss: 0.1306 - accuracy: 0.9664 - val_loss: 3.2190 - val_accuracy: 0.4206\n",
      "Epoch 33/50\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.0923 - accuracy: 0.9765 - val_loss: 3.4107 - val_accuracy: 0.4346\n",
      "Epoch 34/50\n",
      "59/59 [==============================] - 77s 1s/step - loss: 0.0822 - accuracy: 0.9776 - val_loss: 3.2697 - val_accuracy: 0.4626\n",
      "Epoch 35/50\n",
      "59/59 [==============================] - 78s 1s/step - loss: 0.1045 - accuracy: 0.9712 - val_loss: 3.4077 - val_accuracy: 0.4393\n",
      "Epoch 36/50\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.1467 - accuracy: 0.9520 - val_loss: 3.5177 - val_accuracy: 0.4112\n",
      "Epoch 37/50\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.2258 - accuracy: 0.9291 - val_loss: 3.4396 - val_accuracy: 0.4019\n",
      "Epoch 38/50\n",
      "59/59 [==============================] - 80s 1s/step - loss: 0.2296 - accuracy: 0.9200 - val_loss: 3.5429 - val_accuracy: 0.4206\n",
      "Epoch 39/50\n",
      "59/59 [==============================] - 77s 1s/step - loss: 0.1258 - accuracy: 0.9627 - val_loss: 3.5298 - val_accuracy: 0.4393\n",
      "Epoch 40/50\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.0759 - accuracy: 0.9813 - val_loss: 3.4607 - val_accuracy: 0.4299\n",
      "Epoch 41/50\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.0457 - accuracy: 0.9893 - val_loss: 3.5703 - val_accuracy: 0.4393\n",
      "Epoch 42/50\n",
      "59/59 [==============================] - 77s 1s/step - loss: 0.0411 - accuracy: 0.9909 - val_loss: 3.6765 - val_accuracy: 0.4159\n",
      "Epoch 43/50\n",
      "59/59 [==============================] - 76s 1s/step - loss: 0.0441 - accuracy: 0.9915 - val_loss: 3.7290 - val_accuracy: 0.4159\n",
      "Epoch 44/50\n",
      "59/59 [==============================] - 79s 1s/step - loss: 0.0341 - accuracy: 0.9941 - val_loss: 3.8098 - val_accuracy: 0.4252\n",
      "Epoch 45/50\n",
      "59/59 [==============================] - 89s 2s/step - loss: 0.0377 - accuracy: 0.9888 - val_loss: 3.9272 - val_accuracy: 0.4299\n",
      "Epoch 46/50\n",
      "59/59 [==============================] - 85s 1s/step - loss: 0.0561 - accuracy: 0.9808 - val_loss: 3.9670 - val_accuracy: 0.4393\n",
      "Epoch 47/50\n",
      "59/59 [==============================] - 83s 1s/step - loss: 0.1017 - accuracy: 0.9675 - val_loss: 4.0243 - val_accuracy: 0.3972\n",
      "Epoch 48/50\n",
      "59/59 [==============================] - 81s 1s/step - loss: 0.1166 - accuracy: 0.9606 - val_loss: 3.9243 - val_accuracy: 0.3785\n",
      "Epoch 49/50\n",
      "59/59 [==============================] - 75s 1s/step - loss: 0.3064 - accuracy: 0.9025 - val_loss: 3.8354 - val_accuracy: 0.4019\n",
      "Epoch 50/50\n",
      "59/59 [==============================] - 81s 1s/step - loss: 0.2561 - accuracy: 0.9110 - val_loss: 3.6883 - val_accuracy: 0.4065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c24dfb9e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"Adam\", loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 50,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7de704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a501e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831105e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
