{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71b2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.7.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow_addons as tfa\n",
    "import glob, random, os, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a26b636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(x_train, y_train),( X_test, Y_test )= cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "x_train = x_train / 255\n",
    "X_test = X_test / 255 \n",
    "\n",
    "\n",
    "image_size = 32\n",
    "batch_size = 32\n",
    "n_classes = 102\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "num_epochs = 10\n",
    "patch_size = 4  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 2\n",
    "mlp_head_units = [56, 28] \n",
    "\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split( x_train, y_train, test_size=0.2, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a086665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class ImageResizeGenerator(Sequence):\n",
    "    def __init__(self, images, labels, batch_size, image_size, shuffle=True):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = (index + 1) * self.batch_size\n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "\n",
    "        batch_images = self.images[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "\n",
    "        resized_batch_images = np.array([tf.image.resize(img, (self.image_size, self.image_size)).numpy() for img in batch_images])\n",
    "\n",
    "        return resized_batch_images, batch_labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.images))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "            \n",
    "train_generator = ImageResizeGenerator(X_train,  Y_train, batch_size=16, image_size=224)\n",
    "test_generator = ImageResizeGenerator(X_val, Y_val, batch_size=16, image_size=224, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1329dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = L.Dense(units, activation = tf.nn.gelu)(x)\n",
    "        x = L.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images = images,\n",
    "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
    "            strides = [1, self.patch_size, self.patch_size, 1],\n",
    "            rates = [1, 1, 1, 1],\n",
    "            padding = 'VALID',\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45bd654e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 32 X 32\n",
      "Patch size: 4 X 4\n",
      "Patches per image: 64\n",
      "Elements per patch: 48\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFICAYAAAAyFGczAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWyklEQVR4nO3dSW8k93nH8aeqel/IJtmcnbNqsRRZtuw4NnIxkMVITn4BuTivIee8mQC5BAECJIccHCOAHSCQHCvarJE8mhGHFGe4s8lu9l5VOdi34MHzE0DAAfL9nB883ayu/rEO/6efpCzL0gAA/0v6+34DAPB/FQEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQAR0Ut/M9/+yepbjochzX7hwdSr+Oz07CmUdf+hM1+T6pb7XTDmv0Xh1KvX773vlQ3XyzCmlPhWpiZtdqtsOb+K69IvX7x3n9pr9mIP4Nr/TWp1554b7zcOwlr1rs9qVdnpR7WnJ8PpF5rvXWp7s69W2FNf7Mn9Wo3m1JdvpjFvVpar5XuSljTaNSkXt1u/J0zM7t580ZY8+LoUur1Fz/+a6mOJ0gAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcMiTNB//+nOpLhFO64+GA6nX5XgY1kzSROpVLeP3ZWZ2sh9Pcnz84WdSrw8fP5Pqkko8cZAWS6nXci9+/5WkKvV6/d5dqW5nbzusGVwMpF5jYRLLzGw+mYY1h5Njqddk0g5rEvFRYjrV7rNamoc1/ZWG1GshTGKZmS0Loa6Ip4rMzMrlPKxJcm3d1Wiovf/Hk1FYM1lq71/FEyQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8kHx49Nzqa5exAdIux3tp9j7G/FPrGdSJ7OleND62fZOWLN/oh1ArjW1v3M2j9/bcqm9/6KID84//uyJ1OvB1nWpzpbx4ejLS+0AdVloh4uV+YD5XDt0nufxgeyVbkfqVW9ozxyr7fjO3ezFB9jNzKbC/WNmNhgWYc35hbbaI1nGB/Urhfb+dw+113z3cfzdPBnG78vM7Cc/+RupjidIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHDIkzSTsTaVUK3Gp/U31jekXrduxZMcWabN0nzxVJseOTp8GdbkM+1a9BraaoORxZMQE/HvnCXxiMlcXAswHA2kuuZqN6wZjC+lXvlce29rK824V6n9/H6axc8JuTAtZGa2mMWTZGZm+XwS1ozHF1KvItXus5pwD1UTcZJJWN+Qz7WplvEovhZmZnu7+2HN46d7Ui8VT5AA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwyAfFVzrxz9KbmbWreVhTFHGNmVmex3WpGPHTyUiqu3OtF9bc62k/v18stcPdO8fxOosX50PtNcv4oH6lqn3saUN7/3kW3xvfeP2R1Ou1R/eluvW1tbBmttAObV+M40PPR8dHUq9nn38q1b08jA+BL5NdqVda0z7P/kovrGnVtEPns2l8CHwx165/mmgrI167Gw+YjEfa91zFEyQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOORJmo2+tibh2kr8M/fD8xOp15fb22FNmWo/Eb/91QuprlzG0zubXW2S5uRCWzNw5/aNsKbXjtcamJk93t4Jayq9FalXs16T6jr1+Da61l+Xen3nT34s1R0Jk0Xz84HUa70WTwL94YP7Uq9UXM3wq3f/I6z5/IO4xsysdjmQ6pL1eB3HeBHXmJk9P4hfs1nRnr82xPvxjVfvhzWzeCjqa+EJEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8iRNtaLtqkiFJTHqTpSXe8dhzbM9bVfI+WAs1VWFnS6TljYhMzftWH+Zn4U16y1teufN1x6ENYdj7f3Xq9okzdaNeEpmb6Rdi7/72XtS3dlFPElzOdV2otQbzbDm4Yt4b5CZ2dvi7p3+mz8Ia7K1m1Kv8iKenjIza6fx535yfCj1mpVxXUfcb3PtmjZlpTzNHfS1iTMVT5AA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwyAfFJ5OJVDetxodDazXtAHKr0w5rZvmB1Gs40Q4q15P4f8ZqO5N6lYW2DmJSxGsedobaQeVbG/Gh240yfj0zs2YnXp9hZpa14oPWi/arUq9xW1vtMZvFf8My037Kv96O77N98aD76OPPpLreSiusabR7Uq/+1i2prjrfDWsqZ6dSr9V2/D3vr8XX1cxspaXlQbUaf++u9XtSLxVPkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADg0FcuiFFaT+OVBU3x5HyRroU1Wxd9qdfleCrVHR3EPyXfrGsTGv3VValuWiRhzeVEWx/w+fOvwpq3Ht6VejVWtUmIYuNOWHPr/ptSr8Oldm/kFk/vVHLtpt1cj78G2UJbU1ErtempWrUR1iwq2sTW7libcpvk8WRUXmr39u1+/N283dd6tcS/c7mIs2UurtlQ8QQJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA55kqYmjtLUKvEkQV3s1bJ4quL2DW2SZirsMDEzm4ziiYn5Ujv5P7ycSXVpFverpPEOEDOzcR5PDB2I7+vWo7ekuu6DuG4m7PoxM8sm2sRKUsQ7YpqtjtSrKONJprSi7ecpM22SZprEUyGVUpwKWca9zMzOF/Fn0F3TpqzurMZ/51pd+8wrxVKqOzsbhDWLqTYxp+IJEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA595UJNK+12W2HNbK4d5hxdxIeGu+34p+vNzN54ZUuqG56ehzVPv3gh9UrTnlSXpfGh56TQDrqXwoHsea0n9SquvSbVjapxv/OzC6nXFx9+JNWdnB2FNa9867tSr6XdCGsG4/gzMjPL6lpdqxkfTq9PtV7VXBtcyFPhNTPtQPzmjfthTTPXDv2f7e9IdXkeH5xfW4nz5+vgCRIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHPIkTbOhnbDv9Xphzc7urtTrYP8krGl22lKvjY0Vqe6bf/AorHm+cyj1enkST+WYmfVX42mgWqb9rH5SiddUtPp3pF6N7oZUl1fi978stEmaMhFXFgzjfjuf/lrq9ejVeJ3F6GIi9cqFVQpmZuN6PP3SrGvfuVoqrkOxeLVBUWj37EiYYEtS7VrMl9rKhVYzvrcfbMVTUV8HT5AA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwyAfF81w7zDkZxwdq5+JPyY+G8WqG4XAm9Wo1tD/19q1rYc3b335T6vXP//oLqS5fxAeCN1bjQ7JmZuMirvvwo8dSr+0T7XD3+ta9sKa+tin16vbWpLrbi1fDmoNdbTXGh0f/HtZUKl2pV7sd3z9mZqMiPhCfrmrrAzobWl1rFh8CL6enUq8vyvj7VJ0dS71Wq/EqBTOzO9fie6Nev9pnPp4gAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcAhT9KcX4ylusNlPNmSz3Kp11o3nl4os0Tq1eloExq9blz3/XfekHo9ebIj1T3+zXZYczHRpg2yRjxVcfRcm6Sx34h19Xh6p7GqrW9odPpSXZLFqzYmk5HUazYZxkV5U+pVblxKdSasxijn2jUz01YbVPL4epydvZR6LfvxxFBloWXGItG+w5U0vmZZqU3WqXiCBAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABACHPEkzHmkn1M/z+FR/uxnvYDEzu3lzPayZzOIdOGZmSalNoizn8STE2oq2H+Yv//x7Ut3+i3h6Ye9AmwqpNOKphEqi7QRKUu3/ZzGLJ1Gy83i/kJlZOtF2oty8fiOs2Xo9rjEze34QT3b9+km8z8XMbHypfaUazXjiaXqmTZwVhXZvN4T7drHQploaZbxT5/W7t6Re06kwyWRm80V8D40uBlIvFU+QAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcMgHxVd62k/hr63Fh8C7rarUq1qP684G2gHe08GZVDdZxodWm634kK+Z2b0HW1LdH33vO2HNT3/231KvyTI+NFytiT/RLx6uv309Xo3xzhsPpF6tTPuf/Ug4hPz6t78p9fr5J8dhzRfPfiH1mlxqh55TW4Y1SaodFJ/HZ7bNzGxUjdeJtDLt3j47jg/0rz26K/XKheEGM7OTs/g7fHqm5YGKJ0gAcBCQAOAgIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcMiTNP0t7VT8+tpqWHP4Yk/qlQg//5517ki9jl5qqxmO907CmkUe15iZVevaaom0HV+zRlebPpqdx6sxskJ7Xw9eeSjV/eC73whrXr0dT3GYmeXDgVSnTPkMhMkLM7NaLb7+1awt9ZrNtUmaRVX46qXaJFNm2r19dhRPUFXWtHUiB+fx3/nl9nOp10pHux/n03iyKDXt/at4ggQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQAhzxJ8+z5S6luvsjCmu0dbRLl3XffD2sOTgdSr9F4JNWt9oSpipo21TKdxVMtZmYT4b2dDMVdG8J+kjzXPvZFNd41Y2b2yU48sVI24utqZlZLtLqbq/HERF7tSb2+Oo+v/3KuTWgkZVOqGy3iqZaaafdPJt5n80FcV4zi76+Z2Y3b8fTLz9/9ROq1mF1KdSsr8TRWZ0WbeFLxBAkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABACHfFB8cKod5jw+fBzWPH4c15iZ/ebznbCmSOKfYTczu3X7plTXaMQHgi/EQ9tJGR8GNjM7P40PWk9GU6lXnsfXQzhLbmZmv3rvl1LdYrEIa372U+2gdbOpHbT+sx/9MKz542+tS70+/Ww7LqpqwwHpQvs7F4txWFMuxM/8ciDVXQ6PwppxW/s7TzuvhTUXwgF8M7OnT59KdY1WfAi819Pun7+VqniCBAAXAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgIOABACHPEkzvtR+1n06jSc5To4HUq9CmETJqonWq4inPczMSuF/xoP7d6Ve7WZDqhuexSsohuL8y0L4+f0k1f4vdjorUl1dqLu8jCdHzMzOD+JpDzOzf/jHfwlrdj56JvWqFfGERt7oSL3Kakuqqwzi65FPtUmUi+NdqW4xH4Q145F2n53c3ghruokWL6NC+55MJ/E6iFNxqkjFEyQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA4CEgAc8kHx7S+/kuo+/OCzsGY2H0q9LsfxYdp6S/uJ+HpdO4z6/Hm85uH6DW3lwsN796S60uIDsGmq/Z2b13thTacdH4w2M1ssl1LdcBh/nsulelBfUxXWPMxP96Reb2w+DGteTrT1B+PVO1LdfBCv2ZhdXki9ljPtQHlqwjqOUhu8ePokXpOw1tLWT8xKra5SidcpNKpaLxVPkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgkCdp9vePpbq9vcOwprSJ1Csv4pP/hWk/hb+7sy/VjUbx9MJ0qk2Y7O68FF8znoRQroWZ2XAST/mcDbX1B7Wq9v8zL+LrMc+19z/PtWubFfWw5vmpNvH0di+eanlroU1ifaldWptn8STQRalOH2nzR0kST2Nlpk1sTYfxd/hIuK/NzNKGtqai244/81RcJ6LiCRIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHPIkTZ4XYmV8qr8UdmP87lXDiuFQm5Yw096/chL/5PhE6rUQ97BUKvFOmqLQpiXq9XjaYDadS73Gl1pdJlyz6UzrZYX2PztrxLtH1F0nH+y+CGt+uLYp9VocHEh1RTWeHjmqaPdsVtH+ziwR7iHx+lsa91qKC4bqyvsys1Yt/p7MF+J9JuIJEgAcBCQAOAhIAHAQkADgICABwEFAAoCDgAQABwEJAA75oLh60Los45/MVw+KF0IvS6RWslI4s5oU2otWK+LlFdqlwsFcM7NCWM2wmIuHaZWLYWbT5VTrJ0jED7TZjg9Hr66sSL22d5+FNX+6vib1elgXb8h0FpY8Me26lpn2nJMqt6N4ULwQ7o1lqfWqid+Tu1tbYc1soq15UPEECQAOAhIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOeZJGmmoR6+SVC2U8vVPKozTaVEgpvKal8U+/m5mlptVptL9zsYjXPJTihIxy/c3MkiR+b0qNmf7ehheDsOZ8eCb1apXKNdPWZ2zWtGeOy2U8zVSfDqVeC/E+K9L4M1AmZMzMsiT+O5VVHGovM7NapRrWXNvUJp5UPEECgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAQUACgEOepClLcY+MsBOlTMRewsSNOhSiTtIoO1HUlyy0QRRLs3gSQp1EkSZu1KmWQr64cS/xg5KHfObxvZFUtP//M2ESJRevWVf4LM3MOnncr11tSL0m4udUZnFdJt7dmbK7JhEnycT3/3JvL6ypX9f2EKl4ggQABwEJAA4CEgAcBCQAOAhIAHAQkADgICABwEFAAoBDPiiurywQDnebdoK6FE5aF/KxbfGguHIgWD0Bnqh1V1LyO/Fr6ksqxM9cek3tVcVf37e0FPqVX+P2DswSrVejWpPqOsLh6G6rK/U6mo+lukwa9tBWq6TCOoVSvP/Fb4n03dzs98VuGp4gAcBBQAKAg4AEAAcBCQAOAhIAHAQkADgISABwEJAA4CAgAcDxNVYuaFMVhXJavxRP2BfK+gNxkia5wkkg9SXF1RLJFa6WkDczCNSJJ2VKqVTnd9T7TBi5Ud9/RfiYLsRniVpV+ztXF3FNv6695jPx70yXcT91tUQhrW/Q7v+lOPFUbcRTSt//wbelXiqeIAHAQUACgIOABAAHAQkADgISABwEJAA4CEgAcBCQAOC48oPi2moD/Uf/wwrxfZXi4fREPFB+ta8pHIiX/86re//S+gnxNdUD7Or7T4WyMtVedJbFB5pfTs+lXpXb96S6zWr8bHKv1NYffHR4KNVdpvHXPRfvf22xhEYd9hhPJmFNWrnCSQnjCRIAXAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHPIkzZUSD7srZeq0RyL8RP9vaT8Tr0jFSY6rnH75ffRK0/ja6p+TWFcqdeIkUzULa56daZM0z8q4l5nZnfV+WPOgsSr1Wt/RJmkWubDaw7T3r6wwEW9/qyRaDE2ESZr33/9E6vVXUhVPkADgIiABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADguPKdNEUeTy8k2dVNe6iucqeLul8lF67FVSsKZY/P1e3K+W2/+DWzTJzQEMeslsJSGvXvzJbxe9u/1Cas/v6jJ1Ldnc2VsKbX7km9xpW6VLcoZmFNPdeufy7s8VEl8o6qWHGFk2RmPEECgIuABAAHAQkADgISABwEJAA4CEgAcBCQAOAgIAHAIR8Uv3Z9Xap79fWHcZF4LlQ58lkU4oFV9TWlw8XaYdTEllpdIvwU/hUedFfP0l7lOgv1oLh6ZLgU1jyU0loGs7SMe1Uq2lclzbRnjhdp/JkfTadSr7Wtu1LdahLf25Wlds+madxLXbmwFIYbzMzWVzthzZvvfFd7URFPkADgICABwEFAAoCDgAQABwEJAA4CEgAcBCQAOAhIAHAQkADgSEp1RAMA/p/hCRIAHAQkADgISABwEJAA4CAgAcBBQAKAg4AEAAcBCQAOAhIAHP8DijHyY+fFVZYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "image = X_train[10]\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size = (image_size, image_size)\n",
    ")\n",
    "\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f'Image size: {image_size} X {image_size}')\n",
    "print(f'Patch size: {patch_size} X {patch_size}')\n",
    "print(f'Patches per image: {patches.shape[1]}')\n",
    "print(f'Elements per patch: {patches.shape[-1]}')\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31cdcdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(L.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = L.Dense(units = projection_dim)\n",
    "        self.position_embedding = L.Embedding(\n",
    "            input_dim = num_patches, output_dim = projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b57253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bda0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vision_transformer():\n",
    "    inputs = L.Input(shape = (image_size, image_size, 3))\n",
    "    \n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    \n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(6):\n",
    "        \n",
    "        # Layer normalization 1.\n",
    "        x1 = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "        \n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads = 6, key_dim = projection_dim, dropout = 0.1\n",
    "        )(x1, x1)\n",
    "        \n",
    "        # Skip connection 1.\n",
    "        x2 = L.Add()([attention_output, encoded_patches])\n",
    "        \n",
    "        # Layer normalization 2.\n",
    "        x3 = L.LayerNormalization(epsilon = 1e-6)(x2)\n",
    "        \n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units = transformer_units, dropout_rate = 0.1)\n",
    "        \n",
    "        # Skip connection 2.\n",
    "        encoded_patches = L.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = L.LayerNormalization(epsilon = 1e-6)(encoded_patches)\n",
    "    representation = L.Flatten()(representation)\n",
    "    representation = L.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units = mlp_head_units, dropout_rate = 0.5)\n",
    "    \n",
    "    # Classify outputs.\n",
    "    logits = L.Dense(n_classes)(features)\n",
    "    \n",
    "    # Create the model.\n",
    "    model = tf.keras.Model(inputs = inputs, outputs = logits)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485367ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8b236c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " patches_2 (Patches)            (None, None, 48)     0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 64, 64)       7232        ['patches_2[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 64, 64)      128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 64, 64)      99520       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 64)       0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 64, 128)      8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 128)      0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64, 64)       8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 64)       0           ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 64)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 64)      99520       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64)       0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 128)      0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64, 64)       8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 64)       0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 64)      99520       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64)       0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 64, 128)      8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 128)      0           ['dense_17[0][0]']               \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 64, 64)       8256        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 64)       0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64)       0           ['dropout_5[0][0]',              \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 64)      99520       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 64)       0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 64, 128)      8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 128)      0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64, 64)       8256        ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 64)       0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 64)       0           ['dropout_7[0][0]',              \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 64, 64)      128         ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 64, 64)      99520       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 64, 64)       0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 64, 64)      128         ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 64, 128)      8320        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 64, 128)      0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 64, 64)       8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64, 64)       0           ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 64, 64)       0           ['dropout_9[0][0]',              \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 64, 64)      128         ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 64, 64)      99520       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 64, 64)       0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 64, 64)      128         ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 64, 128)      8320        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 64, 128)      0           ['dense_23[0][0]']               \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 64, 64)       8256        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 64, 64)       0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 64, 64)       0           ['dropout_11[0][0]',             \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 64, 64)      128         ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 4096)         0           ['layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 4096)         0           ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 56)           229432      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 56)           0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 28)           1596        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 28)           0           ['dense_26[0][0]']               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 102)          2958        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 939,458\n",
      "Trainable params: 939,458\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = vision_transformer()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08529ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "96/96 [==============================] - 23s 144ms/step - loss: 8.8671 - accuracy: 0.0105 - val_loss: 9.1002 - val_accuracy: 0.0082\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 12s 121ms/step - loss: 8.7251 - accuracy: 0.0105 - val_loss: 8.9226 - val_accuracy: 0.0081\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 10s 108ms/step - loss: 8.5906 - accuracy: 0.0118 - val_loss: 8.3525 - val_accuracy: 0.0077\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 10s 109ms/step - loss: 8.7071 - accuracy: 0.0105 - val_loss: 8.4431 - val_accuracy: 0.0074\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 10s 109ms/step - loss: 8.7100 - accuracy: 0.0088 - val_loss: 9.0234 - val_accuracy: 0.0067\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 10s 109ms/step - loss: 8.5080 - accuracy: 0.0098 - val_loss: 9.3872 - val_accuracy: 0.0074\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 10s 107ms/step - loss: 8.4700 - accuracy: 0.0105 - val_loss: 8.6389 - val_accuracy: 0.0062\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 10s 108ms/step - loss: 8.4366 - accuracy: 0.0072 - val_loss: 9.3280 - val_accuracy: 0.0071\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 11s 114ms/step - loss: 8.3543 - accuracy: 0.0095 - val_loss: 7.0524 - val_accuracy: 0.0059\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 32s 337ms/step - loss: 8.1007 - accuracy: 0.0085 - val_loss: 8.0493 - val_accuracy: 0.0107\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 37s 384ms/step - loss: 8.2730 - accuracy: 0.0072 - val_loss: 9.4006 - val_accuracy: 0.0066\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 32s 337ms/step - loss: 8.1590 - accuracy: 0.0092 - val_loss: 9.1180 - val_accuracy: 0.0071\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 32s 333ms/step - loss: 8.1045 - accuracy: 0.0111 - val_loss: 9.3071 - val_accuracy: 0.0066\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 29s 305ms/step - loss: 8.2481 - accuracy: 0.0101 - val_loss: 9.5163 - val_accuracy: 0.0049\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 28s 289ms/step - loss: 8.2019 - accuracy: 0.0092 - val_loss: 10.0865 - val_accuracy: 0.0044\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 28s 295ms/step - loss: 8.1544 - accuracy: 0.0039 - val_loss: 9.3308 - val_accuracy: 0.0051\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 31s 329ms/step - loss: 8.3373 - accuracy: 0.0105 - val_loss: 10.1755 - val_accuracy: 0.0058\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 30s 315ms/step - loss: 8.0845 - accuracy: 0.0072 - val_loss: 7.8258 - val_accuracy: 0.0064\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 30s 309ms/step - loss: 8.1723 - accuracy: 0.0098 - val_loss: 8.0497 - val_accuracy: 0.0066\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 30s 314ms/step - loss: 8.1612 - accuracy: 0.0088 - val_loss: 9.2800 - val_accuracy: 0.0059\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 29s 304ms/step - loss: 8.0302 - accuracy: 0.0150 - val_loss: 6.5331 - val_accuracy: 0.0058\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 32s 330ms/step - loss: 7.8915 - accuracy: 0.0108 - val_loss: 9.4174 - val_accuracy: 0.0056\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 29s 307ms/step - loss: 8.2874 - accuracy: 0.0114 - val_loss: 7.1074 - val_accuracy: 0.0056\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 28s 290ms/step - loss: 8.2129 - accuracy: 0.0121 - val_loss: 9.6700 - val_accuracy: 0.0058\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 27s 284ms/step - loss: 8.0313 - accuracy: 0.0075 - val_loss: 6.5685 - val_accuracy: 0.0048\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 27s 284ms/step - loss: 8.2118 - accuracy: 0.0101 - val_loss: 6.5500 - val_accuracy: 0.0048\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 27s 285ms/step - loss: 7.8858 - accuracy: 0.0092 - val_loss: 7.0839 - val_accuracy: 0.0061\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 28s 289ms/step - loss: 8.2410 - accuracy: 0.0111 - val_loss: 9.1291 - val_accuracy: 0.0059\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 28s 295ms/step - loss: 8.4425 - accuracy: 0.0098 - val_loss: 9.4482 - val_accuracy: 0.0061\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 38s 393ms/step - loss: 8.0589 - accuracy: 0.0101 - val_loss: 8.6532 - val_accuracy: 0.0059\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 38s 399ms/step - loss: 8.1023 - accuracy: 0.0098 - val_loss: 7.3905 - val_accuracy: 0.0059\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 31s 325ms/step - loss: 8.1278 - accuracy: 0.0111 - val_loss: 8.7776 - val_accuracy: 0.0069\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 35s 364ms/step - loss: 8.4059 - accuracy: 0.0082 - val_loss: 7.8652 - val_accuracy: 0.0069\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 39s 408ms/step - loss: 8.4172 - accuracy: 0.0118 - val_loss: 8.8325 - val_accuracy: 0.0074\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 38s 394ms/step - loss: 8.0420 - accuracy: 0.0124 - val_loss: 8.8532 - val_accuracy: 0.0066\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 34s 361ms/step - loss: 7.9953 - accuracy: 0.0075 - val_loss: 9.1548 - val_accuracy: 0.0067\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 40s 417ms/step - loss: 8.0429 - accuracy: 0.0101 - val_loss: 7.1641 - val_accuracy: 0.0059\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 40s 421ms/step - loss: 8.2104 - accuracy: 0.0105 - val_loss: 8.9303 - val_accuracy: 0.0059\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 33s 344ms/step - loss: 7.9226 - accuracy: 0.0082 - val_loss: 7.2445 - val_accuracy: 0.0059\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 31s 329ms/step - loss: 8.2978 - accuracy: 0.0118 - val_loss: 8.1856 - val_accuracy: 0.0054\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 32s 338ms/step - loss: 8.2472 - accuracy: 0.0085 - val_loss: 8.9771 - val_accuracy: 0.0051\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 31s 323ms/step - loss: 8.0627 - accuracy: 0.0095 - val_loss: 7.5760 - val_accuracy: 0.0061\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 30s 316ms/step - loss: 8.1283 - accuracy: 0.0131 - val_loss: 8.1842 - val_accuracy: 0.0058\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 31s 329ms/step - loss: 8.4144 - accuracy: 0.0088 - val_loss: 7.6196 - val_accuracy: 0.0058\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 35s 363ms/step - loss: 8.1237 - accuracy: 0.0108 - val_loss: 8.1020 - val_accuracy: 0.0061\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 33s 350ms/step - loss: 7.8132 - accuracy: 0.0085 - val_loss: 7.8263 - val_accuracy: 0.0062\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 29s 303ms/step - loss: 7.9052 - accuracy: 0.0078 - val_loss: 8.4044 - val_accuracy: 0.0054\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 29s 302ms/step - loss: 7.9908 - accuracy: 0.0137 - val_loss: 8.7260 - val_accuracy: 0.0043\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 33s 350ms/step - loss: 7.9979 - accuracy: 0.0118 - val_loss: 7.9960 - val_accuracy: 0.0043\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 29s 306ms/step - loss: 8.2107 - accuracy: 0.0105 - val_loss: 8.5705 - val_accuracy: 0.0048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fe3ec05d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer = Adam, loss = \"categorical_crossentropy\", metrics = ['accuracy'])\n",
    "model.fit(ds_train, epochs=50, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23a9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b0cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7dadbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d70a6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 32, 32, 3), (None, 102)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96947bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ebce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7408940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c25bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab591c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8cb53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a54c686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8f246d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
