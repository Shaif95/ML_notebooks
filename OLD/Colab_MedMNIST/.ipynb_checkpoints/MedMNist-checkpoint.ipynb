{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YuAIDLLtksw6",
    "outputId": "6bbb0b97-66f0-4cdd-8d8b-b6f3202b67bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qq medmnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "ZYZQ1rOunH81",
    "outputId": "5351d0e3-3010-42ef-e3ab-4b24316c8d5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\chsha\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Collecting keras\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.8.0\n",
      "    Uninstalling keras-2.8.0:\n",
      "      Successfully uninstalled keras-2.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-cpu 2.8.0 requires keras<2.9,>=2.8.0rc0, but you have keras 2.10.0 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\chsha\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed keras-2.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-c6sYy6zk-5j"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (medmnist.py, line 10)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3444\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\chsha\\AppData\\Local\\Temp/ipykernel_12408/1892663077.py\"\u001b[1;36m, line \u001b[1;32m4\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import medmnist\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\time-distributed\\Source_Code\\Colab_MedMNIST\\medmnist.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    !pip install -qq medmnist\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import imageio\n",
    "import medmnist\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FIQKenIk-6o"
   },
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (28, 28, 28, 1)\n",
    "NUM_CLASSES = 11\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtF54ixMk--j",
    "outputId": "0eb9533a-7686-48df-c7a1-69f86e149f45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://zenodo.org/record/6496656/files/organmnist3d.npz?download=1\n",
      "32661504/32657407 [==============================] - 2s 0us/step\n",
      "32669696/32657407 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "def download_and_prepare_dataset(data_info: dict):\n",
    "\n",
    "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
    "\n",
    "    with np.load(data_path) as data:\n",
    "        # Get videos\n",
    "        train_videos = data[\"train_images\"]\n",
    "        valid_videos = data[\"val_images\"]\n",
    "        test_videos = data[\"test_images\"]\n",
    "\n",
    "        # Get labels\n",
    "        train_labels = data[\"train_labels\"].flatten()\n",
    "        valid_labels = data[\"val_labels\"].flatten()\n",
    "        test_labels = data[\"test_labels\"].flatten()\n",
    "\n",
    "    return (\n",
    "        (train_videos, train_labels),\n",
    "        (valid_videos, valid_labels),\n",
    "        (test_videos, test_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "# Get the metadata of the dataset\n",
    "info = medmnist.INFO[DATASET_NAME]\n",
    "\n",
    "# Get the dataset\n",
    "prepared_dataset = download_and_prepare_dataset(info)\n",
    "(train_videos, train_labels) = prepared_dataset[0]\n",
    "(valid_videos, valid_labels) = prepared_dataset[1]\n",
    "(test_videos, test_labels) = prepared_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34o25m_bk-_h"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
    "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
    "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQMwQq9Mk_Dn"
   },
   "outputs": [],
   "source": [
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju3sa39Ek_Eh"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJCPHMC9k_Iu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvFDTZEmk_Jq",
    "outputId": "87cede06-2731-4b8f-a620-d06bc1086f79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 28, 28, 28,  0           []                               \n",
      "                                 1)]                                                              \n",
      "                                                                                                  \n",
      " tubelet_embedding_4 (TubeletEm  (None, 27, 128)     65664       ['input_5[0][0]']                \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " positional_encoder_4 (Position  (None, 27, 128)     3456        ['tubelet_embedding_4[0][0]']    \n",
      " alEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_68 (LayerN  (None, 27, 128)     256         ['positional_encoder_4[0][0]']   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_32 (Multi  (None, 27, 128)     66048       ['layer_normalization_68[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " add_64 (Add)                   (None, 27, 128)      0           ['multi_head_attention_32[0][0]',\n",
      "                                                                  'positional_encoder_4[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_69 (LayerN  (None, 27, 128)     256         ['add_64[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_32 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " add_65 (Add)                   (None, 27, 128)      0           ['sequential_32[0][0]',          \n",
      "                                                                  'add_64[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_70 (LayerN  (None, 27, 128)     256         ['add_65[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_33 (Multi  (None, 27, 128)     66048       ['layer_normalization_70[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " add_66 (Add)                   (None, 27, 128)      0           ['multi_head_attention_33[0][0]',\n",
      "                                                                  'add_65[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_71 (LayerN  (None, 27, 128)     256         ['add_66[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_33 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " add_67 (Add)                   (None, 27, 128)      0           ['sequential_33[0][0]',          \n",
      "                                                                  'add_66[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_72 (LayerN  (None, 27, 128)     256         ['add_67[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_34 (Multi  (None, 27, 128)     66048       ['layer_normalization_72[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " add_68 (Add)                   (None, 27, 128)      0           ['multi_head_attention_34[0][0]',\n",
      "                                                                  'add_67[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_73 (LayerN  (None, 27, 128)     256         ['add_68[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_34 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_73[0][0]'] \n",
      "                                                                                                  \n",
      " add_69 (Add)                   (None, 27, 128)      0           ['sequential_34[0][0]',          \n",
      "                                                                  'add_68[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_74 (LayerN  (None, 27, 128)     256         ['add_69[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_35 (Multi  (None, 27, 128)     66048       ['layer_normalization_74[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " add_70 (Add)                   (None, 27, 128)      0           ['multi_head_attention_35[0][0]',\n",
      "                                                                  'add_69[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_75 (LayerN  (None, 27, 128)     256         ['add_70[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_35 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " add_71 (Add)                   (None, 27, 128)      0           ['sequential_35[0][0]',          \n",
      "                                                                  'add_70[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_76 (LayerN  (None, 27, 128)     256         ['add_71[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_36 (Multi  (None, 27, 128)     66048       ['layer_normalization_76[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " add_72 (Add)                   (None, 27, 128)      0           ['multi_head_attention_36[0][0]',\n",
      "                                                                  'add_71[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_77 (LayerN  (None, 27, 128)     256         ['add_72[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_36 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " add_73 (Add)                   (None, 27, 128)      0           ['sequential_36[0][0]',          \n",
      "                                                                  'add_72[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_78 (LayerN  (None, 27, 128)     256         ['add_73[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (Multi  (None, 27, 128)     66048       ['layer_normalization_78[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " add_74 (Add)                   (None, 27, 128)      0           ['multi_head_attention_37[0][0]',\n",
      "                                                                  'add_73[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_79 (LayerN  (None, 27, 128)     256         ['add_74[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_37 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " add_75 (Add)                   (None, 27, 128)      0           ['sequential_37[0][0]',          \n",
      "                                                                  'add_74[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_80 (LayerN  (None, 27, 128)     256         ['add_75[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (Multi  (None, 27, 128)     66048       ['layer_normalization_80[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " add_76 (Add)                   (None, 27, 128)      0           ['multi_head_attention_38[0][0]',\n",
      "                                                                  'add_75[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_81 (LayerN  (None, 27, 128)     256         ['add_76[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_38 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " add_77 (Add)                   (None, 27, 128)      0           ['sequential_38[0][0]',          \n",
      "                                                                  'add_76[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_82 (LayerN  (None, 27, 128)     256         ['add_77[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (Multi  (None, 27, 128)     66048       ['layer_normalization_82[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " add_78 (Add)                   (None, 27, 128)      0           ['multi_head_attention_39[0][0]',\n",
      "                                                                  'add_77[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_83 (LayerN  (None, 27, 128)     256         ['add_78[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_39 (Sequential)     (None, 27, 128)      131712      ['layer_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " add_79 (Add)                   (None, 27, 128)      0           ['sequential_39[0][0]',          \n",
      "                                                                  'add_78[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_84 (LayerN  (None, 27, 128)     256         ['add_79[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_4 (Gl  (None, 128)         0           ['layer_normalization_84[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_84 (Dense)               (None, 11)           1419        ['global_average_pooling1d_4[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,656,971\n",
      "Trainable params: 1,656,971\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (8, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QWsQOqok_N7",
    "outputId": "27f072d5-ed9b-4c47-8994-8b3909e95fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd6406e3320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd6406e3320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "31/31 [==============================] - ETA: 0s - loss: 2.4546 - accuracy: 0.1420 - top-5-accuracy: 0.5926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd51205ee60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd51205ee60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "31/31 [==============================] - 10s 78ms/step - loss: 2.4546 - accuracy: 0.1420 - top-5-accuracy: 0.5926 - val_loss: 2.2382 - val_accuracy: 0.1366 - val_top-5-accuracy: 0.6584\n",
      "Epoch 2/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.1215 - accuracy: 0.2047 - top-5-accuracy: 0.7438 - val_loss: 1.8659 - val_accuracy: 0.2671 - val_top-5-accuracy: 0.7888\n",
      "Epoch 3/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.0138 - accuracy: 0.2233 - top-5-accuracy: 0.7809 - val_loss: 1.6916 - val_accuracy: 0.2733 - val_top-5-accuracy: 0.8696\n",
      "Epoch 4/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.8674 - accuracy: 0.3025 - top-5-accuracy: 0.8261 - val_loss: 1.5342 - val_accuracy: 0.3975 - val_top-5-accuracy: 0.8944\n",
      "Epoch 5/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.6675 - accuracy: 0.3508 - top-5-accuracy: 0.8765 - val_loss: 1.4410 - val_accuracy: 0.3851 - val_top-5-accuracy: 0.9006\n",
      "Epoch 6/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.4212 - accuracy: 0.4588 - top-5-accuracy: 0.9239 - val_loss: 1.1534 - val_accuracy: 0.5031 - val_top-5-accuracy: 0.9627\n",
      "Epoch 7/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.3643 - accuracy: 0.4753 - top-5-accuracy: 0.9311 - val_loss: 1.0140 - val_accuracy: 0.5901 - val_top-5-accuracy: 0.9814\n",
      "Epoch 8/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.3273 - accuracy: 0.4877 - top-5-accuracy: 0.9414 - val_loss: 1.0926 - val_accuracy: 0.5652 - val_top-5-accuracy: 0.9814\n",
      "Epoch 9/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.1607 - accuracy: 0.5617 - top-5-accuracy: 0.9609 - val_loss: 0.8301 - val_accuracy: 0.6460 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.1257 - accuracy: 0.5802 - top-5-accuracy: 0.9516 - val_loss: 1.0251 - val_accuracy: 0.5776 - val_top-5-accuracy: 0.9814\n",
      "Epoch 11/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.1126 - accuracy: 0.5874 - top-5-accuracy: 0.9558 - val_loss: 0.7021 - val_accuracy: 0.7516 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.9425 - accuracy: 0.6409 - top-5-accuracy: 0.9630 - val_loss: 0.5818 - val_accuracy: 0.7702 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.8645 - accuracy: 0.6770 - top-5-accuracy: 0.9805 - val_loss: 0.6286 - val_accuracy: 0.7391 - val_top-5-accuracy: 0.9938\n",
      "Epoch 14/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.8754 - accuracy: 0.6615 - top-5-accuracy: 0.9794 - val_loss: 0.6550 - val_accuracy: 0.7578 - val_top-5-accuracy: 0.9876\n",
      "Epoch 15/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.7530 - accuracy: 0.7263 - top-5-accuracy: 0.9887 - val_loss: 0.4875 - val_accuracy: 0.8199 - val_top-5-accuracy: 0.9938\n",
      "Epoch 16/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.7159 - accuracy: 0.7274 - top-5-accuracy: 0.9938 - val_loss: 0.5912 - val_accuracy: 0.7950 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.6490 - accuracy: 0.7623 - top-5-accuracy: 0.9887 - val_loss: 0.4783 - val_accuracy: 0.8137 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.5142 - accuracy: 0.8251 - top-5-accuracy: 0.9928 - val_loss: 0.4313 - val_accuracy: 0.8509 - val_top-5-accuracy: 0.9938\n",
      "Epoch 19/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.4856 - accuracy: 0.8354 - top-5-accuracy: 0.9969 - val_loss: 0.4116 - val_accuracy: 0.8385 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.4562 - accuracy: 0.8447 - top-5-accuracy: 0.9949 - val_loss: 0.3039 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 21/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.3953 - accuracy: 0.8632 - top-5-accuracy: 0.9969 - val_loss: 0.3214 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.3855 - accuracy: 0.8549 - top-5-accuracy: 0.9969 - val_loss: 0.2959 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.3757 - accuracy: 0.8591 - top-5-accuracy: 1.0000 - val_loss: 0.4081 - val_accuracy: 0.8758 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.3316 - accuracy: 0.8837 - top-5-accuracy: 1.0000 - val_loss: 0.3108 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.3318 - accuracy: 0.8827 - top-5-accuracy: 0.9979 - val_loss: 0.3268 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.2758 - accuracy: 0.9053 - top-5-accuracy: 0.9990 - val_loss: 0.3585 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1968 - accuracy: 0.9383 - top-5-accuracy: 0.9990 - val_loss: 0.2352 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1348 - accuracy: 0.9588 - top-5-accuracy: 0.9990 - val_loss: 0.2701 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.1495 - accuracy: 0.9537 - top-5-accuracy: 0.9990 - val_loss: 0.5071 - val_accuracy: 0.8509 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1310 - accuracy: 0.9650 - top-5-accuracy: 1.0000 - val_loss: 0.3634 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1267 - accuracy: 0.9609 - top-5-accuracy: 0.9990 - val_loss: 0.4138 - val_accuracy: 0.8696 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1143 - accuracy: 0.9630 - top-5-accuracy: 1.0000 - val_loss: 0.3638 - val_accuracy: 0.8882 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.1390 - accuracy: 0.9568 - top-5-accuracy: 1.0000 - val_loss: 0.5164 - val_accuracy: 0.8261 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1146 - accuracy: 0.9640 - top-5-accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.8696 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.1191 - accuracy: 0.9619 - top-5-accuracy: 1.0000 - val_loss: 0.2491 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0702 - accuracy: 0.9805 - top-5-accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0504 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 38/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0659 - accuracy: 0.9753 - top-5-accuracy: 1.0000 - val_loss: 0.5310 - val_accuracy: 0.8696 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1139 - accuracy: 0.9640 - top-5-accuracy: 0.9990 - val_loss: 0.4268 - val_accuracy: 0.8820 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0582 - accuracy: 0.9805 - top-5-accuracy: 1.0000 - val_loss: 0.2957 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 41/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0418 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0196 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.3126 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0347 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.3633 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 44/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0470 - accuracy: 0.9846 - top-5-accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.8882 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0198 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.2979 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0162 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.2769 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0237 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.9006 - val_top-5-accuracy: 0.9938\n",
      "Epoch 48/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0294 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.3345 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0884 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.5067 - val_accuracy: 0.8758 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0734 - accuracy: 0.9763 - top-5-accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1401 - accuracy: 0.9486 - top-5-accuracy: 1.0000 - val_loss: 0.4910 - val_accuracy: 0.8820 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0805 - accuracy: 0.9671 - top-5-accuracy: 1.0000 - val_loss: 0.3538 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 53/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0357 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.3056 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0134 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.2861 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0125 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.3019 - val_accuracy: 0.9006 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0089 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0097 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 0.8944 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0085 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.3416 - val_accuracy: 0.8882 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0032 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3256 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
      "Epoch 60/200\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0017 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3215 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 61/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0015 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3141 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 62/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0013 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 63/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0013 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 64/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0012 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 65/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0012 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 66/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0012 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 67/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0011 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3309 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 68/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0010 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3274 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 69/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 9.9863e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3252 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 70/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0010 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3324 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 71/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 9.3905e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 72/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 9.1011e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3339 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 73/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0010 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 74/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 8.3318e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3207 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 75/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 7.9398e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3254 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 76/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 7.4417e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 77/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 7.6897e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 78/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 7.0380e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3299 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 79/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 7.3007e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.8190e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3344 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.8358e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3299 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.6265e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3287 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.4370e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3303 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 6.1645e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3313 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.1652e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 5.6650e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3364 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.1211e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3366 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 5.5605e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3385 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 5.3677e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 5.4119e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3331 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 5.3337e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 5.1331e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3401 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 4.9553e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3457 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 5.2035e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3404 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 4.5341e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3421 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.6016e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 4.4458e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3465 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 4.3875e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3429 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.4298e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.2586e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3392 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.3573e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3294 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 3.9317e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.0358e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3366 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.9980e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3444 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.8901e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3553 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.7298e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3471 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.6725e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3512 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.5255e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 3.4939e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3573 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.4498e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3539 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.4782e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.3257e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.1455e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3525 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.1463e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3556 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 3.1260e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3580 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.0583e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 3.0294e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 2.8911e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3587 - val_accuracy: 0.9255 - val_top-5-accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 2.8762e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3575 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.8099e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.9521e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.8171e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3624 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.6783e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 2.6465e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 2.4972e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3635 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.4393e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.3695e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3633 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.3911e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.3764e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3596 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.3919e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 2.3027e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3708 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 2.1890e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3666 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 2.1738e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 2.1385e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 2.1541e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3651 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 2.0557e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3702 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 2.0592e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3696 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.9476e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3664 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.9460e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3669 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.9443e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3680 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.8394e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3722 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.8750e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 1.8522e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3706 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.7802e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.7332e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3761 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.7326e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3776 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.6907e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3835 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.7410e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.6463e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 1.6367e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.6360e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3793 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.5807e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.5446e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3795 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.6109e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.5126e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3798 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 2.1836e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3846 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 1.5003e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3872 - val_accuracy: 0.9193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.4426e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3824 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.4140e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.3700e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3878 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.4788e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 1.3692e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3915 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.2854e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3943 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.3343e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3957 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.3051e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3959 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.2563e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3997 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.2583e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.2252e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3939 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.1728e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3954 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 1.1563e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.1771e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3967 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.1587e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 1.1100e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.0722e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4044 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.0557e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4022 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 1.0744e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 1.0843e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.3989 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 1.0516e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.0261e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.0257e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4049 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 9.8426e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4037 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.0007e-04 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 9.5664e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 9.6127e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 9.5477e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 9.5034e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4011 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 9.0332e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 8.9932e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4076 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 8.5872e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4123 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 8.5253e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4088 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 8.7012e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 8.1379e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4109 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 8.2872e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 8.0078e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4182 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 7.9606e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4201 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 7.8399e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4230 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 7.8168e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4110 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 7.5960e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4139 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 7.8570e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 7.5914e-05 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9130 - val_top-5-accuracy: 1.0000\n",
      "20/20 [==============================] - 0s 21ms/step - loss: 1.3552 - accuracy: 0.7852 - top-5-accuracy: 0.9689\n",
      "Test accuracy: 78.52%\n",
      "Test top 5 accuracy: 96.89%\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "EPOCHS = 200\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An_dliKKk_O3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBmXgyMYpjbq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVzpK7b2pjc_"
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdbtmSM-k_TA"
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "def create_attn_lstm_classifier(num_classes=NUM_CLASSES):\n",
    "    \n",
    "\n",
    "  model= models.Sequential()\n",
    "\n",
    "  model.add(TimeDistributed(Conv2D(16, (3, 3), strides=(1,1),activation='relu'),input_shape=(28, 28, 28, 1)))\n",
    "  model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "  model.add(TimeDistributed(Conv2D(8, (3, 3), strides=(1,1),activation='relu')))\n",
    "  model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "\n",
    "\n",
    "  model.add(TimeDistributed(Flatten()))\n",
    "  model.add(PatchEncoder(28, 100 )) \n",
    "\n",
    "  model.add(LSTM(100,return_sequences=False,dropout=0.2)) # used 32 units \n",
    "\n",
    "  model.add(Dense(64,activation='relu'))\n",
    "  model.add(Dense(units = num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S2-fcOSzk_T9",
    "outputId": "33993a2e-909a-4645-9bd2-498c5323f95a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_20 (TimeDi  (None, 28, 26, 26, 16)   160       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_21 (TimeDi  (None, 28, 13, 13, 16)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_22 (TimeDi  (None, 28, 11, 11, 8)    1160      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_23 (TimeDi  (None, 28, 5, 5, 8)      0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_24 (TimeDi  (None, 28, 200)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " patch_encoder_4 (PatchEncod  (None, 28, 100)          22900     \n",
      " er)                                                             \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 64)                6464      \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 11)                715       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111,799\n",
      "Trainable params: 111,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "att = create_attn_lstm_classifier(NUM_CLASSES)\n",
    "\n",
    "att.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7M4kAkkk_YP",
    "outputId": "663f7462-16be-4395-db8b-9a30c93dd402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd5a8b52a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fd5a8b52a70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "29/31 [===========================>..] - ETA: 0s - loss: 2.3830 - accuracy: 0.1110 - top-5-accuracy: 0.5356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd512936440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7fd512936440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "31/31 [==============================] - 4s 52ms/step - loss: 2.3827 - accuracy: 0.1121 - top-5-accuracy: 0.5350 - val_loss: 2.3934 - val_accuracy: 0.0994 - val_top-5-accuracy: 0.4720\n",
      "Epoch 2/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 2.3490 - accuracy: 0.1183 - top-5-accuracy: 0.5710 - val_loss: 2.3879 - val_accuracy: 0.0994 - val_top-5-accuracy: 0.4348\n",
      "Epoch 3/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 2.3239 - accuracy: 0.1183 - top-5-accuracy: 0.6019 - val_loss: 2.3718 - val_accuracy: 0.0994 - val_top-5-accuracy: 0.5652\n",
      "Epoch 4/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 2.2899 - accuracy: 0.1255 - top-5-accuracy: 0.6204 - val_loss: 2.3233 - val_accuracy: 0.1056 - val_top-5-accuracy: 0.5652\n",
      "Epoch 5/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 2.2138 - accuracy: 0.1739 - top-5-accuracy: 0.7037 - val_loss: 2.1627 - val_accuracy: 0.2298 - val_top-5-accuracy: 0.7453\n",
      "Epoch 6/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 2.0759 - accuracy: 0.2479 - top-5-accuracy: 0.7767 - val_loss: 1.9737 - val_accuracy: 0.3292 - val_top-5-accuracy: 0.7453\n",
      "Epoch 7/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 1.9577 - accuracy: 0.2953 - top-5-accuracy: 0.8138 - val_loss: 1.8245 - val_accuracy: 0.3354 - val_top-5-accuracy: 0.8385\n",
      "Epoch 8/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 1.8408 - accuracy: 0.3158 - top-5-accuracy: 0.8796 - val_loss: 1.6480 - val_accuracy: 0.3851 - val_top-5-accuracy: 0.9006\n",
      "Epoch 9/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.7226 - accuracy: 0.3714 - top-5-accuracy: 0.8992 - val_loss: 1.5186 - val_accuracy: 0.4969 - val_top-5-accuracy: 0.8944\n",
      "Epoch 10/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.6362 - accuracy: 0.4198 - top-5-accuracy: 0.9074 - val_loss: 1.4100 - val_accuracy: 0.5342 - val_top-5-accuracy: 0.9193\n",
      "Epoch 11/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 1.5435 - accuracy: 0.4578 - top-5-accuracy: 0.9167 - val_loss: 1.2864 - val_accuracy: 0.6149 - val_top-5-accuracy: 0.9627\n",
      "Epoch 12/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.4596 - accuracy: 0.5031 - top-5-accuracy: 0.9239 - val_loss: 1.1712 - val_accuracy: 0.6211 - val_top-5-accuracy: 0.9689\n",
      "Epoch 13/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.3823 - accuracy: 0.5226 - top-5-accuracy: 0.9362 - val_loss: 1.0864 - val_accuracy: 0.7516 - val_top-5-accuracy: 0.9876\n",
      "Epoch 14/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 1.3345 - accuracy: 0.5267 - top-5-accuracy: 0.9455 - val_loss: 1.0086 - val_accuracy: 0.7516 - val_top-5-accuracy: 0.9938\n",
      "Epoch 15/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.2457 - accuracy: 0.5617 - top-5-accuracy: 0.9630 - val_loss: 0.9579 - val_accuracy: 0.7516 - val_top-5-accuracy: 0.9938\n",
      "Epoch 16/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.1944 - accuracy: 0.5957 - top-5-accuracy: 0.9578 - val_loss: 0.8719 - val_accuracy: 0.7826 - val_top-5-accuracy: 0.9938\n",
      "Epoch 17/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 1.1323 - accuracy: 0.6163 - top-5-accuracy: 0.9650 - val_loss: 0.8620 - val_accuracy: 0.7453 - val_top-5-accuracy: 0.9938\n",
      "Epoch 18/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.0728 - accuracy: 0.6276 - top-5-accuracy: 0.9763 - val_loss: 0.7844 - val_accuracy: 0.7950 - val_top-5-accuracy: 0.9876\n",
      "Epoch 19/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 1.0338 - accuracy: 0.6564 - top-5-accuracy: 0.9733 - val_loss: 0.7239 - val_accuracy: 0.8012 - val_top-5-accuracy: 0.9938\n",
      "Epoch 20/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 1.0087 - accuracy: 0.6656 - top-5-accuracy: 0.9743 - val_loss: 0.6785 - val_accuracy: 0.8385 - val_top-5-accuracy: 0.9938\n",
      "Epoch 21/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.9502 - accuracy: 0.6698 - top-5-accuracy: 0.9784 - val_loss: 0.6575 - val_accuracy: 0.8571 - val_top-5-accuracy: 0.9938\n",
      "Epoch 22/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.9216 - accuracy: 0.6965 - top-5-accuracy: 0.9743 - val_loss: 0.6251 - val_accuracy: 0.8509 - val_top-5-accuracy: 0.9938\n",
      "Epoch 23/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.9077 - accuracy: 0.7099 - top-5-accuracy: 0.9784 - val_loss: 0.5882 - val_accuracy: 0.8758 - val_top-5-accuracy: 0.9938\n",
      "Epoch 24/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.8535 - accuracy: 0.7088 - top-5-accuracy: 0.9774 - val_loss: 0.5232 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 25/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.8349 - accuracy: 0.7068 - top-5-accuracy: 0.9825 - val_loss: 0.5608 - val_accuracy: 0.8634 - val_top-5-accuracy: 0.9938\n",
      "Epoch 26/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.7787 - accuracy: 0.7510 - top-5-accuracy: 0.9805 - val_loss: 0.5076 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 27/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.7661 - accuracy: 0.7315 - top-5-accuracy: 0.9794 - val_loss: 0.4448 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 28/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.7714 - accuracy: 0.7335 - top-5-accuracy: 0.9784 - val_loss: 0.4485 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 29/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.7482 - accuracy: 0.7490 - top-5-accuracy: 0.9805 - val_loss: 0.4497 - val_accuracy: 0.9068 - val_top-5-accuracy: 0.9938\n",
      "Epoch 30/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.7284 - accuracy: 0.7593 - top-5-accuracy: 0.9877 - val_loss: 0.4074 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
      "Epoch 31/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.6857 - accuracy: 0.7706 - top-5-accuracy: 0.9887 - val_loss: 0.3917 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 32/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.6739 - accuracy: 0.7665 - top-5-accuracy: 0.9887 - val_loss: 0.3957 - val_accuracy: 0.8944 - val_top-5-accuracy: 0.9938\n",
      "Epoch 33/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.6345 - accuracy: 0.7891 - top-5-accuracy: 0.9877 - val_loss: 0.3777 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 34/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.6273 - accuracy: 0.7922 - top-5-accuracy: 0.9877 - val_loss: 0.3363 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 35/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.6372 - accuracy: 0.7901 - top-5-accuracy: 0.9877 - val_loss: 0.3706 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 36/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.5885 - accuracy: 0.8107 - top-5-accuracy: 0.9907 - val_loss: 0.3424 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 37/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.5743 - accuracy: 0.8035 - top-5-accuracy: 0.9918 - val_loss: 0.3254 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 38/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.5872 - accuracy: 0.7922 - top-5-accuracy: 0.9877 - val_loss: 0.3186 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 39/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.5448 - accuracy: 0.8292 - top-5-accuracy: 0.9907 - val_loss: 0.2962 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 40/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.5361 - accuracy: 0.8374 - top-5-accuracy: 0.9918 - val_loss: 0.3060 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 41/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.5216 - accuracy: 0.8385 - top-5-accuracy: 0.9897 - val_loss: 0.2833 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 42/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.5291 - accuracy: 0.8272 - top-5-accuracy: 0.9877 - val_loss: 0.2726 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 43/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.5086 - accuracy: 0.8313 - top-5-accuracy: 0.9918 - val_loss: 0.2665 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 44/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4987 - accuracy: 0.8374 - top-5-accuracy: 0.9938 - val_loss: 0.2645 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 45/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4786 - accuracy: 0.8467 - top-5-accuracy: 0.9918 - val_loss: 0.2465 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 46/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4621 - accuracy: 0.8580 - top-5-accuracy: 0.9949 - val_loss: 0.2631 - val_accuracy: 0.9193 - val_top-5-accuracy: 0.9938\n",
      "Epoch 47/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4794 - accuracy: 0.8467 - top-5-accuracy: 0.9928 - val_loss: 0.2680 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 48/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4496 - accuracy: 0.8457 - top-5-accuracy: 0.9928 - val_loss: 0.2521 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 49/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.4353 - accuracy: 0.8560 - top-5-accuracy: 0.9928 - val_loss: 0.2631 - val_accuracy: 0.9130 - val_top-5-accuracy: 0.9938\n",
      "Epoch 50/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.4129 - accuracy: 0.8735 - top-5-accuracy: 0.9959 - val_loss: 0.2348 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 51/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.4283 - accuracy: 0.8570 - top-5-accuracy: 0.9928 - val_loss: 0.2160 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 52/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.3887 - accuracy: 0.8807 - top-5-accuracy: 0.9949 - val_loss: 0.2183 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 53/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.3853 - accuracy: 0.8827 - top-5-accuracy: 0.9949 - val_loss: 0.2202 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 54/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.3566 - accuracy: 0.8858 - top-5-accuracy: 0.9969 - val_loss: 0.2068 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 55/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.3785 - accuracy: 0.8827 - top-5-accuracy: 0.9938 - val_loss: 0.2342 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 56/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.3704 - accuracy: 0.8889 - top-5-accuracy: 0.9938 - val_loss: 0.2213 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 57/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.3553 - accuracy: 0.8868 - top-5-accuracy: 0.9959 - val_loss: 0.2248 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 58/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.3414 - accuracy: 0.8920 - top-5-accuracy: 0.9979 - val_loss: 0.2126 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 59/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.3182 - accuracy: 0.8971 - top-5-accuracy: 0.9979 - val_loss: 0.2063 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 60/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.3228 - accuracy: 0.8930 - top-5-accuracy: 0.9959 - val_loss: 0.2499 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 61/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.3033 - accuracy: 0.9064 - top-5-accuracy: 0.9979 - val_loss: 0.1929 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 62/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.3109 - accuracy: 0.8992 - top-5-accuracy: 0.9979 - val_loss: 0.1839 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 63/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2855 - accuracy: 0.9177 - top-5-accuracy: 1.0000 - val_loss: 0.1750 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 64/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2821 - accuracy: 0.9074 - top-5-accuracy: 0.9969 - val_loss: 0.1822 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 65/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2766 - accuracy: 0.9208 - top-5-accuracy: 0.9990 - val_loss: 0.1588 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 66/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2708 - accuracy: 0.9167 - top-5-accuracy: 0.9990 - val_loss: 0.1764 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 67/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2547 - accuracy: 0.9187 - top-5-accuracy: 0.9990 - val_loss: 0.1914 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 68/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2744 - accuracy: 0.9198 - top-5-accuracy: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 69/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2645 - accuracy: 0.9177 - top-5-accuracy: 0.9990 - val_loss: 0.1803 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 70/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2504 - accuracy: 0.9270 - top-5-accuracy: 1.0000 - val_loss: 0.2009 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 71/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2504 - accuracy: 0.9300 - top-5-accuracy: 0.9979 - val_loss: 0.1718 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 72/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2281 - accuracy: 0.9280 - top-5-accuracy: 0.9979 - val_loss: 0.1871 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 73/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2467 - accuracy: 0.9208 - top-5-accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 74/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2208 - accuracy: 0.9403 - top-5-accuracy: 0.9990 - val_loss: 0.1832 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 75/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2111 - accuracy: 0.9393 - top-5-accuracy: 0.9990 - val_loss: 0.1854 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 76/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2019 - accuracy: 0.9475 - top-5-accuracy: 0.9979 - val_loss: 0.1718 - val_accuracy: 0.9752 - val_top-5-accuracy: 0.9938\n",
      "Epoch 77/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.2114 - accuracy: 0.9393 - top-5-accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 78/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1896 - accuracy: 0.9465 - top-5-accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 79/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.2008 - accuracy: 0.9465 - top-5-accuracy: 0.9990 - val_loss: 0.1688 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 80/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.2006 - accuracy: 0.9455 - top-5-accuracy: 0.9990 - val_loss: 0.1797 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 81/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1910 - accuracy: 0.9444 - top-5-accuracy: 0.9990 - val_loss: 0.1821 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 82/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1817 - accuracy: 0.9486 - top-5-accuracy: 0.9990 - val_loss: 0.1796 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 83/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1753 - accuracy: 0.9496 - top-5-accuracy: 1.0000 - val_loss: 0.1645 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 84/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1723 - accuracy: 0.9506 - top-5-accuracy: 1.0000 - val_loss: 0.1887 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 85/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1712 - accuracy: 0.9527 - top-5-accuracy: 1.0000 - val_loss: 0.1649 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 86/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1575 - accuracy: 0.9619 - top-5-accuracy: 0.9990 - val_loss: 0.1838 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 87/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1611 - accuracy: 0.9568 - top-5-accuracy: 0.9990 - val_loss: 0.1900 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 88/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.1508 - accuracy: 0.9588 - top-5-accuracy: 0.9990 - val_loss: 0.2056 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 89/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1450 - accuracy: 0.9671 - top-5-accuracy: 0.9990 - val_loss: 0.1854 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 90/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1576 - accuracy: 0.9486 - top-5-accuracy: 0.9990 - val_loss: 0.1971 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 91/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1424 - accuracy: 0.9609 - top-5-accuracy: 1.0000 - val_loss: 0.1691 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 92/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1349 - accuracy: 0.9650 - top-5-accuracy: 1.0000 - val_loss: 0.1632 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 93/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1396 - accuracy: 0.9660 - top-5-accuracy: 1.0000 - val_loss: 0.1751 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 94/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1361 - accuracy: 0.9681 - top-5-accuracy: 1.0000 - val_loss: 0.1625 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 95/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1244 - accuracy: 0.9712 - top-5-accuracy: 1.0000 - val_loss: 0.2220 - val_accuracy: 0.9317 - val_top-5-accuracy: 0.9938\n",
      "Epoch 96/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1330 - accuracy: 0.9650 - top-5-accuracy: 1.0000 - val_loss: 0.1629 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 97/200\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.1132 - accuracy: 0.9763 - top-5-accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 98/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1119 - accuracy: 0.9722 - top-5-accuracy: 1.0000 - val_loss: 0.1735 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 99/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1226 - accuracy: 0.9712 - top-5-accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 100/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.1204 - accuracy: 0.9712 - top-5-accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 101/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1067 - accuracy: 0.9733 - top-5-accuracy: 1.0000 - val_loss: 0.1490 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 102/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0927 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 103/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0929 - accuracy: 0.9815 - top-5-accuracy: 1.0000 - val_loss: 0.1756 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 104/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0857 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 105/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0965 - accuracy: 0.9805 - top-5-accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 106/200\n",
      "31/31 [==============================] - 0s 16ms/step - loss: 0.0993 - accuracy: 0.9753 - top-5-accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 107/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1184 - accuracy: 0.9691 - top-5-accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 108/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1219 - accuracy: 0.9630 - top-5-accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 109/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1167 - accuracy: 0.9691 - top-5-accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 110/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.1119 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.1931 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 111/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0902 - accuracy: 0.9794 - top-5-accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 112/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0843 - accuracy: 0.9835 - top-5-accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 113/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0813 - accuracy: 0.9794 - top-5-accuracy: 1.0000 - val_loss: 0.1802 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 114/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0851 - accuracy: 0.9805 - top-5-accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 115/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0717 - accuracy: 0.9877 - top-5-accuracy: 1.0000 - val_loss: 0.1495 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 116/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0919 - accuracy: 0.9815 - top-5-accuracy: 1.0000 - val_loss: 0.1672 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 117/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0801 - accuracy: 0.9856 - top-5-accuracy: 1.0000 - val_loss: 0.1724 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 118/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0676 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.1413 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 119/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0624 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 120/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0674 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.1401 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 121/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0651 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.1829 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 122/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0668 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 123/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0820 - accuracy: 0.9805 - top-5-accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 124/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0669 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 125/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0569 - accuracy: 0.9877 - top-5-accuracy: 1.0000 - val_loss: 0.1618 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 126/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0533 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 127/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0660 - accuracy: 0.9866 - top-5-accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 128/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0618 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 129/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0556 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.1480 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 130/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0506 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1692 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 131/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0504 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 132/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0531 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 133/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0502 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 134/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0512 - accuracy: 0.9887 - top-5-accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 135/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0435 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 136/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0514 - accuracy: 0.9907 - top-5-accuracy: 1.0000 - val_loss: 0.1641 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 137/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0430 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 138/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0416 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1732 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 139/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0471 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1743 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 140/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0361 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 141/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0361 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.2071 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 142/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0425 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.1720 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 143/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0395 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.1815 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 144/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0376 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 145/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0389 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1834 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 146/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0774 - accuracy: 0.9763 - top-5-accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 147/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0410 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1773 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 148/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0390 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.1891 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 149/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0708 - accuracy: 0.9815 - top-5-accuracy: 1.0000 - val_loss: 0.1913 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 150/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0402 - accuracy: 0.9928 - top-5-accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 151/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0326 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1704 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 152/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0546 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 153/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0342 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 154/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0387 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1854 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 155/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0367 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.1856 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 156/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0271 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1624 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 157/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0260 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.2010 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 158/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0481 - accuracy: 0.9835 - top-5-accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 159/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0246 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1709 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 160/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0318 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 161/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0271 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1797 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 162/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0216 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1684 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 163/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0194 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 164/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0163 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 165/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0186 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 166/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0162 - accuracy: 1.0000 - top-5-accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 167/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0191 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.1464 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 168/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0186 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 169/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0162 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1414 - val_accuracy: 0.9752 - val_top-5-accuracy: 0.9938\n",
      "Epoch 170/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0143 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9752 - val_top-5-accuracy: 0.9938\n",
      "Epoch 171/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0140 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 172/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0232 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 173/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0145 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 174/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0173 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 175/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0147 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 176/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0170 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9565 - val_top-5-accuracy: 0.9938\n",
      "Epoch 177/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0274 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9441 - val_top-5-accuracy: 0.9938\n",
      "Epoch 178/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0168 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1651 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 179/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0131 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 180/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0134 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1534 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 181/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0119 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1448 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 182/200\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 0.0115 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1435 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 183/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0129 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 184/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0200 - accuracy: 0.9969 - top-5-accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 185/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0122 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1658 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 186/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0098 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 187/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0199 - accuracy: 0.9938 - top-5-accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 0.9255 - val_top-5-accuracy: 0.9938\n",
      "Epoch 188/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0204 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.1614 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 189/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0134 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1395 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 190/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0091 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 191/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0107 - accuracy: 0.9979 - top-5-accuracy: 1.0000 - val_loss: 0.1353 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 192/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0104 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 193/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0099 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1320 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "Epoch 194/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0194 - accuracy: 0.9949 - top-5-accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9379 - val_top-5-accuracy: 0.9938\n",
      "Epoch 195/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0378 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 196/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0503 - accuracy: 0.9825 - top-5-accuracy: 1.0000 - val_loss: 0.1813 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 197/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0425 - accuracy: 0.9897 - top-5-accuracy: 0.9990 - val_loss: 0.1217 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 198/200\n",
      "31/31 [==============================] - 1s 19ms/step - loss: 0.0737 - accuracy: 0.9753 - top-5-accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9503 - val_top-5-accuracy: 0.9938\n",
      "Epoch 199/200\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 0.0374 - accuracy: 0.9897 - top-5-accuracy: 1.0000 - val_loss: 0.1094 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9938\n",
      "Epoch 200/200\n",
      "31/31 [==============================] - 1s 18ms/step - loss: 0.0137 - accuracy: 0.9990 - top-5-accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9627 - val_top-5-accuracy: 0.9938\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8982 - accuracy: 0.8148 - top-5-accuracy: 0.9721\n",
      "Test accuracy: 81.48%\n",
      "Test top 5 accuracy: 97.21%\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "EPOCHS = 200\n",
    "\n",
    "\n",
    "def run_lstm_experiment():\n",
    "    # Initialize model\n",
    "    model = create_attn_lstm_classifier(11)\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_lstm_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ohg4ryQk_ZL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ap3V15fbk_dx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlAfielKk_es"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXr4ai1Zk_jB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRkH1_DSk_j6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCmCFlN1k_oS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xpb8coKk_pK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qozw98-ik_uH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
