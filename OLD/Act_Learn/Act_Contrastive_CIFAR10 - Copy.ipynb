{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81867ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaif\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 64\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e66a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d210bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e907221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "#train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.5, random_state=42 ) #50% unlab\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.2, random_state=40 ) #10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe2d039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcf31d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.02),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bddf6be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, None, None, 3)     7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_encoder():\n",
    "    resnet = tf.keras.applications.ResNet50V2( include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\" )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88895121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab5d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e7bf92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "997e86e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6e4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        logits = tf.divide( tf.matmul(  \n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)),self.temperature,)\n",
    "        \n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ad0f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,089,351\n",
      "Trainable params: 24,043,904\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7568\\3577858032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mencoder_with_projection_head\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m history = encoder_with_projection_head.fit(\n\u001b[0m\u001b[0;32m     12\u001b[0m     x=X_train, y=Y_train, batch_size=batch_size, epochs=2)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    961\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    964\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 785\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    786\u001b[0m             *args, **kwds))\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2758\u001b[0m             \u001b[1;31m# Only get placeholders for arguments, not captures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2759\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaceholder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"args\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2760\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2762\u001b[0m           \u001b[0mgraph_capture_container\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capture_func_lib\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2668\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 2670\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   2671\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2672\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1286\u001b[0m         if x is not None)\n\u001b[0;32m   1287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m     \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0mcontrol_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m       \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mMUST_RUN_ORDER_INSENSITIVE_STATEFUL_OPS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m         \u001b[1;31m# This will add it to self._independent_ops, but also mark it with an\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[1;31m# attribute.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2581\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2583\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2585\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=X_train, y=Y_train, batch_size=batch_size, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "933c5b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "157/157 [==============================] - 13s 47ms/step - loss: nan - sparse_categorical_accuracy: 0.1015\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 10s 62ms/step - loss: nan - sparse_categorical_accuracy: 0.1015\n",
      "157/157 [==============================] - 4s 21ms/step - loss: nan - sparse_categorical_accuracy: 0.1044\n",
      "Test accuracy: 10.44%\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "424f1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd5768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39dd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resmodel():\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(32))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b426045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model(X,Y,X_test, Y_test,num_epochs):\n",
    "    \n",
    "    data_augmentation.layers[0].adapt(X)\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    \n",
    "    encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                                     loss=SupervisedContrastiveLoss(temperature),)\n",
    "    \n",
    "    \n",
    "    history = encoder_with_projection_head.fit(x=X, y=Y, batch_size=256, epochs=num_epochs)\n",
    "    \n",
    "    \n",
    "    classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "    history = classifier.fit(x=X, y=Y, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    #model.summary()\n",
    "    return encoder,classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e1ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.concatenate((X_train, x_unlab))\n",
    "X_all = arr\n",
    "arr = np.concatenate((Y_train, y_unlab))\n",
    "Y_all = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c021533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test,num):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    encoder,classifier = create_model(X_train,Y_train,X_test, Y_test,num)\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") \n",
    "    \n",
    "    \n",
    "    return encoder,classifier\n",
    "\n",
    "\n",
    "encoder,classifier = train_full_model(X_all, X_test, Y_all, Y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,classifier = train_full_model(X_train, X_test, Y_train, Y_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e353d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef425778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb6c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "313/313 [==============================] - 836s 3s/step - loss: 3.9879\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 846s 3s/step - loss: 3.8646\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 860s 3s/step - loss: 3.8040\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 866s 3s/step - loss: 3.7500\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 866s 3s/step - loss: 3.7154\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "313/313 [==============================] - 82s 245ms/step - loss: 2.2071 - sparse_categorical_accuracy: 0.2391\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 80s 256ms/step - loss: 1.8932 - sparse_categorical_accuracy: 0.2936\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 77s 247ms/step - loss: 1.8172 - sparse_categorical_accuracy: 0.3063\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 84s 267ms/step - loss: 1.7967 - sparse_categorical_accuracy: 0.3158\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 81s 258ms/step - loss: 1.7724 - sparse_categorical_accuracy: 0.3231\n",
      "157/157 [==============================] - 17s 96ms/step - loss: 1.6518 - sparse_categorical_accuracy: 0.3800\n",
      "Acc Test : \n",
      "\n",
      "\n",
      "0.3799999952316284\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "360\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3210\n",
      "\n",
      "\n",
      "391/391 [==============================] - 70s 175ms/step\n",
      "782/782 [==============================] - 77s 97ms/step\n",
      "(2932,)\n",
      "(360,)\n",
      "(180,)\n",
      "(360,)\n",
      "92/92 [==============================] - 9s 98ms/step - loss: 1.5278 - sparse_categorical_accuracy: 0.4192\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.41916781663894653\n",
      "Now2\n",
      "(20360, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1273/1273 [==============================] - 115s 90ms/step - loss: 1.7747 - sparse_categorical_accuracy: 0.3265\n",
      "Epoch 2/5\n",
      "1273/1273 [==============================] - 115s 90ms/step - loss: 1.7423 - sparse_categorical_accuracy: 0.3391\n",
      "Epoch 3/5\n",
      "1273/1273 [==============================] - 117s 92ms/step - loss: 1.7341 - sparse_categorical_accuracy: 0.3453\n",
      "Epoch 4/5\n",
      "1273/1273 [==============================] - 115s 91ms/step - loss: 1.7279 - sparse_categorical_accuracy: 0.3458\n",
      "Epoch 5/5\n",
      "1273/1273 [==============================] - 116s 91ms/step - loss: 1.7086 - sparse_categorical_accuracy: 0.3555\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "360\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3210\n",
      "\n",
      "\n",
      "344/344 [==============================] - 62s 181ms/step\n",
      "687/687 [==============================] - 77s 112ms/step\n",
      "687/687 [==============================] - 74s 108ms/step\n",
      "(3031,)\n",
      "(360,)\n",
      "(180,)\n",
      "(360,)\n",
      "95/95 [==============================] - 10s 106ms/step - loss: 1.5406 - sparse_categorical_accuracy: 0.4088\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.4087759852409363\n",
      "Now2\n",
      "(20720, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1295/1295 [==============================] - 125s 97ms/step - loss: 1.7113 - sparse_categorical_accuracy: 0.3535\n",
      "Epoch 2/5\n",
      "1295/1295 [==============================] - 137s 106ms/step - loss: 1.7069 - sparse_categorical_accuracy: 0.3542\n",
      "Epoch 3/5\n",
      "1295/1295 [==============================] - 133s 103ms/step - loss: 1.6949 - sparse_categorical_accuracy: 0.3551\n",
      "Epoch 4/5\n",
      "1295/1295 [==============================] - 123s 95ms/step - loss: 1.6985 - sparse_categorical_accuracy: 0.3611\n",
      "Epoch 5/5\n",
      "1295/1295 [==============================] - 134s 103ms/step - loss: 1.6962 - sparse_categorical_accuracy: 0.3601\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "360\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3210\n",
      "\n",
      "\n",
      "294/294 [==============================] - 58s 198ms/step\n",
      "587/587 [==============================] - 64s 108ms/step\n",
      "587/587 [==============================] - 63s 108ms/step\n",
      "(2950,)\n",
      "(360,)\n",
      "(180,)\n",
      "(360,)\n",
      "93/93 [==============================] - 10s 105ms/step - loss: 1.5642 - sparse_categorical_accuracy: 0.4061\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.40610170364379883\n",
      "Now2\n",
      "(21080, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "1318/1318 [==============================] - 132s 100ms/step - loss: 1.6925 - sparse_categorical_accuracy: 0.3662\n",
      "Epoch 2/5\n",
      "1318/1318 [==============================] - 123s 94ms/step - loss: 1.6948 - sparse_categorical_accuracy: 0.3582\n",
      "Epoch 3/5\n",
      "1318/1318 [==============================] - 131s 99ms/step - loss: 1.6907 - sparse_categorical_accuracy: 0.3573\n",
      "Epoch 4/5\n",
      "1318/1318 [==============================] - 124s 94ms/step - loss: 1.6797 - sparse_categorical_accuracy: 0.3661\n",
      "Epoch 5/5\n",
      "1318/1318 [==============================] - 120s 91ms/step - loss: 1.6858 - sparse_categorical_accuracy: 0.3661\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "360\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "3210\n",
      "\n",
      "\n",
      "245/245 [==============================] - 48s 194ms/step\n",
      "490/490 [==============================] - 50s 103ms/step\n",
      "490/490 [==============================] - 54s 110ms/step\n",
      "(2979,)\n",
      "(360,)\n",
      "(180,)\n",
      "(360,)\n",
      "94/94 [==============================] - 10s 101ms/step - loss: 1.5712 - sparse_categorical_accuracy: 0.4206\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.4206109344959259\n",
      "Now2\n",
      "(21440, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      " 996/1340 [=====================>........] - ETA: 33s - loss: 1.6734 - sparse_categorical_accuracy: 0.3709"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "    \n",
    "    \n",
    "    print(\"Acc Test : \")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    entropy = []\n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "        nn_clusters = 10\n",
    "        num_points_per_class = int ( np.round (d/10)   )\n",
    "        \n",
    "        budget =  int ( np.round (d/200)   )\n",
    "        num_points_per_class = num_points_per_class - (budget*2)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*20)\n",
    "        print(\"\\n\")\n",
    "        print(\"Chosen in each iter : \")\n",
    "        print(num_points_per_class*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        x_ulb = encoder.predict(x_unlab, batch_size=batch_size)\n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        closest_points_indices = []\n",
    "        annotate_indices = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            closest_indices = cluster_indices[np.argsort(distances)[:num_points_per_class]]\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]] \n",
    "            \n",
    "            closest_points_indices.extend(closest_indices)\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "\n",
    "        chosen_indices = closest_points_indices\n",
    "    \n",
    "        rnd = chosen_indices\n",
    "        \n",
    "        if (iteration == 0):\n",
    "            xx_ulb = classifier.predict(x_unlab, batch_size=32)\n",
    "            argmax_indices = np.argmax(xx_ulb, axis=1)\n",
    "            entropy = x_ulb[np.arange(len(xx_ulb)), argmax_indices]\n",
    "            most_uncertain_idx = np.argsort(entropy)[-(budget*10):]\n",
    "            \n",
    "        else :\n",
    "            xx_ulb = cls0.predict(x_unlab, batch_size=32)\n",
    "            argmax_indices = np.argmax(xx_ulb, axis=1)\n",
    "            entropy = x_ulb[np.arange(len(xx_ulb)), argmax_indices]\n",
    "            \n",
    "            xx_ulb = cls1.predict(x_unlab, batch_size=32)\n",
    "            argmax_indices = np.argmax(xx_ulb, axis=1)\n",
    "            entropy_new = x_ulb[np.arange(len(xx_ulb)), argmax_indices]\n",
    "            \n",
    "            most_uncertain_idx = np.argsort(np.abs(entropy_new - entropy))[::-1][:(budget*10)]\n",
    "            \n",
    "        annotate_indices.extend(most_uncertain_idx)\n",
    "        annt = annotate_indices\n",
    "        \n",
    "        print(np.shape(rnd))\n",
    "        print(np.shape(annotate_indices))\n",
    "        print(np.shape(most_uncertain_idx))\n",
    "        print(np.shape(annt))\n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(rnd) - set(annt))\n",
    "        new_lab = x_unlab[rnd]\n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train,new_annt))\n",
    "        X_train = arr\n",
    "\n",
    "        \n",
    "        accuracy = classifier.evaluate(new_lab, y_unlab[rnd], batch_size=32)[1]\n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        \n",
    "        \n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        #history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=5)\n",
    "        \n",
    "        print(\"Now2\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        cls0=classifier\n",
    "        cls1=classifier\n",
    "        history = classifier.fit(x=X_train, y=Y_train, batch_size=16, epochs=20) \n",
    "        cls1=classifier\n",
    "   \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "#train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.5, random_state=42 )\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.2, random_state=40 )\n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=64, epochs=50)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=64, epochs=40) \n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a1dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb8437f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0279e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(final_encoder, final_classifier, X_test, Y_test):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Test set evaluation: \", model.evaluate( X_test, Y_test , verbose=0, return_dict=True), )\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee2f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7460f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4827f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "   \n",
    "    \n",
    "    entropy = []\n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    #x = int(np.round( l/d ))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "        \n",
    "        nn_clusters = 10\n",
    "        num_points_per_class = int ( np.round (d/10)   )\n",
    "        \n",
    "        budget =  int(num_points_per_class /10 )\n",
    "        num_points_per_class = num_points_per_class - budget\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "        print(\"\\n\")\n",
    "        print(\"Chosen in each iter : \")\n",
    "        print(num_points_per_class*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "  \n",
    "        try :\n",
    "            x_ulb = encoder.predict(x_unlab, batch_size=batch_size)\n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        if (iteration == 0):\n",
    "            argmax_indices = np.argmax(x_ulb, axis=1)\n",
    "            entropy = x_ulb[np.arange(len(x_ulb)), argmax_indices]\n",
    "            most_uncertain_idx = np.argsort(entropy)[-budget:]\n",
    "            \n",
    "        else :\n",
    "            argmax_indices = np.argmax(x_ulb, axis=1)\n",
    "            entropy_new = x_ulb[np.arange(len(x_ulb)), argmax_indices]\n",
    "            most_uncertain_idx = np.argsort(np.abs(entropy_new - entropy))[::-1][:budget]\n",
    "            entropy = entropy_new\n",
    "        \n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "        annotate_indices = most_uncertain_idx\n",
    "        \n",
    "    \n",
    "        print(np.shape(annotate_indices))\n",
    "        \n",
    "        annt = annotate_indices\n",
    "        \n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(annt))\n",
    "        \n",
    "        #add those index to from unlablled set to training set\n",
    "        #new_lab = x_unlab[rnd]\n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train,new_annt))\n",
    "        X_train = arr\n",
    "\n",
    "        \n",
    "        try :\n",
    "            accuracy = classifier.evaluate(new_lab, y_unlab[rnd], batch_size=8)[1]\n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "        \n",
    "        annt_y = y_unlab[annt]\n",
    "        print(\"This\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"Now\")\n",
    "        #print(np.shape(new_y))\n",
    "        \n",
    "        #new_yy = np.argmax(new_y, axis=1)\n",
    "        \n",
    "        #new_yy = new_yy.reshape(len(new_y),1)\n",
    "        \n",
    "        \n",
    "        arr = np.concatenate((Y_train, annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        #try :\n",
    "            #history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=5)\n",
    "        #except :\n",
    "            #return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Now2\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try :\n",
    "            history = classifier.fit(x=X_train, y=Y_train, batch_size=4, epochs=1) \n",
    "        except:\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        #test on data\n",
    "    \n",
    "    #accuracy = classifier.evaluate(X_test, Y_test, batch_size=4)[1]\n",
    "   \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8f260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f7d5f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6744c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762c00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b504927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538b0b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b4055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d33f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41903ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bded4da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2faabd69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d20487e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96f029f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b867a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1da12574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [7],\n",
       "       [8],\n",
       "       ...,\n",
       "       [3],\n",
       "       [1],\n",
       "       [3]], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0af7370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =np.argmax(Y_train)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c88a610a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5784\\240059899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf313e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2ae798e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7ae8f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7524aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dbae46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18868389, 0.13183018, 0.78863877, 0.5255329 , 0.26790884,\n",
       "       0.50838265, 0.92804925, 0.97256256, 0.51417701, 0.12704275])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy = []\n",
    "x_ulb = np.random.rand(500, 10)\n",
    "argmax_indices = np.argmax(x_ulb, axis=1)\n",
    "x_ulb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b3898f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9725625636930236"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = 50\n",
    "entropy = x_ulb[np.arange(len(x_ulb)), argmax_indices]\n",
    "entropy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bebfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fdd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b802a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
