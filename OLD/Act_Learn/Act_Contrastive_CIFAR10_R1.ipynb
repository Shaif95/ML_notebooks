{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0626244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras import Input\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "CROP_TO = 32\n",
    "SEED = 26\n",
    "\n",
    "PROJECT_DIM = 128\n",
    "LATENT_DIM = 512\n",
    "WEIGHT_DECAY = 0.0005\n",
    "learning_rate = 0.0001\n",
    "batch_size = 128\n",
    "hidden_units = 512\n",
    "projection_units = 256\n",
    "num_epochs = 2\n",
    "dropout_rate = 0.5\n",
    "\n",
    "temperature = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4bc075",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0e5c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ec62419",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "#train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 \n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.5, random_state=42 )\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.2, random_state=40 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7fce7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fed3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Normalization(),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.02),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.2),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Setting the state of the normalization layer.\n",
    "data_augmentation.layers[0].adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaaaa837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar10-encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 32, 32, 3)         7         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 2048)              23564800  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,564,807\n",
      "Trainable params: 23,519,360\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "def create_encoder():\n",
    "    resnet = tf.keras.applications.ResNet50V2( include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling=\"avg\" )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    augmented = data_augmentation(inputs)\n",
    "    outputs = resnet(augmented)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-encoder\")\n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19543794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fe732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a789c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder, trainable=True):\n",
    "\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    features = layers.Dense(hidden_units, activation=\"relu\")(features)\n",
    "    features = layers.Dropout(dropout_rate)(features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"cifar10-classifier\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c8341c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8cc5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "\n",
    "        logits = tf.divide( tf.matmul(  \n",
    "            feature_vectors_normalized, tf.transpose(feature_vectors_normalized)),self.temperature,)\n",
    "        \n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f0185c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifar-encoder_with_projection-head\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " cifar10-encoder (Functional  (None, 2048)             23564807  \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,089,351\n",
      "Trainable params: 24,043,904\n",
      "Non-trainable params: 45,447\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "157/157 [==============================] - 51s 171ms/step - loss: 6.2643\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 18s 115ms/step - loss: 4.6970\n"
     ]
    }
   ],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    x=X_train, y=Y_train, batch_size=batch_size, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f581b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c248173c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a4d7c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "157/157 [==============================] - 16s 45ms/step - loss: 2.6423 - sparse_categorical_accuracy: 0.1870\n",
      "Epoch 2/2\n",
      "157/157 [==============================] - 7s 42ms/step - loss: 2.2224 - sparse_categorical_accuracy: 0.2683\n",
      "157/157 [==============================] - 10s 47ms/step - loss: 1.7564 - sparse_categorical_accuracy: 0.3674\n",
      "Test accuracy: 36.74%\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e715bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560034d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263cb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resmodel():\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.Conv2D(64, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(32))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b477783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model(X,Y,X_test, Y_test,num_epochs):\n",
    "    \n",
    "    data_augmentation.layers[0].adapt(X)\n",
    "    \n",
    "    encoder = create_encoder()\n",
    "\n",
    "    encoder_with_projection_head = add_projection_head(encoder)\n",
    "    \n",
    "    encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                                     loss=SupervisedContrastiveLoss(temperature),)\n",
    "    \n",
    "    \n",
    "    history = encoder_with_projection_head.fit(x=X, y=Y, batch_size=256, epochs=num_epochs)\n",
    "    \n",
    "    \n",
    "    classifier = create_classifier(encoder, trainable=False) \n",
    "\n",
    "    history = classifier.fit(x=X, y=Y, batch_size=batch_size, epochs=num_epochs) \n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    #model.summary()\n",
    "    return encoder,classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a15d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.concatenate((X_train, x_unlab))\n",
    "X_all = arr\n",
    "arr = np.concatenate((Y_train, y_unlab))\n",
    "Y_all = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b44199e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 50s 187ms/step - loss: 6.4580\n",
      "352/352 [==============================] - 24s 43ms/step - loss: 2.0910 - sparse_categorical_accuracy: 0.2558\n",
      "157/157 [==============================] - 9s 42ms/step - loss: 1.6691 - sparse_categorical_accuracy: 0.4218\n",
      "Test accuracy: 42.18%\n",
      "157/157 [==============================] - 5s 33ms/step - loss: 1.6691 - sparse_categorical_accuracy: 0.4218\n",
      "Test accuracy: 42.18%\n"
     ]
    }
   ],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test,num):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    encoder,classifier = create_model(X_train,Y_train,X_test, Y_test,num)\n",
    "\n",
    "    accuracy = classifier.evaluate(X_test, Y_test)[1] \n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\") \n",
    "    \n",
    "    \n",
    "    return encoder,classifier\n",
    "\n",
    "\n",
    "encoder,classifier = train_full_model(X_all, X_test, Y_all, Y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder,classifier = train_full_model(X_train, X_test, Y_train, Y_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28f8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef192154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4feff23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8250\n",
      "8250\n",
      "Epoch 1/10\n",
      "65/65 [==============================] - 24s 128ms/step - loss: 8.0198\n",
      "Epoch 2/10\n",
      "65/65 [==============================] - 8s 127ms/step - loss: 4.9923\n",
      "Epoch 3/10\n",
      "65/65 [==============================] - 7s 102ms/step - loss: 4.7582\n",
      "Epoch 4/10\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 4.6621\n",
      "Epoch 5/10\n",
      "65/65 [==============================] - 7s 111ms/step - loss: 4.5721\n",
      "Epoch 6/10\n",
      "65/65 [==============================] - 8s 122ms/step - loss: 4.4990\n",
      "Epoch 7/10\n",
      "65/65 [==============================] - 8s 124ms/step - loss: 4.4396\n",
      "Epoch 8/10\n",
      "65/65 [==============================] - 7s 106ms/step - loss: 4.3716\n",
      "Epoch 9/10\n",
      "65/65 [==============================] - 7s 103ms/step - loss: 4.3125\n",
      "Epoch 10/10\n",
      "65/65 [==============================] - 8s 130ms/step - loss: 4.2367\n",
      "Epoch 1/10\n",
      "129/129 [==============================] - 18s 69ms/step - loss: 1.8925 - sparse_categorical_accuracy: 0.4444\n",
      "Epoch 2/10\n",
      "129/129 [==============================] - 6s 48ms/step - loss: 1.4569 - sparse_categorical_accuracy: 0.5781\n",
      "Epoch 3/10\n",
      "129/129 [==============================] - 7s 52ms/step - loss: 1.3725 - sparse_categorical_accuracy: 0.6082\n",
      "Epoch 4/10\n",
      "129/129 [==============================] - 7s 58ms/step - loss: 1.3006 - sparse_categorical_accuracy: 0.6108\n",
      "Epoch 5/10\n",
      "129/129 [==============================] - 7s 50ms/step - loss: 1.2328 - sparse_categorical_accuracy: 0.6224\n",
      "Epoch 6/10\n",
      "129/129 [==============================] - 6s 46ms/step - loss: 1.2217 - sparse_categorical_accuracy: 0.6245\n",
      "Epoch 7/10\n",
      "129/129 [==============================] - 7s 56ms/step - loss: 1.1768 - sparse_categorical_accuracy: 0.6282\n",
      "Epoch 8/10\n",
      "129/129 [==============================] - 8s 63ms/step - loss: 1.1675 - sparse_categorical_accuracy: 0.6356\n",
      "Epoch 9/10\n",
      "129/129 [==============================] - 7s 56ms/step - loss: 1.1351 - sparse_categorical_accuracy: 0.6322\n",
      "Epoch 10/10\n",
      "129/129 [==============================] - 6s 47ms/step - loss: 1.1317 - sparse_categorical_accuracy: 0.6371\n",
      "313/313 [==============================] - 13s 35ms/step - loss: 1.9402 - sparse_categorical_accuracy: 0.3925\n",
      "Acc Test : \n",
      "\n",
      "\n",
      "0.39250001311302185\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(11000, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(449,)\n",
      "(449,)\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 1.7072 - sparse_categorical_accuracy: 0.7305\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.730512261390686\n",
      "This\n",
      "(8250, 1)\n",
      "Now\n",
      "(449, 10)\n",
      "Epoch 1/5\n",
      "72/72 [==============================] - 18s 114ms/step - loss: 4.1495\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 7s 100ms/step - loss: 4.0883\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 8s 110ms/step - loss: 4.0481\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 7s 98ms/step - loss: 3.9946\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 7s 103ms/step - loss: 3.9452\n",
      "Now2\n",
      "(9148, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "143/143 [==============================] - 10s 40ms/step - loss: 0.8595 - sparse_categorical_accuracy: 0.7441\n",
      "Epoch 2/5\n",
      "143/143 [==============================] - 6s 45ms/step - loss: 0.8333 - sparse_categorical_accuracy: 0.7475\n",
      "Epoch 3/5\n",
      "143/143 [==============================] - 6s 43ms/step - loss: 0.8070 - sparse_categorical_accuracy: 0.7554\n",
      "Epoch 4/5\n",
      "143/143 [==============================] - 6s 40ms/step - loss: 0.8164 - sparse_categorical_accuracy: 0.7477\n",
      "Epoch 5/5\n",
      "143/143 [==============================] - 6s 43ms/step - loss: 0.8009 - sparse_categorical_accuracy: 0.7523\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(10550, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(500,)\n",
      "(500,)\n",
      "8/8 [==============================] - 1s 116ms/step - loss: 0.7697 - sparse_categorical_accuracy: 0.7620\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7620000243186951\n",
      "This\n",
      "(9148, 1)\n",
      "Now\n",
      "(500, 10)\n",
      "Epoch 1/5\n",
      "80/80 [==============================] - 8s 105ms/step - loss: 3.9294\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 8s 99ms/step - loss: 3.8639\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 7s 90ms/step - loss: 3.8285\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 9s 107ms/step - loss: 3.7938\n",
      "Epoch 5/5\n",
      "80/80 [==============================] - 7s 84ms/step - loss: 3.7661\n",
      "Now2\n",
      "(10148, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "159/159 [==============================] - 5s 34ms/step - loss: 0.6863 - sparse_categorical_accuracy: 0.8007\n",
      "Epoch 2/5\n",
      "159/159 [==============================] - 5s 35ms/step - loss: 0.6554 - sparse_categorical_accuracy: 0.8055\n",
      "Epoch 3/5\n",
      "159/159 [==============================] - 6s 38ms/step - loss: 0.6430 - sparse_categorical_accuracy: 0.8066\n",
      "Epoch 4/5\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.6358 - sparse_categorical_accuracy: 0.8083\n",
      "Epoch 5/5\n",
      "159/159 [==============================] - 6s 37ms/step - loss: 0.6368 - sparse_categorical_accuracy: 0.8086\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(10049, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(463,)\n",
      "(463,)\n",
      "8/8 [==============================] - 1s 112ms/step - loss: 0.8364 - sparse_categorical_accuracy: 0.7473\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7473002076148987\n",
      "This\n",
      "(10148, 1)\n",
      "Now\n",
      "(463, 10)\n",
      "Epoch 1/5\n",
      "87/87 [==============================] - 9s 108ms/step - loss: 3.7682\n",
      "Epoch 2/5\n",
      "87/87 [==============================] - 8s 87ms/step - loss: 3.7424\n",
      "Epoch 3/5\n",
      "87/87 [==============================] - 10s 112ms/step - loss: 3.6912\n",
      "Epoch 4/5\n",
      "87/87 [==============================] - 9s 98ms/step - loss: 3.6693\n",
      "Epoch 5/5\n",
      "87/87 [==============================] - 10s 121ms/step - loss: 3.6524\n",
      "Now2\n",
      "(11074, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "174/174 [==============================] - 7s 41ms/step - loss: 0.5803 - sparse_categorical_accuracy: 0.8309\n",
      "Epoch 2/5\n",
      "174/174 [==============================] - 8s 48ms/step - loss: 0.5690 - sparse_categorical_accuracy: 0.8320\n",
      "Epoch 3/5\n",
      "174/174 [==============================] - 9s 52ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.8375\n",
      "Epoch 4/5\n",
      "174/174 [==============================] - 9s 52ms/step - loss: 0.5456 - sparse_categorical_accuracy: 0.8401\n",
      "Epoch 5/5\n",
      "174/174 [==============================] - 8s 47ms/step - loss: 0.5417 - sparse_categorical_accuracy: 0.8364\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(9585, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(458,)\n",
      "(458,)\n",
      "8/8 [==============================] - 1s 120ms/step - loss: 0.7621 - sparse_categorical_accuracy: 0.8122\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8122270703315735\n",
      "This\n",
      "(11074, 1)\n",
      "Now\n",
      "(458, 10)\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 11s 114ms/step - loss: 3.6438\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 11s 117ms/step - loss: 3.6168\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 11s 117ms/step - loss: 3.6002\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 9s 96ms/step - loss: 3.5869\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 9s 95ms/step - loss: 3.5538\n",
      "Now2\n",
      "(11990, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 10s 53ms/step - loss: 0.4809 - sparse_categorical_accuracy: 0.8595\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 10s 51ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.8638\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 7s 40ms/step - loss: 0.4571 - sparse_categorical_accuracy: 0.8685\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 8s 42ms/step - loss: 0.4541 - sparse_categorical_accuracy: 0.8658\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 9s 46ms/step - loss: 0.4465 - sparse_categorical_accuracy: 0.8654\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(9126, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(500,)\n",
      "(500,)\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 0.8027 - sparse_categorical_accuracy: 0.7660\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.765999972820282\n",
      "This\n",
      "(11990, 1)\n",
      "Now\n",
      "(500, 10)\n",
      "Epoch 1/5\n",
      "102/102 [==============================] - 11s 107ms/step - loss: 3.5784\n",
      "Epoch 2/5\n",
      "102/102 [==============================] - 12s 113ms/step - loss: 3.5604\n",
      "Epoch 3/5\n",
      "102/102 [==============================] - 13s 130ms/step - loss: 3.5361\n",
      "Epoch 4/5\n",
      "102/102 [==============================] - 10s 98ms/step - loss: 3.5145\n",
      "Epoch 5/5\n",
      "102/102 [==============================] - 10s 96ms/step - loss: 3.4996\n",
      "Now2\n",
      "(12990, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "203/203 [==============================] - 9s 46ms/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8768\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 9s 42ms/step - loss: 0.4065 - sparse_categorical_accuracy: 0.8823\n",
      "Epoch 3/5\n",
      "203/203 [==============================] - 9s 44ms/step - loss: 0.4083 - sparse_categorical_accuracy: 0.8805\n",
      "Epoch 4/5\n",
      "203/203 [==============================] - 9s 46ms/step - loss: 0.4062 - sparse_categorical_accuracy: 0.8821\n",
      "Epoch 5/5\n",
      "203/203 [==============================] - 9s 42ms/step - loss: 0.4002 - sparse_categorical_accuracy: 0.8794\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(8625, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(462,)\n",
      "(462,)\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.6628 - sparse_categorical_accuracy: 0.8009\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8008658289909363\n",
      "This\n",
      "(12990, 1)\n",
      "Now\n",
      "(462, 10)\n",
      "Epoch 1/5\n",
      "109/109 [==============================] - 13s 115ms/step - loss: 3.5173\n",
      "Epoch 2/5\n",
      "109/109 [==============================] - 10s 94ms/step - loss: 3.5147\n",
      "Epoch 3/5\n",
      "109/109 [==============================] - 11s 105ms/step - loss: 3.4972\n",
      "Epoch 4/5\n",
      "109/109 [==============================] - 13s 118ms/step - loss: 3.4861\n",
      "Epoch 5/5\n",
      "109/109 [==============================] - 10s 95ms/step - loss: 3.4680\n",
      "Now2\n",
      "(13914, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "218/218 [==============================] - 11s 48ms/step - loss: 0.3999 - sparse_categorical_accuracy: 0.8854\n",
      "Epoch 2/5\n",
      "218/218 [==============================] - 11s 53ms/step - loss: 0.3895 - sparse_categorical_accuracy: 0.8877\n",
      "Epoch 3/5\n",
      "218/218 [==============================] - 12s 54ms/step - loss: 0.3857 - sparse_categorical_accuracy: 0.8869\n",
      "Epoch 4/5\n",
      "218/218 [==============================] - 11s 50ms/step - loss: 0.3708 - sparse_categorical_accuracy: 0.8936\n",
      "Epoch 5/5\n",
      "218/218 [==============================] - 10s 45ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8908\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(8162, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(474,)\n",
      "(474,)\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.6131 - sparse_categorical_accuracy: 0.8354\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8354430198669434\n",
      "This\n",
      "(13914, 1)\n",
      "Now\n",
      "(474, 10)\n",
      "Epoch 1/5\n",
      "117/117 [==============================] - 12s 99ms/step - loss: 3.4869\n",
      "Epoch 2/5\n",
      "117/117 [==============================] - 12s 101ms/step - loss: 3.4728\n",
      "Epoch 3/5\n",
      "117/117 [==============================] - 13s 110ms/step - loss: 3.4680\n",
      "Epoch 4/5\n",
      "117/117 [==============================] - 14s 117ms/step - loss: 3.4604\n",
      "Epoch 5/5\n",
      "117/117 [==============================] - 15s 131ms/step - loss: 3.4374\n",
      "Now2\n",
      "(14862, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.3681 - sparse_categorical_accuracy: 0.8926\n",
      "Epoch 2/5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.3581 - sparse_categorical_accuracy: 0.8930\n",
      "Epoch 3/5\n",
      "233/233 [==============================] - 10s 41ms/step - loss: 0.3591 - sparse_categorical_accuracy: 0.8943\n",
      "Epoch 4/5\n",
      "233/233 [==============================] - 11s 46ms/step - loss: 0.3460 - sparse_categorical_accuracy: 0.8985\n",
      "Epoch 5/5\n",
      "233/233 [==============================] - 10s 43ms/step - loss: 0.3508 - sparse_categorical_accuracy: 0.8934\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(7688, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(477,)\n",
      "(477,)\n",
      "8/8 [==============================] - 1s 131ms/step - loss: 0.8214 - sparse_categorical_accuracy: 0.7987\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7987421154975891\n",
      "This\n",
      "(14862, 1)\n",
      "Now\n",
      "(477, 10)\n",
      "Epoch 1/5\n",
      "124/124 [==============================] - 14s 108ms/step - loss: 3.4608\n",
      "Epoch 2/5\n",
      "124/124 [==============================] - 13s 109ms/step - loss: 3.4593\n",
      "Epoch 3/5\n",
      "124/124 [==============================] - 12s 97ms/step - loss: 3.4465\n",
      "Epoch 4/5\n",
      "124/124 [==============================] - 14s 110ms/step - loss: 3.4359\n",
      "Epoch 5/5\n",
      "124/124 [==============================] - 13s 106ms/step - loss: 3.4274\n",
      "Now2\n",
      "(15816, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "248/248 [==============================] - 11s 46ms/step - loss: 0.3461 - sparse_categorical_accuracy: 0.8990\n",
      "Epoch 2/5\n",
      "248/248 [==============================] - 14s 55ms/step - loss: 0.3341 - sparse_categorical_accuracy: 0.8981\n",
      "Epoch 3/5\n",
      "248/248 [==============================] - 13s 52ms/step - loss: 0.3333 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 4/5\n",
      "248/248 [==============================] - 15s 62ms/step - loss: 0.3316 - sparse_categorical_accuracy: 0.9012\n",
      "Epoch 5/5\n",
      "248/248 [==============================] - 13s 51ms/step - loss: 0.3184 - sparse_categorical_accuracy: 0.9024\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(7211, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(467,)\n",
      "(467,)\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 0.7721 - sparse_categorical_accuracy: 0.7880\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.7880085706710815\n",
      "This\n",
      "(15816, 1)\n",
      "Now\n",
      "(467, 10)\n",
      "Epoch 1/5\n",
      "131/131 [==============================] - 15s 117ms/step - loss: 3.4328\n",
      "Epoch 2/5\n",
      "131/131 [==============================] - 16s 123ms/step - loss: 3.4278\n",
      "Epoch 3/5\n",
      "131/131 [==============================] - 17s 127ms/step - loss: 3.4182\n",
      "Epoch 4/5\n",
      "131/131 [==============================] - 14s 110ms/step - loss: 3.4114\n",
      "Epoch 5/5\n",
      "131/131 [==============================] - 14s 105ms/step - loss: 3.4155\n",
      "Now2\n",
      "(16750, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "262/262 [==============================] - 12s 44ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 2/5\n",
      "262/262 [==============================] - 13s 50ms/step - loss: 0.3222 - sparse_categorical_accuracy: 0.9027\n",
      "Epoch 3/5\n",
      "262/262 [==============================] - 13s 49ms/step - loss: 0.3272 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 4/5\n",
      "262/262 [==============================] - 12s 45ms/step - loss: 0.3238 - sparse_categorical_accuracy: 0.9013\n",
      "Epoch 5/5\n",
      "262/262 [==============================] - 12s 47ms/step - loss: 0.3198 - sparse_categorical_accuracy: 0.8996\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration : \n",
      "10\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(6743, 2048)\n",
      "\n",
      "\n",
      "Annotated in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "Chosen in each iter : \n",
      "500\n",
      "\n",
      "\n",
      "(500,)\n",
      "(500,)\n",
      "8/8 [==============================] - 1s 50ms/step - loss: 0.6689 - sparse_categorical_accuracy: 0.8320\n",
      "Acc : \n",
      "\n",
      "\n",
      "0.8320000171661377\n",
      "This\n",
      "(16750, 1)\n",
      "Now\n",
      "(500, 10)\n",
      "Epoch 1/5\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 3.4200\n",
      "Epoch 2/5\n",
      "139/139 [==============================] - 15s 106ms/step - loss: 3.4188\n",
      "Epoch 3/5\n",
      "139/139 [==============================] - 15s 105ms/step - loss: 3.4065\n",
      "Epoch 4/5\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 3.4099\n",
      "Epoch 5/5\n",
      "139/139 [==============================] - 15s 109ms/step - loss: 3.4002\n",
      "Now2\n",
      "(17750, 1)\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "278/278 [==============================] - 11s 40ms/step - loss: 0.3239 - sparse_categorical_accuracy: 0.9001\n",
      "Epoch 2/5\n",
      "278/278 [==============================] - 14s 50ms/step - loss: 0.3137 - sparse_categorical_accuracy: 0.9030\n",
      "Epoch 3/5\n",
      "278/278 [==============================] - 14s 51ms/step - loss: 0.3030 - sparse_categorical_accuracy: 0.9042\n",
      "Epoch 4/5\n",
      "278/278 [==============================] - 12s 44ms/step - loss: 0.2997 - sparse_categorical_accuracy: 0.9049\n",
      "Epoch 5/5\n",
      "278/278 [==============================] - 15s 54ms/step - loss: 0.2949 - sparse_categorical_accuracy: 0.9082\n",
      "1250/1250 [==============================] - 46s 35ms/step - loss: 1.3050 - sparse_categorical_accuracy: 0.6520\n",
      "Acc Test : \n",
      "\n",
      "\n",
      "0.6520000100135803\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_active_learning_models(encoder,classifier,X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations,num_epochs=1):\n",
    "\n",
    "    # Creating lists for storing metrics\n",
    "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "\n",
    "    try :\n",
    "        accuracy = classifier.evaluate(X_test, Y_test, batch_size=32)[1]\n",
    "    except :\n",
    "        return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "    \n",
    "    print(\"Acc Test : \")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy)\n",
    "    print(\"\\n\")\n",
    "   \n",
    "    \n",
    "    \n",
    "    l = len(y_unlab)\n",
    "    d = int ( np.round ( l/num_iterations ) )\n",
    "    #x = int(np.round( l/d ))\n",
    "    \n",
    "    for iteration in range(10):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration+1)\n",
    "        \n",
    "        print(\"\\n\\n\\n\\n\\n\")\n",
    "\n",
    "  \n",
    "        try :\n",
    "            x_ulb = encoder.predict(x_unlab, batch_size=batch_size)\n",
    "        except :\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(np.shape(x_ulb))\n",
    "\n",
    "\n",
    "        nn_clusters = 10\n",
    "        num_points_per_class = int ( np.round (d/10)   )\n",
    "        \n",
    "        budget =  50\n",
    "        num_points_per_class = 50\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"Annotated in each iter : \")\n",
    "        print(budget*10)\n",
    "        print(\"\\n\")\n",
    "        print(\"Chosen in each iter : \")\n",
    "        print(num_points_per_class*10)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=nn_clusters, init='k-means++', n_init=10).fit(x_ulb)\n",
    "\n",
    "        closest_points_indices = []\n",
    "        annotate_indices = []\n",
    "        \n",
    "        for i in range(10):\n",
    "            \n",
    "            cluster_center = kmeans.cluster_centers_[i]\n",
    "            \n",
    "            cluster_indices = np.where(kmeans.labels_ == i)[0]\n",
    "            \n",
    "            distances = np.linalg.norm(x_ulb[cluster_indices] - cluster_center, axis=1)\n",
    "            \n",
    "            closest_indices = cluster_indices[np.argsort(distances)[:budget]]\n",
    "            \n",
    "            annotate = cluster_indices[np.argsort(distances,-1)[:budget]]\n",
    "            \n",
    "            closest_points_indices.extend(closest_indices)\n",
    "            \n",
    "            annotate_indices.extend(annotate)\n",
    "\n",
    "        chosen_indices = closest_points_indices\n",
    "    \n",
    "        print(np.shape(chosen_indices))\n",
    "        print(np.shape(annotate_indices))\n",
    "        \n",
    "        rnd = chosen_indices\n",
    "        \n",
    "        annt = annotate_indices\n",
    "        \n",
    "        \n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(rnd) - set(annt))\n",
    "        \n",
    "        #add those index to from unlablled set to training set\n",
    "        new_lab = x_unlab[rnd]\n",
    "        new_annt = x_unlab[annt]\n",
    "        arr = np.concatenate((X_train, new_lab,new_annt))\n",
    "        X_train = arr\n",
    "\n",
    "        \n",
    "        try :\n",
    "            accuracy = classifier.evaluate(new_lab, y_unlab[rnd], batch_size=64)[1]\n",
    "        except :\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Acc : \")\n",
    "        print(\"\\n\")\n",
    "        print(accuracy)\n",
    "        \n",
    "        #predict on the set and add to training data\n",
    "        try :\n",
    "            new_y = np.round(classifier.predict(new_lab, batch_size=64))\n",
    "        except :\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        annt_y = y_unlab[annt]\n",
    "        print(\"This\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"Now\")\n",
    "        print(np.shape(new_y))\n",
    "        \n",
    "        new_yy = np.argmax(new_y, axis=1)\n",
    "        \n",
    "        new_yy = new_yy.reshape(len(new_y),1)\n",
    "        \n",
    "        \n",
    "        arr = np.concatenate((Y_train, new_yy,annt_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        \n",
    "        try :\n",
    "            history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=5)\n",
    "        except :\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        print(\"Now2\")\n",
    "        print(np.shape(Y_train))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        try :\n",
    "            history = classifier.fit(x=X_train, y=Y_train, batch_size=64, epochs=5) \n",
    "        except :\n",
    "            return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "        \n",
    "        #test on data\n",
    "    \n",
    "    try :\n",
    "        accuracy = classifier.evaluate(X_test, Y_test, batch_size=8)[1]\n",
    "    except :\n",
    "        return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "    \n",
    "    print(\"Acc Test : \")\n",
    "    print(\"\\n\")\n",
    "    print(accuracy)\n",
    "    print(\"\\n\")\n",
    "   \n",
    "    \n",
    "    return encoder, classifier,X_train,Y_train,x_unlab,y_unlab \n",
    "\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load CIFAR-10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train_flat = y_train.flatten()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Choose 5 random classes to undersample\n",
    "all_classes = np.arange(10)\n",
    "low_classes = np.random.choice(all_classes, 5, replace=False)\n",
    "high_classes = np.setdiff1d(all_classes, low_classes)\n",
    "\n",
    "# Create imbalanced training set\n",
    "x_new, y_new = [], []\n",
    "\n",
    "for cls in all_classes:\n",
    "    idx = np.where(y_train_flat == cls)[0]\n",
    "    if cls in low_classes:\n",
    "        selected_idx = idx[:500]\n",
    "    else:\n",
    "        selected_idx = idx\n",
    "    x_new.append(x_train[selected_idx])\n",
    "    y_new.append(y_train[selected_idx])\n",
    "\n",
    "# Combine and shuffle the imbalanced training data\n",
    "x_train = np.concatenate(x_new, axis=0)\n",
    "y_train = np.concatenate(y_new, axis=0)\n",
    "\n",
    "# Final shuffle\n",
    "final_idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[final_idx]\n",
    "y_train = y_train[final_idx]\n",
    "\n",
    "# Normalize and one-hot encode\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "#y_train = to_categorical(y_train, 10)\n",
    "#y_test = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "x_train, x_unlab, y_train, y_unlab = train_test_split( x_train, y_train , test_size=0.4, random_state=42 )\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.5, random_state=40 )\n",
    "\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))\n",
    "\n",
    "data_augmentation.layers[0].adapt(X_train)\n",
    "encoder = create_encoder()\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                        loss=SupervisedContrastiveLoss(temperature))\n",
    "    \n",
    "history = encoder_with_projection_head.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=10)\n",
    "classifier = create_classifier(encoder, trainable=False) \n",
    "history = classifier.fit(x=X_train, y=Y_train, batch_size=64, epochs=10) \n",
    "\n",
    "encoder, classifier,X_train,Y_train,x_unlab,y_unlab = train_active_learning_models(encoder, classifier,X_train,Y_train,x_unlab,y_unlab,x_test,y_test,num_iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b79688a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f225155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b63cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fcf42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:22:59.202783\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d419458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046e5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc99ab00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecede22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb15a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bc8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8c7370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9e2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89812f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae66eaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d97963b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bde3a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73b54d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b829e9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f93f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94e15861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [7],\n",
       "       [8],\n",
       "       ...,\n",
       "       [3],\n",
       "       [1],\n",
       "       [3]], dtype=uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3300b45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =np.argmax(Y_train)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e25618",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5784\\240059899.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8977779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7b4ad58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bcdd178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a9f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa8ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49471793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
