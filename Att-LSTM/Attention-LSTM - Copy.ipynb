{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acba090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ffa13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "del tr1,tr2,tr3\n",
    "\n",
    "trn1='D:/data/invasive-aquatic-species-data/noninvasive/*/'\n",
    "trn2='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Ostracod Image1/*/'\n",
    "trn3='D:/data/invasive-aquatic-species-data/invasive/*/'\n",
    "\n",
    "tr1= glob(trn1)\n",
    "tr2= glob(trn2)\n",
    "tr3= glob(trn3)\n",
    "\n",
    "trn11='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Not Veligers/*/'\n",
    "trn33='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Zebra Pediveliger Image1a/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trn11='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Not/*/'\n",
    "trn33='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Pedi-Zebra Veligers/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trn11='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Not/*/'\n",
    "trn33='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Preserved Zebra Ped 1a/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trnl1='D:/data/Ostracod/Ostracod Day 2 Image12 Short To Baylor/Ostracod Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl2='D:/data/Ostracod/Ostracods Day 2 Image1 To Baylor/Ostracods Day 2 Image1 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl3='D:/data/Ostracod/Ostracods Day 2 Image2 To Baylor/Ostracods Day 2 Image2 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl4='D:/data/Ostracod/Ostracods Day 2 Image3 To Baylor/Ostracods Day 2 Image3 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl5='D:/data/Ostracod/Ostracods Day 2 Image12 To Baylor/Ostracods Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl6='D:/data/Ostracod/Preserved Ostracods 1 To Baylor/Preserved Ostracods 1 To Baylor/Sorted Images/Preserve Ostracods/*/'\n",
    "trnl7='D:/data/Ostracod/Preserved Ostracods 1a To Baylor/Preserved Ostracods 1a To Baylor/Sorted Images/Preserved Ostracods 1a/*/'\n",
    "\n",
    "trl1= glob(trnl1)\n",
    "trl2= glob(trnl2)\n",
    "trl3= glob(trnl3)\n",
    "trl4= glob(trnl4)\n",
    "trl5= glob(trnl5)\n",
    "trl6= glob(trnl6)\n",
    "trl7= glob(trnl7)\n",
    "\n",
    "\n",
    "tr2.extend(trl1)\n",
    "tr2.extend(trl2)\n",
    "tr2.extend(trl3)\n",
    "tr2.extend(trl4)\n",
    "tr2.extend(trl5)\n",
    "tr2.extend(trl6)\n",
    "tr2.extend(trl7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf0822e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeab214c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8a504b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3717.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tr1= shuffle(tr1)\n",
    "tr2= shuffle(tr2)\n",
    "tr3= shuffle(tr3)\n",
    "\n",
    "tran_index_noninv = np.round( len(tr1)* .7  )\n",
    "tran_index_osc = np.round( len(tr2)* .8  )\n",
    "tran_index_inv = np.round( len(tr3)* .7  )\n",
    "tran_index_noninv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9c51980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[:(int) (tran_index_inv)]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[:(int) (tran_index_noninv)]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[:(int) (tran_index_osc)]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr3[:(int) (tran_index_inv)])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[:(int) (tran_index_noninv)])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])        \n",
    "\n",
    "for j in range(0,len(tr2[:(int) (tran_index_osc)])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k]) \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 28, 28)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),28,28,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "079c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4697, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "train_df= []\n",
    "breath = 5\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*5+k)\n",
    "        \n",
    "        deff.append(X_train[index])\n",
    "        \n",
    "    train_df.append(deff)\n",
    "\n",
    "Y_train = to_categorical(label)\n",
    "train_df = np.array(train_df)\n",
    "YY_Train = label\n",
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "104123f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24d262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[(int) (tran_index_inv) + 1 :]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[ (int)(tran_index_noninv) + 1:]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in tr2[ (int)(tran_index_osc) + 1:]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in range(0,len(tr3[(int) (tran_index_inv) + 1 :])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[ (int)(tran_index_noninv) + 1:])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  \n",
    "        \n",
    "for j in range(0,len(tr2[ (int)(tran_index_osc) + 1:])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 28, 28)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),28,28,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "661c2759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1996, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "test_df= []\n",
    "breath = 5\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*5 + k)\n",
    "        \n",
    "        deff.append(X_test[index])\n",
    "        \n",
    "    test_df.append(deff)\n",
    "    \n",
    "Y_test = to_categorical(label)\n",
    "test_df = np.array(test_df)\n",
    "YY_Test = label\n",
    "np.shape(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56c507ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52b5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = ( 6, 28, 28, 3 )\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 32\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae32102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88acaa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c674b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6123, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31947b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc77872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6, 2352)]    0           []                               \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 6, 32)        75488       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 6, 32)       64          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 32)       25184       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 6, 32)        8320        ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6, 32)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]',          \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 6, 32)       64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 32)        0           ['sequential_1[0][0]',           \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 32)       64          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 32)       25184       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 32)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]',                  \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 32)       64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6, 32)        0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 32)       64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 6, 32)       25184       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 6, 32)        0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]',                  \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 6, 32)       64          ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 6, 32)        0           ['sequential_3[0][0]',           \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 6, 32)       64          ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 6, 32)       25184       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 6, 32)        0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]',                  \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 6, 32)       64          ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 6, 32)        0           ['sequential_4[0][0]',           \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 6, 32)       64          ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 6, 32)       25184       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 6, 32)        0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]',                  \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 6, 32)       64          ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 6, 32)        0           ['sequential_5[0][0]',           \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 6, 32)       64          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 6, 32)       25184       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 6, 32)        0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]',                  \n",
      "                                                                  'lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 6, 32)       64          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 6, 32)        0           ['sequential_6[0][0]',           \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 6, 32)       64          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_12[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            66          ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 283,746\n",
      "Trainable params: 283,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c833aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_df = train_df.reshape( train_df.shape[0] , train_df.shape[1],( train_df.shape[2] * train_df.shape[3] * 3)  )\n",
    "#tt_df = test_df.reshape(test_df.shape[0] ,test_df.shape[1],( test_df.shape[2] * test_df.shape[3] * 3)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e83af007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_84197]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1388\\2042756921.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_84197]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3abe3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1103,    0],\n",
       "       [   6,  201]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786c9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852941176470589 0.9972801450589301 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87100053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6, 2352)]    0           []                               \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 6, 32)        75488       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 6, 32)       64          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 32)       8416        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6, 32)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 6, 32)       64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 32)        0           ['sequential_1[0][0]',           \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 32)       64          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 32)       8416        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 32)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 32)       64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6, 32)        0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 32)       64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_4[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            66          ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94,818\n",
      "Trainable params: 94,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tra_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14576\\1200473485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tra_df' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7ea818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995049504950495 0.999096657633243 0.9901477832512315\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd475d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 6, 32)        75488       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 6, 32)        64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 6, 32)        50336       layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 32)        0           multi_head_attention[0][0]       \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 6, 32)        64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 6, 32)        1056        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 32)        0           sequential_13[0][0]              \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 6, 32)        64          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 6, 32)        50336       layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 32)        0           multi_head_attention_1[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 6, 32)        64          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 6, 32)        1056        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 32)        0           sequential_14[0][0]              \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 6, 32)        64          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 6, 32)        50336       layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 32)        0           multi_head_attention_2[0][0]     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 6, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 6, 32)        1056        layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 6, 32)        0           sequential_15[0][0]              \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 6, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 6, 32)        50336       layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 6, 32)        0           multi_head_attention_3[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 6, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)      (None, 6, 32)        1056        layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 6, 32)        0           sequential_16[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 6, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 6, 32)        50336       layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 6, 32)        0           multi_head_attention_4[0][0]     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 6, 32)        64          add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 6, 32)        1056        layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 6, 32)        0           sequential_17[0][0]              \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 6, 32)        64          add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 6, 32)        50336       layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 6, 32)        0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 6, 32)        64          add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 6, 32)        1056        layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 6, 32)        0           sequential_18[0][0]              \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 6, 32)        64          add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 6, 32)        50336       layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 6, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 6, 32)        64          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 6, 32)        1056        layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 32)        0           sequential_19[0][0]              \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 6, 32)        64          add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 6, 32)        50336       layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 6, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 6, 32)        64          add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 6, 32)        1056        layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 6, 32)        0           sequential_20[0][0]              \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 6, 32)        64          add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_8 (MultiHe (None, 6, 32)        50336       layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 6, 32)        0           multi_head_attention_8[0][0]     \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 6, 32)        64          add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 6, 32)        1056        layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 6, 32)        0           sequential_21[0][0]              \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 6, 32)        64          add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_9 (MultiHe (None, 6, 32)        50336       layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 6, 32)        0           multi_head_attention_9[0][0]     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 6, 32)        64          add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 6, 32)        1056        layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 6, 32)        0           sequential_22[0][0]              \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 6, 32)        64          add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 6, 32)        50336       layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 6, 32)        0           multi_head_attention_10[0][0]    \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 6, 32)        64          add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 6, 32)        1056        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 6, 32)        0           sequential_23[0][0]              \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 6, 32)        64          add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 6, 32)        50336       layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 6, 32)        0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 6, 32)        64          add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_24 (Sequential)      (None, 6, 32)        1056        layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 6, 32)        0           sequential_24[0][0]              \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 6, 32)        64          add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 693,858\n",
      "Trainable params: 693,858\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 23s 103ms/step - loss: 0.5219 - accuracy: 0.7946 - val_loss: 0.2336 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.3663 - accuracy: 0.8432 - val_loss: 0.4138 - val_accuracy: 0.8206\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.2030 - accuracy: 0.9151 - val_loss: 0.2323 - val_accuracy: 0.8842\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.1755 - accuracy: 0.9294 - val_loss: 0.2131 - val_accuracy: 0.9086\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1551 - accuracy: 0.9326 - val_loss: 0.1142 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1516 - accuracy: 0.9347 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1491 - accuracy: 0.9379 - val_loss: 0.1124 - val_accuracy: 0.9494\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1667 - accuracy: 0.9343 - val_loss: 0.1455 - val_accuracy: 0.9347\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.1355 - accuracy: 0.9461 - val_loss: 0.1634 - val_accuracy: 0.9233\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1162 - accuracy: 0.9559 - val_loss: 0.0763 - val_accuracy: 0.9674\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1389 - accuracy: 0.9539 - val_loss: 0.0910 - val_accuracy: 0.9608\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.1209 - accuracy: 0.9522 - val_loss: 0.2704 - val_accuracy: 0.8989\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1106 - accuracy: 0.9575 - val_loss: 0.0810 - val_accuracy: 0.9608\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1089 - accuracy: 0.9563 - val_loss: 0.1549 - val_accuracy: 0.9299\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.1170 - accuracy: 0.9563 - val_loss: 0.0509 - val_accuracy: 0.9788\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.1228 - accuracy: 0.9514 - val_loss: 0.1363 - val_accuracy: 0.9396\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1019 - accuracy: 0.9604 - val_loss: 0.0798 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0915 - accuracy: 0.9653 - val_loss: 0.1441 - val_accuracy: 0.9413\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1009 - accuracy: 0.9608 - val_loss: 0.2379 - val_accuracy: 0.9233\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0921 - accuracy: 0.9661 - val_loss: 0.1586 - val_accuracy: 0.9478\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0931 - accuracy: 0.9653 - val_loss: 0.2354 - val_accuracy: 0.9038\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0979 - accuracy: 0.9600 - val_loss: 0.0736 - val_accuracy: 0.9674\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0867 - accuracy: 0.9673 - val_loss: 0.1006 - val_accuracy: 0.9560\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0817 - accuracy: 0.9714 - val_loss: 0.0889 - val_accuracy: 0.9706\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.1131 - val_accuracy: 0.9527\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0778 - accuracy: 0.9755 - val_loss: 0.1503 - val_accuracy: 0.9315\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0736 - accuracy: 0.9743 - val_loss: 0.3570 - val_accuracy: 0.8940\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0810 - accuracy: 0.9641 - val_loss: 0.3989 - val_accuracy: 0.8499\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0914 - accuracy: 0.9628 - val_loss: 0.1788 - val_accuracy: 0.9315\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.1003 - val_accuracy: 0.9674\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0675 - accuracy: 0.9722 - val_loss: 0.2169 - val_accuracy: 0.9282\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0621 - accuracy: 0.9739 - val_loss: 0.1146 - val_accuracy: 0.9511\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0536 - accuracy: 0.9767 - val_loss: 0.1284 - val_accuracy: 0.9543\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.1891 - val_accuracy: 0.9282\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0566 - accuracy: 0.9763 - val_loss: 0.1903 - val_accuracy: 0.9331\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0715 - accuracy: 0.9722 - val_loss: 0.3347 - val_accuracy: 0.9462\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0535 - accuracy: 0.9816 - val_loss: 0.1838 - val_accuracy: 0.9462\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0659 - accuracy: 0.9755 - val_loss: 0.2889 - val_accuracy: 0.8972\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0868 - accuracy: 0.9661 - val_loss: 0.1128 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0629 - accuracy: 0.9759 - val_loss: 0.2529 - val_accuracy: 0.9413\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0442 - accuracy: 0.9829 - val_loss: 0.0514 - val_accuracy: 0.9821\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0566 - accuracy: 0.9755 - val_loss: 0.2314 - val_accuracy: 0.9413\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0364 - accuracy: 0.9878 - val_loss: 0.2113 - val_accuracy: 0.9315\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0368 - accuracy: 0.9861 - val_loss: 0.3788 - val_accuracy: 0.8874\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.1981 - val_accuracy: 0.9347\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 0.0533 - accuracy: 0.9792 - val_loss: 0.1110 - val_accuracy: 0.9641\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0373 - accuracy: 0.9861 - val_loss: 0.1416 - val_accuracy: 0.9576\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0775 - accuracy: 0.9710 - val_loss: 0.1295 - val_accuracy: 0.9511\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0339 - accuracy: 0.9853 - val_loss: 0.2037 - val_accuracy: 0.9478\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0259 - accuracy: 0.9890 - val_loss: 0.1323 - val_accuracy: 0.9625\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0455 - accuracy: 0.9849 - val_loss: 0.0739 - val_accuracy: 0.9723\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0415 - accuracy: 0.9833 - val_loss: 0.1021 - val_accuracy: 0.9608\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0333 - accuracy: 0.9857 - val_loss: 0.1161 - val_accuracy: 0.9560\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0295 - accuracy: 0.9878 - val_loss: 0.1552 - val_accuracy: 0.9560\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.2156 - val_accuracy: 0.9608\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0389 - accuracy: 0.9849 - val_loss: 0.2261 - val_accuracy: 0.9527\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0262 - accuracy: 0.9894 - val_loss: 0.3019 - val_accuracy: 0.9364\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0304 - accuracy: 0.9853 - val_loss: 0.2947 - val_accuracy: 0.9299\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.3137 - val_accuracy: 0.9396\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0398 - accuracy: 0.9849 - val_loss: 0.1475 - val_accuracy: 0.9576\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.1855 - val_accuracy: 0.9445\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0304 - accuracy: 0.9869 - val_loss: 0.1199 - val_accuracy: 0.9706\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0410 - accuracy: 0.9837 - val_loss: 0.2752 - val_accuracy: 0.9445\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.2091 - val_accuracy: 0.9608\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.2158 - val_accuracy: 0.9592\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0369 - accuracy: 0.9878 - val_loss: 0.2110 - val_accuracy: 0.9462\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1017 - accuracy: 0.9641 - val_loss: 0.1465 - val_accuracy: 0.9560\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0837 - accuracy: 0.9690 - val_loss: 0.1615 - val_accuracy: 0.9560\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0396 - accuracy: 0.9873 - val_loss: 0.1601 - val_accuracy: 0.9560\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.1442 - val_accuracy: 0.9592\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0739 - accuracy: 0.9718 - val_loss: 0.1142 - val_accuracy: 0.9723\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0166 - accuracy: 0.9931 - val_loss: 0.1481 - val_accuracy: 0.9657\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0219 - accuracy: 0.9914 - val_loss: 0.1104 - val_accuracy: 0.9739\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.2892 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.1324 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0399 - accuracy: 0.9857 - val_loss: 0.2416 - val_accuracy: 0.9266\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 0.1394 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.1475 - val_accuracy: 0.9690\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.1367 - val_accuracy: 0.9739\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.2944 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.4960 - val_accuracy: 0.9184\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 7s 84ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0156 - accuracy: 0.9943 - val_loss: 0.1937 - val_accuracy: 0.9592\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.2368 - val_accuracy: 0.9413\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.1489 - val_accuracy: 0.9641\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0144 - accuracy: 0.9943 - val_loss: 0.1541 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0129 - accuracy: 0.9939 - val_loss: 0.2583 - val_accuracy: 0.9576\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.1197 - val_accuracy: 0.9739\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2260 - val_accuracy: 0.9608\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 6.6339e-04 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 6s 85ms/step - loss: 2.4115e-04 - accuracy: 1.0000 - val_loss: 0.2826 - val_accuracy: 0.9576\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 2.8148e-04 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9657\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 2.0978e-04 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9511\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.1025 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.2442 - val_accuracy: 0.9576\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0086 - accuracy: 0.9963 - val_loss: 0.3115 - val_accuracy: 0.9429\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 0.9608\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.2760 - val_accuracy: 0.9625\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0537 - accuracy: 0.9792 - val_loss: 0.1679 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263c237ab50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(12):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output =layers.MultiHeadAttention (  num_heads=12, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c6c53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771 0.9863415555924266 0.98989898989899\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a6163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26057200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5, 28, 28,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " patch_encoder_3 (PatchEncoder)  (None, 5, 32)       11064       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 5, 32)       64          ['patch_encoder_3[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 5, 32)       25184       ['layer_normalization_39[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 5, 32)        0           ['multi_head_attention_18[0][0]',\n",
      "                                                                  'patch_encoder_3[0][0]',        \n",
      "                                                                  'lstm_18[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_40 (LayerN  (None, 5, 32)       64          ['add_36[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_22 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 5, 32)        0           ['sequential_22[0][0]',          \n",
      "                                                                  'add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_41 (LayerN  (None, 5, 32)       64          ['add_37[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 5, 32)       25184       ['layer_normalization_41[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_19 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 5, 32)        0           ['multi_head_attention_19[0][0]',\n",
      "                                                                  'add_37[0][0]',                 \n",
      "                                                                  'lstm_19[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_42 (LayerN  (None, 5, 32)       64          ['add_38[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_23 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 5, 32)        0           ['sequential_23[0][0]',          \n",
      "                                                                  'add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_43 (LayerN  (None, 5, 32)       64          ['add_39[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 5, 32)       25184       ['layer_normalization_43[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_20 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 5, 32)        0           ['multi_head_attention_20[0][0]',\n",
      "                                                                  'add_39[0][0]',                 \n",
      "                                                                  'lstm_20[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_44 (LayerN  (None, 5, 32)       64          ['add_40[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_24 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 5, 32)        0           ['sequential_24[0][0]',          \n",
      "                                                                  'add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_45 (LayerN  (None, 5, 32)       64          ['add_41[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_21 (Multi  (None, 5, 32)       25184       ['layer_normalization_45[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_21 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 5, 32)        0           ['multi_head_attention_21[0][0]',\n",
      "                                                                  'add_41[0][0]',                 \n",
      "                                                                  'lstm_21[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_46 (LayerN  (None, 5, 32)       64          ['add_42[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_25 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 5, 32)        0           ['sequential_25[0][0]',          \n",
      "                                                                  'add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_47 (LayerN  (None, 5, 32)       64          ['add_43[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (Multi  (None, 5, 32)       25184       ['layer_normalization_47[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_22 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 5, 32)        0           ['multi_head_attention_22[0][0]',\n",
      "                                                                  'add_43[0][0]',                 \n",
      "                                                                  'lstm_22[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_48 (LayerN  (None, 5, 32)       64          ['add_44[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_26 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, 5, 32)        0           ['sequential_26[0][0]',          \n",
      "                                                                  'add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_49 (LayerN  (None, 5, 32)       64          ['add_45[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_23 (Multi  (None, 5, 32)       25184       ['layer_normalization_49[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_23 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, 5, 32)        0           ['multi_head_attention_23[0][0]',\n",
      "                                                                  'add_45[0][0]',                 \n",
      "                                                                  'lstm_23[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_50 (LayerN  (None, 5, 32)       64          ['add_46[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 5, 32)        0           ['sequential_27[0][0]',          \n",
      "                                                                  'add_46[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_51 (LayerN  (None, 5, 32)       64          ['add_47[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 32)          0           ['layer_normalization_51[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3)            99          ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 219,355\n",
      "Trainable params: 219,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (5,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(5, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=3, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2676f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "118/118 [==============================] - 105s 300ms/step - loss: 0.1104 - accuracy: 0.9697 - val_loss: 1.7220 - val_accuracy: 0.8883\n",
      "Epoch 2/100\n",
      "118/118 [==============================] - 24s 207ms/step - loss: 0.0655 - accuracy: 0.9811 - val_loss: 1.6530 - val_accuracy: 0.8947\n",
      "Epoch 3/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0499 - accuracy: 0.9864 - val_loss: 1.6303 - val_accuracy: 0.9021\n",
      "Epoch 4/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 0.0464 - accuracy: 0.9872 - val_loss: 1.7050 - val_accuracy: 0.8809\n",
      "Epoch 5/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 0.0356 - accuracy: 0.9888 - val_loss: 1.7440 - val_accuracy: 0.8947\n",
      "Epoch 6/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 0.0416 - accuracy: 0.9846 - val_loss: 1.7676 - val_accuracy: 0.8830\n",
      "Epoch 7/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0365 - accuracy: 0.9886 - val_loss: 1.8028 - val_accuracy: 0.8723\n",
      "Epoch 8/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 0.0349 - accuracy: 0.9888 - val_loss: 1.7614 - val_accuracy: 0.8426\n",
      "Epoch 9/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0271 - accuracy: 0.9915 - val_loss: 1.7325 - val_accuracy: 0.8968\n",
      "Epoch 10/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0217 - accuracy: 0.9928 - val_loss: 1.8297 - val_accuracy: 0.8840\n",
      "Epoch 11/100\n",
      "118/118 [==============================] - 24s 204ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 1.8857 - val_accuracy: 0.8957\n",
      "Epoch 12/100\n",
      "118/118 [==============================] - 25s 211ms/step - loss: 0.0306 - accuracy: 0.9904 - val_loss: 1.9357 - val_accuracy: 0.8713\n",
      "Epoch 13/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0250 - accuracy: 0.9920 - val_loss: 1.8868 - val_accuracy: 0.8745\n",
      "Epoch 14/100\n",
      "118/118 [==============================] - 24s 202ms/step - loss: 0.0236 - accuracy: 0.9925 - val_loss: 1.8253 - val_accuracy: 0.8872\n",
      "Epoch 15/100\n",
      "118/118 [==============================] - 24s 200ms/step - loss: 0.0229 - accuracy: 0.9912 - val_loss: 1.8242 - val_accuracy: 0.8872\n",
      "Epoch 16/100\n",
      "118/118 [==============================] - 25s 209ms/step - loss: 0.0297 - accuracy: 0.9896 - val_loss: 1.8131 - val_accuracy: 0.8989\n",
      "Epoch 17/100\n",
      "118/118 [==============================] - 25s 212ms/step - loss: 0.0047 - accuracy: 0.9992 - val_loss: 1.7446 - val_accuracy: 0.8957\n",
      "Epoch 18/100\n",
      "118/118 [==============================] - 25s 209ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 1.7133 - val_accuracy: 0.8979\n",
      "Epoch 19/100\n",
      "118/118 [==============================] - 25s 214ms/step - loss: 0.0122 - accuracy: 0.9965 - val_loss: 1.8806 - val_accuracy: 0.9021\n",
      "Epoch 20/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0055 - accuracy: 0.9992 - val_loss: 1.8195 - val_accuracy: 0.8947\n",
      "Epoch 21/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 1.7151 - val_accuracy: 0.8936\n",
      "Epoch 22/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0411 - accuracy: 0.9875 - val_loss: 1.8486 - val_accuracy: 0.8883\n",
      "Epoch 23/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0137 - accuracy: 0.9957 - val_loss: 1.8183 - val_accuracy: 0.8883\n",
      "Epoch 24/100\n",
      "118/118 [==============================] - 24s 204ms/step - loss: 0.0136 - accuracy: 0.9963 - val_loss: 1.7707 - val_accuracy: 0.8883\n",
      "Epoch 25/100\n",
      "118/118 [==============================] - 24s 200ms/step - loss: 0.0112 - accuracy: 0.9976 - val_loss: 1.7760 - val_accuracy: 0.8883\n",
      "Epoch 26/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 1.9006 - val_accuracy: 0.8755\n",
      "Epoch 27/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0186 - accuracy: 0.9931 - val_loss: 1.8337 - val_accuracy: 0.8851\n",
      "Epoch 28/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 1.8110 - val_accuracy: 0.8915\n",
      "Epoch 29/100\n",
      "118/118 [==============================] - 24s 202ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 1.7249 - val_accuracy: 0.8862\n",
      "Epoch 30/100\n",
      "118/118 [==============================] - 23s 198ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 1.6895 - val_accuracy: 0.8883\n",
      "Epoch 31/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0312 - accuracy: 0.9894 - val_loss: 1.7832 - val_accuracy: 0.8681\n",
      "Epoch 32/100\n",
      "118/118 [==============================] - 24s 204ms/step - loss: 0.0094 - accuracy: 0.9955 - val_loss: 1.8178 - val_accuracy: 0.8872\n",
      "Epoch 33/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 1.8621 - val_accuracy: 0.8894\n",
      "Epoch 34/100\n",
      "118/118 [==============================] - 27s 232ms/step - loss: 0.0223 - accuracy: 0.9928 - val_loss: 1.7297 - val_accuracy: 0.8915\n",
      "Epoch 35/100\n",
      "118/118 [==============================] - 25s 212ms/step - loss: 0.0255 - accuracy: 0.9904 - val_loss: 1.7342 - val_accuracy: 0.8851\n",
      "Epoch 36/100\n",
      "118/118 [==============================] - 24s 203ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 1.7847 - val_accuracy: 0.8894\n",
      "Epoch 37/100\n",
      "118/118 [==============================] - 24s 205ms/step - loss: 0.0123 - accuracy: 0.9960 - val_loss: 1.8119 - val_accuracy: 0.8936\n",
      "Epoch 38/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 1.8003 - val_accuracy: 0.8872\n",
      "Epoch 39/100\n",
      "118/118 [==============================] - 26s 222ms/step - loss: 0.0119 - accuracy: 0.9955 - val_loss: 1.7847 - val_accuracy: 0.8957\n",
      "Epoch 40/100\n",
      "118/118 [==============================] - 25s 210ms/step - loss: 0.0164 - accuracy: 0.9955 - val_loss: 1.8698 - val_accuracy: 0.8702\n",
      "Epoch 41/100\n",
      "118/118 [==============================] - 24s 204ms/step - loss: 0.0052 - accuracy: 0.9981 - val_loss: 1.7689 - val_accuracy: 0.8936\n",
      "Epoch 42/100\n",
      "118/118 [==============================] - 23s 199ms/step - loss: 4.2272e-04 - accuracy: 1.0000 - val_loss: 1.7819 - val_accuracy: 0.8862\n",
      "Epoch 43/100\n",
      "118/118 [==============================] - 25s 208ms/step - loss: 4.7901e-04 - accuracy: 1.0000 - val_loss: 1.7945 - val_accuracy: 0.8830\n",
      "Epoch 44/100\n",
      "118/118 [==============================] - 25s 215ms/step - loss: 2.2709e-04 - accuracy: 1.0000 - val_loss: 1.7603 - val_accuracy: 0.8883\n",
      "Epoch 45/100\n",
      "118/118 [==============================] - 24s 206ms/step - loss: 1.4205e-04 - accuracy: 1.0000 - val_loss: 1.7660 - val_accuracy: 0.8904\n",
      "Epoch 46/100\n",
      "118/118 [==============================] - 24s 204ms/step - loss: 1.7529e-04 - accuracy: 1.0000 - val_loss: 1.7589 - val_accuracy: 0.8936\n",
      "Epoch 47/100\n",
      "118/118 [==============================] - 25s 209ms/step - loss: 9.1584e-05 - accuracy: 1.0000 - val_loss: 1.7839 - val_accuracy: 0.8894\n",
      "Epoch 48/100\n",
      "118/118 [==============================] - 25s 214ms/step - loss: 7.6730e-05 - accuracy: 1.0000 - val_loss: 1.8108 - val_accuracy: 0.8894\n",
      "Epoch 49/100\n",
      "118/118 [==============================] - 28s 237ms/step - loss: 4.9431e-05 - accuracy: 1.0000 - val_loss: 1.8014 - val_accuracy: 0.8904\n",
      "Epoch 50/100\n",
      "118/118 [==============================] - 26s 221ms/step - loss: 9.4997e-05 - accuracy: 1.0000 - val_loss: 1.8341 - val_accuracy: 0.8862\n",
      "Epoch 51/100\n",
      "118/118 [==============================] - 26s 220ms/step - loss: 5.7095e-05 - accuracy: 1.0000 - val_loss: 1.7917 - val_accuracy: 0.8926\n",
      "Epoch 52/100\n",
      "118/118 [==============================] - 27s 230ms/step - loss: 5.4737e-05 - accuracy: 1.0000 - val_loss: 1.8370 - val_accuracy: 0.8872\n",
      "Epoch 53/100\n",
      "118/118 [==============================] - 26s 219ms/step - loss: 0.0389 - accuracy: 0.9883 - val_loss: 1.8797 - val_accuracy: 0.8862\n",
      "Epoch 54/100\n",
      "118/118 [==============================] - 26s 220ms/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 1.8710 - val_accuracy: 0.8723\n",
      "Epoch 55/100\n",
      "118/118 [==============================] - 27s 227ms/step - loss: 0.0076 - accuracy: 0.9973 - val_loss: 1.7840 - val_accuracy: 0.8894\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 32s 270ms/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 1.8818 - val_accuracy: 0.8691\n",
      "Epoch 57/100\n",
      "118/118 [==============================] - 28s 238ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 1.9645 - val_accuracy: 0.8617\n",
      "Epoch 58/100\n",
      "118/118 [==============================] - 25s 215ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 1.7712 - val_accuracy: 0.8819\n",
      "Epoch 59/100\n",
      "118/118 [==============================] - 26s 222ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 1.8510 - val_accuracy: 0.8266\n",
      "Epoch 60/100\n",
      "118/118 [==============================] - 26s 220ms/step - loss: 0.0371 - accuracy: 0.9872 - val_loss: 1.6405 - val_accuracy: 0.8957\n",
      "Epoch 61/100\n",
      "118/118 [==============================] - 30s 257ms/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 1.7100 - val_accuracy: 0.8862\n",
      "Epoch 62/100\n",
      "118/118 [==============================] - 27s 231ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 1.8469 - val_accuracy: 0.8606\n",
      "Epoch 63/100\n",
      "118/118 [==============================] - 26s 217ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 1.6214 - val_accuracy: 0.9043\n",
      "Epoch 64/100\n",
      "118/118 [==============================] - 25s 216ms/step - loss: 0.0260 - accuracy: 0.9912 - val_loss: 1.6844 - val_accuracy: 0.8777\n",
      "Epoch 65/100\n",
      "118/118 [==============================] - 26s 218ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 1.6746 - val_accuracy: 0.8872\n",
      "Epoch 66/100\n",
      "118/118 [==============================] - 26s 223ms/step - loss: 5.2431e-04 - accuracy: 0.9997 - val_loss: 1.6853 - val_accuracy: 0.8745\n",
      "Epoch 67/100\n",
      "118/118 [==============================] - 26s 220ms/step - loss: 1.9582e-04 - accuracy: 1.0000 - val_loss: 1.6422 - val_accuracy: 0.8819\n",
      "Epoch 68/100\n",
      "118/118 [==============================] - 25s 216ms/step - loss: 1.6272e-04 - accuracy: 1.0000 - val_loss: 1.6757 - val_accuracy: 0.8840\n",
      "Epoch 69/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 0.0276 - accuracy: 0.9917 - val_loss: 1.7729 - val_accuracy: 0.8926: 0.0240 - accura -\n",
      "Epoch 70/100\n",
      "118/118 [==============================] - 26s 222ms/step - loss: 0.0138 - accuracy: 0.9947 - val_loss: 1.8567 - val_accuracy: 0.8734\n",
      "Epoch 71/100\n",
      "118/118 [==============================] - 25s 215ms/step - loss: 0.0132 - accuracy: 0.9952 - val_loss: 1.5887 - val_accuracy: 0.8883\n",
      "Epoch 72/100\n",
      "118/118 [==============================] - 26s 217ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.5753 - val_accuracy: 0.8926\n",
      "Epoch 73/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 9.2889e-04 - accuracy: 0.9997 - val_loss: 1.6077 - val_accuracy: 0.8904\n",
      "Epoch 74/100\n",
      "118/118 [==============================] - 26s 220ms/step - loss: 0.0089 - accuracy: 0.9965 - val_loss: 1.8500 - val_accuracy: 0.8713\n",
      "Epoch 75/100\n",
      "118/118 [==============================] - 25s 212ms/step - loss: 0.0168 - accuracy: 0.9933 - val_loss: 1.7904 - val_accuracy: 0.8915\n",
      "Epoch 76/100\n",
      "118/118 [==============================] - 25s 209ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 1.9332 - val_accuracy: 0.8511\n",
      "Epoch 77/100\n",
      "118/118 [==============================] - 25s 210ms/step - loss: 0.0226 - accuracy: 0.9947 - val_loss: 1.8074 - val_accuracy: 0.8745\n",
      "Epoch 78/100\n",
      "118/118 [==============================] - 25s 216ms/step - loss: 0.0028 - accuracy: 0.9995 - val_loss: 1.8235 - val_accuracy: 0.8723\n",
      "Epoch 79/100\n",
      "118/118 [==============================] - 26s 222ms/step - loss: 3.7158e-04 - accuracy: 1.0000 - val_loss: 1.7502 - val_accuracy: 0.8840\n",
      "Epoch 80/100\n",
      "118/118 [==============================] - 26s 221ms/step - loss: 2.1180e-04 - accuracy: 1.0000 - val_loss: 1.7394 - val_accuracy: 0.8883\n",
      "Epoch 81/100\n",
      "118/118 [==============================] - 26s 217ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 1.7684 - val_accuracy: 0.8851\n",
      "Epoch 82/100\n",
      "118/118 [==============================] - 25s 215ms/step - loss: 0.0166 - accuracy: 0.9939 - val_loss: 1.8678 - val_accuracy: 0.8798\n",
      "Epoch 83/100\n",
      "118/118 [==============================] - 25s 214ms/step - loss: 0.0221 - accuracy: 0.9917 - val_loss: 1.7222 - val_accuracy: 0.8883\n",
      "Epoch 84/100\n",
      "118/118 [==============================] - 25s 214ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 1.7989 - val_accuracy: 0.8872\n",
      "Epoch 85/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 1.7031 - val_accuracy: 0.8968\n",
      "Epoch 86/100\n",
      "118/118 [==============================] - 25s 212ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 1.7186 - val_accuracy: 0.8830\n",
      "Epoch 87/100\n",
      "118/118 [==============================] - 26s 223ms/step - loss: 4.5099e-04 - accuracy: 1.0000 - val_loss: 1.7006 - val_accuracy: 0.8979\n",
      "Epoch 88/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 6.0515e-04 - accuracy: 1.0000 - val_loss: 1.7259 - val_accuracy: 0.8915\n",
      "Epoch 89/100\n",
      "118/118 [==============================] - 32s 274ms/step - loss: 0.0096 - accuracy: 0.9979 - val_loss: 1.7142 - val_accuracy: 0.8691\n",
      "Epoch 90/100\n",
      "118/118 [==============================] - 32s 271ms/step - loss: 0.0252 - accuracy: 0.9904 - val_loss: 1.5085 - val_accuracy: 0.8883\n",
      "Epoch 91/100\n",
      "118/118 [==============================] - 29s 248ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 1.6103 - val_accuracy: 0.8840\n",
      "Epoch 92/100\n",
      "118/118 [==============================] - 34s 291ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 1.6135 - val_accuracy: 0.8830\n",
      "Epoch 93/100\n",
      "118/118 [==============================] - 28s 239ms/step - loss: 2.3967e-04 - accuracy: 1.0000 - val_loss: 1.5722 - val_accuracy: 0.8904\n",
      "Epoch 94/100\n",
      "118/118 [==============================] - 27s 229ms/step - loss: 0.0112 - accuracy: 0.9965 - val_loss: 1.5868 - val_accuracy: 0.9000\n",
      "Epoch 95/100\n",
      "118/118 [==============================] - 28s 239ms/step - loss: 0.0201 - accuracy: 0.9936 - val_loss: 1.5882 - val_accuracy: 0.8691\n",
      "Epoch 96/100\n",
      "118/118 [==============================] - 26s 222ms/step - loss: 0.0031 - accuracy: 0.9995 - val_loss: 1.6217 - val_accuracy: 0.8798\n",
      "Epoch 97/100\n",
      "118/118 [==============================] - 25s 214ms/step - loss: 5.3345e-04 - accuracy: 1.0000 - val_loss: 1.5961 - val_accuracy: 0.8851\n",
      "Epoch 98/100\n",
      "118/118 [==============================] - 25s 213ms/step - loss: 2.1675e-04 - accuracy: 1.0000 - val_loss: 1.5954 - val_accuracy: 0.8872\n",
      "Epoch 99/100\n",
      "118/118 [==============================] - 25s 215ms/step - loss: 1.7112e-04 - accuracy: 1.0000 - val_loss: 1.6316 - val_accuracy: 0.8830\n",
      "Epoch 100/100\n",
      "118/118 [==============================] - 26s 219ms/step - loss: 1.2456e-04 - accuracy: 1.0000 - val_loss: 1.6281 - val_accuracy: 0.8883\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "history = model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94e20c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef7bac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.00%\n",
      "Balanced accuracy: 66.67%\n",
      "F1 score for class 1: 99.87%\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, p)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(Y_test, p)\n",
    "print(\"Balanced accuracy: {:.2f}%\".format(balanced_accuracy * 100))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for class 1\n",
    "f1_class1 = f1_score(Y_test, p, labels=[1], average='macro')\n",
    "print(\"F1 score for class 1: {:.2f}%\".format(f1_class1 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b4d89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(Y_test, p)\n",
    "\n",
    "# Create a heatmap of the confusion matrix using seaborn\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c36be08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514281067975519"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3bb2f5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy over time\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3afd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cf3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( train_df,YY_Train , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cc7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(X_train , y_train , \"train\")\n",
    "validloader = prepare_dataloader(X_val, y_val, \"valid\")\n",
    "testloader = prepare_dataloader(test_df,YY_Test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0523831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6b82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28803bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tubelet_embedding (TubeletEmbed (None, 9, 32)        24608       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoder (PositionalE (None, 9, 32)        288         tubelet_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 9, 32)        64          positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 9, 32)        4224        layer_normalization_15[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 9, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 9, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 9, 32)        8352        layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 9, 32)        0           sequential_9[0][0]               \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 9, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 9, 32)        4224        layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 9, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 9, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 9, 32)        8352        layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 9, 32)        0           sequential_10[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 9, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            66          global_average_pooling1d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 50,434\n",
      "Trainable params: 50,434\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "PROJECTION_DIM = 32\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf326e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa5b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 20s 78ms/step - loss: 0.3926 - accuracy: 0.8436 - top-5-accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.8254 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.2927 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.8418 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2794 - accuracy: 0.8653 - top-5-accuracy: 1.0000 - val_loss: 0.2698 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2298 - accuracy: 0.8881 - top-5-accuracy: 1.0000 - val_loss: 0.2940 - val_accuracy: 0.8499 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.2393 - accuracy: 0.8877 - top-5-accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.8760 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.2201 - accuracy: 0.8918 - top-5-accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.2056 - accuracy: 0.9028 - top-5-accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.8711 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1891 - accuracy: 0.9110 - top-5-accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1907 - accuracy: 0.9138 - top-5-accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.8989 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1848 - accuracy: 0.9167 - top-5-accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1675 - accuracy: 0.9261 - top-5-accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1725 - accuracy: 0.9306 - top-5-accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1630 - accuracy: 0.9290 - top-5-accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1800 - accuracy: 0.9220 - top-5-accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1566 - accuracy: 0.9355 - top-5-accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1476 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1507 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1488 - accuracy: 0.9367 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1385 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1394 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1332 - accuracy: 0.9453 - top-5-accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1340 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1256 - accuracy: 0.9490 - top-5-accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1275 - accuracy: 0.9449 - top-5-accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1226 - accuracy: 0.9502 - top-5-accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1257 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1256 - accuracy: 0.9461 - top-5-accuracy: 1.0000 - val_loss: 0.1774 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1153 - accuracy: 0.9539 - top-5-accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1192 - accuracy: 0.9510 - top-5-accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1188 - accuracy: 0.9535 - top-5-accuracy: 1.0000 - val_loss: 0.1698 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1002 - accuracy: 0.9575 - top-5-accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0950 - accuracy: 0.9645 - top-5-accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1041 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1014 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0927 - accuracy: 0.9624 - top-5-accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0882 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0882 - accuracy: 0.9628 - top-5-accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0879 - accuracy: 0.9600 - top-5-accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0828 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0872 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0748 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0807 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0703 - accuracy: 0.9686 - top-5-accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0779 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.0776 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.0707 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0759 - accuracy: 0.9690 - top-5-accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0656 - accuracy: 0.9735 - top-5-accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0671 - accuracy: 0.9714 - top-5-accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0598 - accuracy: 0.9755 - top-5-accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9527 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0708 - accuracy: 0.9698 - top-5-accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.0569 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0569 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0515 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.1930 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0562 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1689 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0487 - accuracy: 0.9820 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0623 - accuracy: 0.9751 - top-5-accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0609 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0538 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0446 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0544 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0602 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0460 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0383 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0504 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0452 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0495 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0388 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.1932 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0435 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0436 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0387 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0437 - accuracy: 0.9824 - top-5-accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0354 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0331 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0396 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.2242 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0461 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0346 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0306 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0453 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0390 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0405 - accuracy: 0.9837 - top-5-accuracy: 1.0000 - val_loss: 0.2045 - val_accuracy: 0.9494 - val_top-5-accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0337 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0243 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0212 - accuracy: 0.9947 - top-5-accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0521 - accuracy: 0.9808 - top-5-accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0250 - accuracy: 0.9939 - top-5-accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.0295 - accuracy: 0.9890 - top-5-accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 5s 64ms/step - loss: 0.0298 - accuracy: 0.9882 - top-5-accuracy: 1.0000 - val_loss: 0.2464 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0415 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0401 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0265 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0365 - accuracy: 0.9869 - top-5-accuracy: 1.0000 - val_loss: 0.3200 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0349 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0632 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.2011 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0355 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.2601 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0273 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0286 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0280 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0741 - accuracy: 0.9794 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 97.94%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c6df61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1104,   22],\n",
       "       [   5,  179]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY_Test = YY_Test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = YY_Test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7d70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298701298701298 0.9362829932798964 0.9728260869565217\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3620fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398567119155354"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f75542c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "199466a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train_df[19]\n",
    "y = train_df[3]\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [x[0], x[1], x[2], x[3],y[0], y[1], y[2], y[3]]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3eb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027188ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea8a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 6, 28, 28, 64)     5248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 6, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 6, 14, 14, 128)    221312    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 3, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 3, 7, 7, 256)      884992    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 1, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 1, 3, 3, 512)      3539456   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 9,372,674\n",
      "Trainable params: 9,372,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3, 3, 3), activation=\"relu\",name=\"conv1\",   input_shape=(6,28,28,3), strides=(1, 1, 1), padding=\"same\"))  \n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
    "model.add(Conv3D(128, (3, 3, 3), activation=\"relu\",name=\"conv2\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
    "model.add(Conv3D(256, (3, 3, 3), activation=\"relu\",name=\"conv3a\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
    "model.add(Conv3D(512, (3, 3, 3), activation=\"relu\",name=\"conv4a\", strides=(1, 1, 1), padding=\"same\"))   \n",
    "\n",
    "model.add(Flatten())\n",
    "                     \n",
    "    # FC layers group\n",
    "model.add(Dense(1024, activation='relu', name='fc6'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax', name='fc8'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d1b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 357s 5s/step - loss: 0.3074 - accuracy: 0.8591 - val_loss: 0.1795 - val_accuracy: 0.9184\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 356s 5s/step - loss: 0.2036 - accuracy: 0.9224 - val_loss: 0.0635 - val_accuracy: 0.9723\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 338s 4s/step - loss: 0.1969 - accuracy: 0.9216 - val_loss: 0.0981 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 360s 5s/step - loss: 0.1657 - accuracy: 0.9306 - val_loss: 0.0700 - val_accuracy: 0.9674\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 358s 5s/step - loss: 0.1690 - accuracy: 0.9310 - val_loss: 0.0765 - val_accuracy: 0.9723\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 56s - loss: 0.1179 - accuracy: 0.9495 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19660/1501325532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73fd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_29 (TimeDis (None, 6, 26, 26, 2)      56        \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 6, 13, 13, 2)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 6, 11, 11, 4)      76        \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 6, 5, 5, 4)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 6, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 3,638\n",
      "Trainable params: 3,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 9s 63ms/step - loss: 0.5996 - accuracy: 0.7999 - val_loss: 0.1954 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.3871 - accuracy: 0.8109 - val_loss: 0.0698 - val_accuracy: 0.9951\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.2653 - accuracy: 0.8775 - val_loss: 0.1443 - val_accuracy: 0.9119\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2197 - accuracy: 0.9036 - val_loss: 0.1015 - val_accuracy: 0.9396\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 2s 22ms/step - loss: 0.2255 - accuracy: 0.9036 - val_loss: 0.0931 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1977 - accuracy: 0.9216 - val_loss: 0.1024 - val_accuracy: 0.9413\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1912 - accuracy: 0.9192 - val_loss: 0.1007 - val_accuracy: 0.9413\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1736 - accuracy: 0.9249 - val_loss: 0.0728 - val_accuracy: 0.9625\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1723 - accuracy: 0.9285 - val_loss: 0.0595 - val_accuracy: 0.9723\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1685 - accuracy: 0.9281 - val_loss: 0.0819 - val_accuracy: 0.9560\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1617 - accuracy: 0.9334 - val_loss: 0.0673 - val_accuracy: 0.9690\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1567 - accuracy: 0.9367 - val_loss: 0.0865 - val_accuracy: 0.9560\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1526 - accuracy: 0.9330 - val_loss: 0.1450 - val_accuracy: 0.9217\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1517 - accuracy: 0.9355 - val_loss: 0.0741 - val_accuracy: 0.9608\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1458 - accuracy: 0.9371 - val_loss: 0.0587 - val_accuracy: 0.9723\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1495 - accuracy: 0.9388 - val_loss: 0.1112 - val_accuracy: 0.9462\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1493 - accuracy: 0.9351 - val_loss: 0.0823 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1511 - accuracy: 0.9404 - val_loss: 0.0887 - val_accuracy: 0.9592\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1392 - accuracy: 0.9432 - val_loss: 0.0730 - val_accuracy: 0.9608\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1403 - accuracy: 0.9424 - val_loss: 0.0548 - val_accuracy: 0.9706\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1383 - accuracy: 0.9416 - val_loss: 0.0401 - val_accuracy: 0.9821\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1377 - accuracy: 0.9383 - val_loss: 0.0796 - val_accuracy: 0.9625\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1334 - accuracy: 0.9449 - val_loss: 0.0729 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1263 - accuracy: 0.9514 - val_loss: 0.0650 - val_accuracy: 0.9657\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1363 - accuracy: 0.9424 - val_loss: 0.0694 - val_accuracy: 0.9674\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1304 - accuracy: 0.9453 - val_loss: 0.0843 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1290 - accuracy: 0.9449 - val_loss: 0.0952 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1299 - accuracy: 0.9465 - val_loss: 0.0770 - val_accuracy: 0.9657\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1281 - accuracy: 0.9477 - val_loss: 0.0804 - val_accuracy: 0.9674\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 0.0688 - val_accuracy: 0.9690\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1204 - accuracy: 0.9526 - val_loss: 0.0862 - val_accuracy: 0.9641\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1332 - accuracy: 0.9449 - val_loss: 0.0644 - val_accuracy: 0.9690\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1205 - accuracy: 0.9457 - val_loss: 0.0761 - val_accuracy: 0.9674\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1197 - accuracy: 0.9473 - val_loss: 0.0930 - val_accuracy: 0.9641\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1345 - accuracy: 0.9388 - val_loss: 0.0513 - val_accuracy: 0.9772\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1221 - accuracy: 0.9469 - val_loss: 0.0652 - val_accuracy: 0.9690\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1235 - accuracy: 0.9473 - val_loss: 0.0506 - val_accuracy: 0.9755\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1137 - accuracy: 0.9518 - val_loss: 0.0591 - val_accuracy: 0.9706\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1185 - accuracy: 0.9494 - val_loss: 0.1011 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1152 - accuracy: 0.9526 - val_loss: 0.0479 - val_accuracy: 0.9772\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1166 - accuracy: 0.9522 - val_loss: 0.0808 - val_accuracy: 0.9674\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1106 - accuracy: 0.9530 - val_loss: 0.0765 - val_accuracy: 0.9657\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1124 - accuracy: 0.9555 - val_loss: 0.0914 - val_accuracy: 0.9641\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1097 - accuracy: 0.9526 - val_loss: 0.1018 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1180 - accuracy: 0.9477 - val_loss: 0.0502 - val_accuracy: 0.9755\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1083 - accuracy: 0.9514 - val_loss: 0.0792 - val_accuracy: 0.9657\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1104 - accuracy: 0.9522 - val_loss: 0.0804 - val_accuracy: 0.9657\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1078 - accuracy: 0.9530 - val_loss: 0.0793 - val_accuracy: 0.9674\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1058 - accuracy: 0.9543 - val_loss: 0.0647 - val_accuracy: 0.9706\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1088 - accuracy: 0.9506 - val_loss: 0.0543 - val_accuracy: 0.9723\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1102 - accuracy: 0.9526 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1052 - accuracy: 0.9604 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1075 - accuracy: 0.9530 - val_loss: 0.0544 - val_accuracy: 0.9723\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1023 - accuracy: 0.9588 - val_loss: 0.0930 - val_accuracy: 0.9657\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1092 - accuracy: 0.9584 - val_loss: 0.1430 - val_accuracy: 0.9429\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1212 - accuracy: 0.9469 - val_loss: 0.0491 - val_accuracy: 0.9788\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0981 - accuracy: 0.9559 - val_loss: 0.0854 - val_accuracy: 0.9657\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1032 - accuracy: 0.9518 - val_loss: 0.1032 - val_accuracy: 0.9576\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1055 - accuracy: 0.9547 - val_loss: 0.1238 - val_accuracy: 0.9560\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1008 - accuracy: 0.9596 - val_loss: 0.0652 - val_accuracy: 0.9706\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1004 - accuracy: 0.9575 - val_loss: 0.0691 - val_accuracy: 0.9674\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1131 - accuracy: 0.9539 - val_loss: 0.0863 - val_accuracy: 0.9657\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0976 - accuracy: 0.9608 - val_loss: 0.1033 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9600 - val_loss: 0.1076 - val_accuracy: 0.9641\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0962 - accuracy: 0.9592 - val_loss: 0.0723 - val_accuracy: 0.9674\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9559 - val_loss: 0.1495 - val_accuracy: 0.9494\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1030 - accuracy: 0.9551 - val_loss: 0.1109 - val_accuracy: 0.9641\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0991 - accuracy: 0.9559 - val_loss: 0.1028 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1012 - accuracy: 0.9588 - val_loss: 0.0904 - val_accuracy: 0.9657\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1014 - accuracy: 0.9551 - val_loss: 0.1130 - val_accuracy: 0.9576\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1081 - accuracy: 0.9539 - val_loss: 0.1036 - val_accuracy: 0.9641\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0987 - accuracy: 0.9579 - val_loss: 0.0856 - val_accuracy: 0.9641\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.1028 - val_accuracy: 0.9576\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0906 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9641\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0969 - accuracy: 0.9547 - val_loss: 0.0551 - val_accuracy: 0.9804\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0920 - accuracy: 0.9592 - val_loss: 0.1260 - val_accuracy: 0.9543\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0985 - accuracy: 0.9563 - val_loss: 0.0661 - val_accuracy: 0.9690\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0886 - accuracy: 0.9608 - val_loss: 0.0898 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0961 - accuracy: 0.9592 - val_loss: 0.1046 - val_accuracy: 0.9625\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0889 - accuracy: 0.9612 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0872 - accuracy: 0.9624 - val_loss: 0.0987 - val_accuracy: 0.9608\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0914 - accuracy: 0.9608 - val_loss: 0.0799 - val_accuracy: 0.9674\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0948 - accuracy: 0.9596 - val_loss: 0.1392 - val_accuracy: 0.9429\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9637 - val_loss: 0.0783 - val_accuracy: 0.9674\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.0971 - accuracy: 0.9575 - val_loss: 0.1097 - val_accuracy: 0.9592\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0962 - accuracy: 0.9563 - val_loss: 0.0723 - val_accuracy: 0.9690\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0826 - accuracy: 0.9641 - val_loss: 0.0919 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0796 - accuracy: 0.9645 - val_loss: 0.0800 - val_accuracy: 0.9690\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0905 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9674\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0873 - accuracy: 0.9608 - val_loss: 0.1020 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0776 - accuracy: 0.9690 - val_loss: 0.0982 - val_accuracy: 0.9690\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0839 - accuracy: 0.9641 - val_loss: 0.0813 - val_accuracy: 0.9674\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0857 - accuracy: 0.9633 - val_loss: 0.1204 - val_accuracy: 0.9608\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9665 - val_loss: 0.0947 - val_accuracy: 0.9657\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0865 - accuracy: 0.9641 - val_loss: 0.1054 - val_accuracy: 0.9657\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0940 - accuracy: 0.9612 - val_loss: 0.0821 - val_accuracy: 0.9690\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0846 - accuracy: 0.9641 - val_loss: 0.1317 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0773 - accuracy: 0.9682 - val_loss: 0.1421 - val_accuracy: 0.9576\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0958 - accuracy: 0.9584 - val_loss: 0.1490 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0880 - accuracy: 0.9653 - val_loss: 0.0884 - val_accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263d82cdd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "model= models.Sequential()\n",
    "model.add(TimeDistributed(Conv2D(2, (3, 3), strides=(1,1),activation='relu'),input_shape=(6, 28, 28, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "model.add(TimeDistributed(Conv2D(4, (3, 3), strides=(1,1),activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(8,return_sequences=False,dropout=0.2)) # used 32 units\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23f6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253731343283582 0.9528218434864658 0.9253731343283582\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5b058ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e43530db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2643d22cdf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99db6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "742d452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 6, 32)        11096       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 6, 32)        64          patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_36 (MultiH (None, 6, 32)        8416        layer_normalization_85[0][0]     \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 6, 32)        8320        layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 6, 32)        0           multi_head_attention_36[0][0]    \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "                                                                 lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 6, 32)        64          add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_52 (Sequential)      (None, 6, 32)        1056        layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 6, 32)        0           sequential_52[0][0]              \n",
      "                                                                 add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 6, 32)        64          add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_37 (MultiH (None, 6, 32)        8416        layer_normalization_87[0][0]     \n",
      "                                                                 layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 6, 32)        8320        layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 6, 32)        0           multi_head_attention_37[0][0]    \n",
      "                                                                 add_81[0][0]                     \n",
      "                                                                 lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 6, 32)        64          add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_53 (Sequential)      (None, 6, 32)        1056        layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 6, 32)        0           sequential_53[0][0]              \n",
      "                                                                 add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 6, 32)        64          add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 47,066\n",
      "Trainable params: 47,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 45s 182ms/step - loss: 0.3119 - accuracy: 0.8730 - val_loss: 0.1708 - val_accuracy: 0.9331\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 9s 119ms/step - loss: 0.1945 - accuracy: 0.9253 - val_loss: 0.1656 - val_accuracy: 0.9396\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.1590 - accuracy: 0.9379 - val_loss: 0.1555 - val_accuracy: 0.9380\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 10s 137ms/step - loss: 0.1506 - accuracy: 0.9420 - val_loss: 0.2573 - val_accuracy: 0.9119\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.1288 - accuracy: 0.9535 - val_loss: 0.2039 - val_accuracy: 0.9331\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.1138 - accuracy: 0.9588 - val_loss: 0.1660 - val_accuracy: 0.9347\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 12s 150ms/step - loss: 0.1093 - accuracy: 0.9624 - val_loss: 0.1294 - val_accuracy: 0.9511\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 11s 147ms/step - loss: 0.0845 - accuracy: 0.9694 - val_loss: 0.2172 - val_accuracy: 0.9413\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0719 - accuracy: 0.9767 - val_loss: 0.1852 - val_accuracy: 0.9250\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0763 - accuracy: 0.9690 - val_loss: 0.2777 - val_accuracy: 0.9054\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0587 - accuracy: 0.9775 - val_loss: 0.3214 - val_accuracy: 0.8923\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.1472 - val_accuracy: 0.9380\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.1741 - val_accuracy: 0.9478\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0469 - accuracy: 0.9829 - val_loss: 0.4499 - val_accuracy: 0.8825\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0551 - accuracy: 0.9792 - val_loss: 0.2270 - val_accuracy: 0.9380\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 13s 164ms/step - loss: 0.0378 - accuracy: 0.9869 - val_loss: 0.2365 - val_accuracy: 0.9331\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 11s 141ms/step - loss: 0.0393 - accuracy: 0.9865 - val_loss: 0.3281 - val_accuracy: 0.9070\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 13s 166ms/step - loss: 0.0336 - accuracy: 0.9886 - val_loss: 0.5056 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 12s 151ms/step - loss: 0.0563 - accuracy: 0.9804 - val_loss: 0.2307 - val_accuracy: 0.9299\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0328 - accuracy: 0.9886 - val_loss: 0.2689 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0313 - accuracy: 0.9886 - val_loss: 0.0974 - val_accuracy: 0.9674\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0393 - accuracy: 0.9853 - val_loss: 0.1747 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0449 - accuracy: 0.9853 - val_loss: 0.1142 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0167 - accuracy: 0.9931 - val_loss: 0.4680 - val_accuracy: 0.9021\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0479 - accuracy: 0.9816 - val_loss: 0.1475 - val_accuracy: 0.9608\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 0.0168 - accuracy: 0.9931 - val_loss: 0.2110 - val_accuracy: 0.9511\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.3497 - val_accuracy: 0.9282\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.3747 - val_accuracy: 0.9233\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.3154 - val_accuracy: 0.9380\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 0.2614 - val_accuracy: 0.9396\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3280 - val_accuracy: 0.9331\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.3375 - val_accuracy: 0.9347\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.4055 - val_accuracy: 0.9299\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0205 - accuracy: 0.9955 - val_loss: 0.1498 - val_accuracy: 0.9576\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0625 - accuracy: 0.9788 - val_loss: 0.3064 - val_accuracy: 0.9266\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.2326 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.3434 - val_accuracy: 0.9364\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3835 - val_accuracy: 0.9299\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0364 - accuracy: 0.9857 - val_loss: 0.4883 - val_accuracy: 0.8793\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 10s 132ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.2401 - val_accuracy: 0.9445\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9396\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.9396\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 8.8423e-04 - accuracy: 1.0000 - val_loss: 0.3626 - val_accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.3936 - val_accuracy: 0.9299\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3034 - val_accuracy: 0.9478\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.1240 - accuracy: 0.9608 - val_loss: 0.1293 - val_accuracy: 0.9592\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.2322 - val_accuracy: 0.9429\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1845 - val_accuracy: 0.9641\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0163 - accuracy: 0.9935 - val_loss: 0.2686 - val_accuracy: 0.9445\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.2194 - val_accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9396\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.6788e-04 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.9429\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.2790 - val_accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2257 - val_accuracy: 0.9576\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0542 - accuracy: 0.9812 - val_loss: 0.1926 - val_accuracy: 0.9543\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0236 - accuracy: 0.9890 - val_loss: 0.2859 - val_accuracy: 0.9413\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2832 - val_accuracy: 0.9494\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 11s 148ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.3047 - val_accuracy: 0.9413\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2930 - val_accuracy: 0.9511\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 6.1541e-04 - accuracy: 1.0000 - val_loss: 0.3011 - val_accuracy: 0.9527\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 6.8710e-04 - accuracy: 1.0000 - val_loss: 0.3303 - val_accuracy: 0.9478\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.9433e-04 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9462\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 2.6167e-04 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9445\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 2.9263e-04 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9478\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.7604e-04 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9462\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 2.8471e-04 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9478\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 1.3727e-04 - accuracy: 1.0000 - val_loss: 0.3786 - val_accuracy: 0.9445\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.4250 - val_accuracy: 0.9331\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.2091e-04 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 0.9494\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0477 - accuracy: 0.9873 - val_loss: 0.2780 - val_accuracy: 0.9527\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.2241 - val_accuracy: 0.9478\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.3305 - val_accuracy: 0.9364\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 0.3235 - val_accuracy: 0.9478\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 0.9478\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 9.5974e-04 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 10s 133ms/step - loss: 2.8283e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9494\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 2.5996e-04 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9445\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 1.1576e-04 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 0.9429\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.1305e-04 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.9445\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 9.8853e-05 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9462\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.3591e-04 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.6559e-04 - accuracy: 1.0000 - val_loss: 0.3077 - val_accuracy: 0.9543\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 0.2865 - val_accuracy: 0.9315\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0272 - accuracy: 0.9886 - val_loss: 0.5347 - val_accuracy: 0.8842\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0279 - accuracy: 0.9918 - val_loss: 0.2224 - val_accuracy: 0.9462\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.2061 - val_accuracy: 0.9527\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2713 - val_accuracy: 0.9478\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 9.6324e-04 - accuracy: 0.9996 - val_loss: 0.3461 - val_accuracy: 0.9429\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.2564e-04 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9445\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 5.9142e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9462\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 3.6335e-04 - accuracy: 1.0000 - val_loss: 0.3490 - val_accuracy: 0.9462\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 1.9334e-04 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9478\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.6028e-04 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.9429\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.2040e-04 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.9462\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 1.7867e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9494\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 1.2447e-04 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.9478\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.7745e-05 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 0.9445\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 9.9591e-05 - accuracy: 1.0000 - val_loss: 0.4102 - val_accuracy: 0.9445\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 8.3640e-05 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.6141e-05 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2643d486f40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2545595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2643d2d6f10>,\n",
       " <__main__.PatchEncoder at 0x2643d2d67f0>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d2d6a60>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2644020d220>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2640c307fd0>,\n",
       " <keras.layers.merge.Add at 0x2643d33f850>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2640b258eb0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d28c8b0>,\n",
       " <keras.layers.merge.Add at 0x2644020e730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d27bd00>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2643d3004f0>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2643d28c820>,\n",
       " <keras.layers.merge.Add at 0x2643d3b9970>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d362dc0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d40a670>,\n",
       " <keras.layers.merge.Add at 0x2643d27b730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d3002e0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling1D at 0x2643d3d6a30>,\n",
       " <keras.layers.core.Dense at 0x2643d42e9d0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5724c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000264554A5310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "attn = Model(inputs=model.input, outputs = model.layers[10].output)\n",
    "\n",
    "pred = attn.predict(test_df[:2], steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41a95b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.4132207 , -1.2195096 ,  0.8252611 ,  0.2068735 ,\n",
       "         -1.9345852 , -0.3024784 ,  0.68288636,  0.24726246,\n",
       "          1.4878579 , -0.38394576,  1.9602623 , -1.2115494 ,\n",
       "          0.7494351 , -1.2607741 ,  1.5064203 ,  0.4506906 ,\n",
       "         -0.10477968,  0.67580473,  0.11815078,  0.5772801 ,\n",
       "         -0.5792286 , -2.167793  ,  0.25299546,  0.17493363,\n",
       "         -1.3997712 ,  0.21966185, -0.42484006,  0.366758  ,\n",
       "         -0.8750425 , -0.6389731 , -1.5038117 ,  0.5427528 ],\n",
       "        [-0.49964952, -1.3981657 ,  0.9291378 ,  0.33286896,\n",
       "         -2.1367726 , -0.33195415,  0.7276804 ,  0.17592141,\n",
       "          1.6696632 , -0.3734444 ,  2.1794388 , -1.3467876 ,\n",
       "          0.8988415 , -1.5041916 ,  1.6590377 ,  0.6519078 ,\n",
       "         -0.22443868,  0.88728726,  0.08924537,  0.5581587 ,\n",
       "         -0.6181777 , -2.513119  ,  0.44085464,  0.16715612,\n",
       "         -1.6275524 ,  0.24514648, -0.36834878,  0.35315546,\n",
       "         -1.0476707 , -0.6746873 , -1.7237133 ,  0.67369545],\n",
       "        [-0.5054516 , -1.4051408 ,  0.931937  ,  0.33905593,\n",
       "         -2.1503923 , -0.32643858,  0.72040987,  0.16062592,\n",
       "          1.6761467 , -0.37332964,  2.192372  , -1.3548307 ,\n",
       "          0.91177434, -1.5271318 ,  1.674906  ,  0.67372906,\n",
       "         -0.23302507,  0.90983194,  0.08345299,  0.550993  ,\n",
       "         -0.6208706 , -2.5409727 ,  0.45941108,  0.17093034,\n",
       "         -1.6397773 ,  0.24939248, -0.35014296,  0.3488802 ,\n",
       "         -1.0668329 , -0.6731022 , -1.7361139 ,  0.685371  ],\n",
       "        [-0.45331264, -1.306415  ,  0.8866546 ,  0.25975394,\n",
       "         -2.0246832 , -0.34005788,  0.7407966 ,  0.26629165,\n",
       "          1.5913628 , -0.38598648,  2.0556822 , -1.2800528 ,\n",
       "          0.79983205, -1.3376297 ,  1.550189  ,  0.49013677,\n",
       "         -0.15583493,  0.7203113 ,  0.11949462,  0.58744025,\n",
       "         -0.58959144, -2.295015  ,  0.2989272 ,  0.1614392 ,\n",
       "         -1.4926645 ,  0.22279164, -0.45505288,  0.36787757,\n",
       "         -0.9085372 , -0.6712196 , -1.6083722 ,  0.58423537],\n",
       "        [-0.46676803, -1.3375354 ,  0.89921296,  0.2818547 ,\n",
       "         -2.0564473 , -0.3375323 ,  0.72806275,  0.22456515,\n",
       "          1.6079518 , -0.3822405 ,  2.0922425 , -1.2935742 ,\n",
       "          0.83231705, -1.3891375 ,  1.5930897 ,  0.543874  ,\n",
       "         -0.16311598,  0.7822279 ,  0.11047427,  0.5792245 ,\n",
       "         -0.6069935 , -2.3648562 ,  0.34867156,  0.16283749,\n",
       "         -1.5390068 ,  0.23143686, -0.41973758,  0.36701533,\n",
       "         -0.9627736 , -0.66932124, -1.6384935 ,  0.61658263],\n",
       "        [-0.47220904, -1.3485214 ,  0.9062314 ,  0.28910938,\n",
       "         -2.0689666 , -0.34121707,  0.73520386,  0.22686623,\n",
       "          1.6213392 , -0.38222903,  2.104739  , -1.3030748 ,\n",
       "          0.8397041 , -1.4010637 ,  1.5984538 ,  0.551494  ,\n",
       "         -0.17236249,  0.7889492 ,  0.10953354,  0.57948005,\n",
       "         -0.60737693, -2.3821776 ,  0.35537735,  0.16158032,\n",
       "         -1.5513701 ,  0.2319126 , -0.42227972,  0.36642838,\n",
       "         -0.967269  , -0.67334425, -1.6527948 ,  0.6217404 ]],\n",
       "\n",
       "       [[-0.42649883, -1.07714   ,  1.1484387 ,  0.08490907,\n",
       "         -1.6851516 , -0.46723652,  0.5061661 , -0.5605585 ,\n",
       "          1.0461252 , -0.49998504,  1.0360165 , -0.6946146 ,\n",
       "          0.97356695, -1.060988  ,  1.7463284 ,  0.32441568,\n",
       "          0.33397505,  1.1660689 ,  0.3562874 ,  0.45945892,\n",
       "         -0.47635746, -1.5587056 ,  0.7094144 , -0.17435731,\n",
       "         -1.5678124 ,  0.21163751,  0.17598131,  0.34968036,\n",
       "         -1.2812881 , -0.66485435, -1.0910249 ,  0.6593576 ],\n",
       "        [-0.42786318, -1.0868777 ,  1.1333342 ,  0.09772161,\n",
       "         -1.6886925 , -0.47518152,  0.5093493 , -0.56699693,\n",
       "          1.0518668 , -0.47610977,  1.0434015 , -0.7050921 ,\n",
       "          0.9808329 , -1.0820138 ,  1.7231802 ,  0.33740744,\n",
       "          0.35213426,  1.1791012 ,  0.36592385,  0.45341754,\n",
       "         -0.47471106, -1.5531352 ,  0.7377216 , -0.1596379 ,\n",
       "         -1.5623512 ,  0.20603526,  0.16128173,  0.33179644,\n",
       "         -1.2726437 , -0.669748  , -1.0993601 ,  0.6809811 ],\n",
       "        [-0.46682185, -1.154356  ,  1.1586797 ,  0.13679314,\n",
       "         -1.6567787 , -0.49478176,  0.52970284, -0.6290013 ,\n",
       "          1.0179622 , -0.5025515 ,  0.99508554, -0.7152843 ,\n",
       "          0.97520405, -1.1048411 ,  1.7275623 ,  0.32937735,\n",
       "          0.36986497,  1.1806638 ,  0.43615305,  0.43307212,\n",
       "         -0.48267272, -1.4823427 ,  0.75971186, -0.19307967,\n",
       "         -1.5393314 ,  0.22102971,  0.18088883,  0.34285673,\n",
       "         -1.2822428 , -0.71883744, -1.0900193 ,  0.6960212 ],\n",
       "        [-0.4363001 , -1.0829927 ,  1.1493462 ,  0.0923662 ,\n",
       "         -1.6651357 , -0.48290762,  0.5026756 , -0.5942865 ,\n",
       "          1.0257357 , -0.49765265,  0.9899746 , -0.69200104,\n",
       "          0.9832234 , -1.0667776 ,  1.7409017 ,  0.3290018 ,\n",
       "          0.35922012,  1.1847463 ,  0.38116938,  0.435992  ,\n",
       "         -0.4662827 , -1.517116  ,  0.7349468 , -0.183203  ,\n",
       "         -1.5593985 ,  0.21721348,  0.19318488,  0.33741537,\n",
       "         -1.2820184 , -0.67617387, -1.0793917 ,  0.6741583 ],\n",
       "        [-0.49830765, -1.1531427 ,  1.2187309 ,  0.10719223,\n",
       "         -1.587356  , -0.54115945,  0.49432966, -0.7039962 ,\n",
       "          0.9452714 , -0.57704806,  0.8339936 , -0.6744776 ,\n",
       "          0.9837212 , -1.0368228 ,  1.7723382 ,  0.28157845,\n",
       "          0.40617546,  1.2058271 ,  0.47497472,  0.3932675 ,\n",
       "         -0.44723678, -1.3788327 ,  0.7450216 , -0.2586214 ,\n",
       "         -1.5238549 ,  0.25344393,  0.27242252,  0.34878147,\n",
       "         -1.2884816 , -0.73330414, -1.0293506 ,  0.6805313 ],\n",
       "        [-0.4841547 , -1.1146897 ,  1.2149087 ,  0.08220633,\n",
       "         -1.5884408 , -0.53472805,  0.4779762 , -0.6898922 ,\n",
       "          0.942987  , -0.5763631 ,  0.8229502 , -0.6605422 ,\n",
       "          0.9893767 , -1.0167508 ,  1.7826542 ,  0.28118014,\n",
       "          0.4056453 ,  1.2089812 ,  0.44735393,  0.39052385,\n",
       "         -0.43789554, -1.3930148 ,  0.73019046, -0.25510353,\n",
       "         -1.5354162 ,  0.25418186,  0.28484488,  0.3454959 ,\n",
       "         -1.2892004 , -0.71183616, -1.0225661 ,  0.6692119 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0c239b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4260804453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[:3]\n",
    "y = attention_layer.predict(test_df, test_df, return_attention_scores=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e844434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51cf7614",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected input 0 to have rank 3 but got: 2 [Op:Einsum]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4266437127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_attention_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take one sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridspec_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\multi_head_attention.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, value, key, attention_mask, return_attention_scores, training)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;31m#   H = `size_per_head`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# `query` = [B, T, N ,H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;31m# `key` = [B, S, N, H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\einsum_dense.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7186\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected input 0 to have rank 3 but got: 2 [Op:Einsum]"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[10]\n",
    "_, attention_scores = attention_layer(Y_test[:1], test_df[:1], return_attention_scores=True) # take one sample\n",
    "fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[5,5,0.2]))\n",
    "sb.heatmap(attention_scores[0, 0, :, :], annot=True, cbar=False, ax=axs[0])\n",
    "sb.heatmap(attention_scores[0, 1, :, :], annot=True, yticklabels=False, cbar=False, ax=axs[1])\n",
    "fig.colorbar(axs[1].collections[0], cax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b30013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196759ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250da400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
