{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acba090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa13c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf0822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeab214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8a504b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2590.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn1='D:/INV/data/invasive-aquatic-species-data/invasive/*/'\n",
    "trn2='D:/INV/data/invasive-aquatic-species-data/noninvasive/*/'\n",
    "tr1= glob(trn1)\n",
    "tr2= glob(trn2)\n",
    "tr1= shuffle(tr1)\n",
    "tr2= shuffle(tr2)\n",
    "\n",
    "tran_index_inv = np.round( len(tr1)* .7 )\n",
    "tran_index_noninv = np.round( len(tr2)* .7  )\n",
    "tran_index_noninv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9c51980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr1[:(int) (tran_index_inv)]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[:(int) (tran_index_noninv)]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr1[:(int) (tran_index_inv)])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr2[:(int) (tran_index_noninv)])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])        \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = a.resize((28, 28))\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),28,28,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "079c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3062, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "train_df= []\n",
    "breath = 6\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*6+k)\n",
    "        \n",
    "        deff.append(X_train[index])\n",
    "        \n",
    "    train_df.append(deff)\n",
    "\n",
    "Y_train = to_categorical(label)\n",
    "train_df = np.array(train_df)\n",
    "YY_Train = label\n",
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24d262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr1[(int) (tran_index_inv) + 1 :]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[ (int)(tran_index_noninv) + 1:]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr1[(int) (tran_index_inv) + 1 :])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr2[ (int)(tran_index_noninv) + 1:])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,6):\n",
    "        data.append(a[k])        \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = a.resize((28, 28))\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),28,28,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "661c2759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1310, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "test_df= []\n",
    "breath = 6\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*6+k)\n",
    "        \n",
    "        deff.append(X_test[index])\n",
    "        \n",
    "    test_df.append(deff)\n",
    "    \n",
    "Y_test = to_categorical(label)\n",
    "test_df = np.array(test_df)\n",
    "YY_Test = label\n",
    "np.shape(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56c507ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52b5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = ( 6, 28, 28, 3 )\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 32\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae32102",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88acaa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c674b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6123, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31947b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc77872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 6, 32)        75488       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 6, 32)        64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_14 (MultiH (None, 6, 32)        25184       layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 6, 32)        8320        layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 6, 32)        0           multi_head_attention_14[0][0]    \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "                                                                 lstm_14[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 6, 32)        64          add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 6, 32)        1056        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 6, 32)        0           sequential_17[0][0]              \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 6, 32)        64          add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_15 (MultiH (None, 6, 32)        25184       layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 6, 32)        8320        layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 6, 32)        0           multi_head_attention_15[0][0]    \n",
      "                                                                 add_29[0][0]                     \n",
      "                                                                 lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 6, 32)        64          add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 6, 32)        1056        layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 6, 32)        0           sequential_18[0][0]              \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 6, 32)        64          add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_16 (MultiH (None, 6, 32)        25184       layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 6, 32)        8320        layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 6, 32)        0           multi_head_attention_16[0][0]    \n",
      "                                                                 add_31[0][0]                     \n",
      "                                                                 lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 6, 32)        64          add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 6, 32)        1056        layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 6, 32)        0           sequential_19[0][0]              \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 6, 32)        64          add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_17 (MultiH (None, 6, 32)        25184       layer_normalization_36[0][0]     \n",
      "                                                                 layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  (None, 6, 32)        8320        layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 6, 32)        0           multi_head_attention_17[0][0]    \n",
      "                                                                 add_33[0][0]                     \n",
      "                                                                 lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 6, 32)        64          add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 6, 32)        1056        layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 6, 32)        0           sequential_20[0][0]              \n",
      "                                                                 add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 6, 32)        64          add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_18 (MultiH (None, 6, 32)        25184       layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  (None, 6, 32)        8320        layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 6, 32)        0           multi_head_attention_18[0][0]    \n",
      "                                                                 add_35[0][0]                     \n",
      "                                                                 lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 6, 32)        64          add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 6, 32)        1056        layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 6, 32)        0           sequential_21[0][0]              \n",
      "                                                                 add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, 6, 32)        64          add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_19 (MultiH (None, 6, 32)        25184       layer_normalization_40[0][0]     \n",
      "                                                                 layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  (None, 6, 32)        8320        layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 6, 32)        0           multi_head_attention_19[0][0]    \n",
      "                                                                 add_37[0][0]                     \n",
      "                                                                 lstm_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, 6, 32)        64          add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 6, 32)        1056        layer_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 6, 32)        0           sequential_22[0][0]              \n",
      "                                                                 add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, 6, 32)        64          add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 283,746\n",
      "Trainable params: 283,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c833aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_df = train_df.reshape( train_df.shape[0] , train_df.shape[1],( train_df.shape[2] * train_df.shape[3] * 3)  )\n",
    "tt_df = test_df.reshape(test_df.shape[0] ,test_df.shape[1],( test_df.shape[2] * test_df.shape[3] * 3)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e83af007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 54s 187ms/step - loss: 0.5151 - accuracy: 0.7938 - val_loss: 0.2481 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.3214 - accuracy: 0.8640 - val_loss: 0.3681 - val_accuracy: 0.8581\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.1961 - accuracy: 0.9200 - val_loss: 0.0400 - val_accuracy: 0.9853\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.1838 - accuracy: 0.9204 - val_loss: 0.0879 - val_accuracy: 0.9592\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.1672 - accuracy: 0.9355 - val_loss: 0.1763 - val_accuracy: 0.9233\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.1519 - accuracy: 0.9334 - val_loss: 0.3547 - val_accuracy: 0.8564\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.1565 - accuracy: 0.9343 - val_loss: 0.1397 - val_accuracy: 0.9347\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.1504 - accuracy: 0.9392 - val_loss: 0.1850 - val_accuracy: 0.9135\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1785 - accuracy: 0.9232 - val_loss: 0.1234 - val_accuracy: 0.9445\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.1257 - accuracy: 0.9494 - val_loss: 0.1086 - val_accuracy: 0.9511\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.1170 - accuracy: 0.9543 - val_loss: 0.1143 - val_accuracy: 0.9543\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.1267 - accuracy: 0.9539 - val_loss: 0.1984 - val_accuracy: 0.9315\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.2050 - accuracy: 0.9265 - val_loss: 0.4642 - val_accuracy: 0.8483\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 8s 102ms/step - loss: 0.1983 - accuracy: 0.9228 - val_loss: 0.2348 - val_accuracy: 0.8728\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 0.1637 - accuracy: 0.9383 - val_loss: 0.0508 - val_accuracy: 0.9772\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.3159 - accuracy: 0.8808 - val_loss: 0.1718 - val_accuracy: 0.9951\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.1692 - accuracy: 0.9367 - val_loss: 0.1691 - val_accuracy: 0.9364\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.1315 - accuracy: 0.9535 - val_loss: 0.2753 - val_accuracy: 0.8940\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.1200 - accuracy: 0.9535 - val_loss: 0.2381 - val_accuracy: 0.9201\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 7s 97ms/step - loss: 0.1163 - accuracy: 0.9551 - val_loss: 0.0613 - val_accuracy: 0.9772\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.1123 - accuracy: 0.9543 - val_loss: 0.0866 - val_accuracy: 0.9592\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.1100 - accuracy: 0.9600 - val_loss: 0.1351 - val_accuracy: 0.9511\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.1098 - accuracy: 0.9571 - val_loss: 0.1813 - val_accuracy: 0.9347\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.0972 - accuracy: 0.9624 - val_loss: 0.2477 - val_accuracy: 0.9250\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 0.0974 - accuracy: 0.9600 - val_loss: 0.2487 - val_accuracy: 0.9201\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 7s 95ms/step - loss: 0.0911 - accuracy: 0.9673 - val_loss: 0.1309 - val_accuracy: 0.9429\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.0878 - accuracy: 0.9661 - val_loss: 0.1213 - val_accuracy: 0.9543\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0850 - accuracy: 0.9661 - val_loss: 0.1577 - val_accuracy: 0.9478\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 8s 110ms/step - loss: 0.0780 - accuracy: 0.9710 - val_loss: 0.1238 - val_accuracy: 0.9445\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 19s 251ms/step - loss: 0.0860 - accuracy: 0.9649 - val_loss: 0.0782 - val_accuracy: 0.9625\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 24s 308ms/step - loss: 0.0905 - accuracy: 0.9661 - val_loss: 0.0742 - val_accuracy: 0.9674\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 21s 276ms/step - loss: 0.0870 - accuracy: 0.9653 - val_loss: 0.1784 - val_accuracy: 0.9380\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 21s 278ms/step - loss: 0.0657 - accuracy: 0.9759 - val_loss: 0.2365 - val_accuracy: 0.9233\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 22s 286ms/step - loss: 0.0816 - accuracy: 0.9706 - val_loss: 0.1508 - val_accuracy: 0.9494\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 24s 317ms/step - loss: 0.0645 - accuracy: 0.9739 - val_loss: 0.1951 - val_accuracy: 0.9413\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 21s 278ms/step - loss: 0.0858 - accuracy: 0.9633 - val_loss: 0.1101 - val_accuracy: 0.9592\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 21s 271ms/step - loss: 0.0777 - accuracy: 0.9722 - val_loss: 0.1100 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 23s 295ms/step - loss: 0.0610 - accuracy: 0.9763 - val_loss: 0.1631 - val_accuracy: 0.9478\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 25s 324ms/step - loss: 0.0665 - accuracy: 0.9784 - val_loss: 0.1059 - val_accuracy: 0.9608\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.0613 - accuracy: 0.9784 - val_loss: 0.0605 - val_accuracy: 0.9772\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 24s 318ms/step - loss: 0.0761 - accuracy: 0.9731 - val_loss: 0.1708 - val_accuracy: 0.9429\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 23s 304ms/step - loss: 0.0642 - accuracy: 0.9763 - val_loss: 0.2503 - val_accuracy: 0.9364\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 22s 280ms/step - loss: 0.0538 - accuracy: 0.9829 - val_loss: 0.3435 - val_accuracy: 0.8923\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 20s 262ms/step - loss: 0.0831 - accuracy: 0.9665 - val_loss: 0.1775 - val_accuracy: 0.9413\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 21s 277ms/step - loss: 0.0469 - accuracy: 0.9824 - val_loss: 0.3052 - val_accuracy: 0.9201\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 25s 329ms/step - loss: 0.0595 - accuracy: 0.9763 - val_loss: 0.2155 - val_accuracy: 0.9299\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 24s 311ms/step - loss: 0.0644 - accuracy: 0.9759 - val_loss: 0.1775 - val_accuracy: 0.9494\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 21s 276ms/step - loss: 0.0665 - accuracy: 0.9780 - val_loss: 0.1471 - val_accuracy: 0.9511\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 20s 254ms/step - loss: 0.0534 - accuracy: 0.9837 - val_loss: 0.1283 - val_accuracy: 0.9543\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 19s 241ms/step - loss: 0.0685 - accuracy: 0.9714 - val_loss: 0.0778 - val_accuracy: 0.9690\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 19s 253ms/step - loss: 0.0712 - accuracy: 0.9731 - val_loss: 0.0954 - val_accuracy: 0.9706\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 19s 251ms/step - loss: 0.0603 - accuracy: 0.9771 - val_loss: 0.1085 - val_accuracy: 0.9641\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 21s 269ms/step - loss: 0.0447 - accuracy: 0.9853 - val_loss: 0.0859 - val_accuracy: 0.9739\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 19s 253ms/step - loss: 0.0500 - accuracy: 0.9816 - val_loss: 0.0982 - val_accuracy: 0.9690\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 22s 287ms/step - loss: 0.0751 - accuracy: 0.9739 - val_loss: 0.1458 - val_accuracy: 0.9543\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 22s 283ms/step - loss: 0.0541 - accuracy: 0.9784 - val_loss: 0.1649 - val_accuracy: 0.9576\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 20s 266ms/step - loss: 0.0407 - accuracy: 0.9853 - val_loss: 0.1029 - val_accuracy: 0.9592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "77/77 [==============================] - 18s 239ms/step - loss: 0.0375 - accuracy: 0.9849 - val_loss: 0.2557 - val_accuracy: 0.9347\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 18s 229ms/step - loss: 0.0406 - accuracy: 0.9849 - val_loss: 0.1971 - val_accuracy: 0.9282\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 18s 230ms/step - loss: 0.0433 - accuracy: 0.9829 - val_loss: 0.1376 - val_accuracy: 0.9674\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 19s 249ms/step - loss: 0.0345 - accuracy: 0.9865 - val_loss: 0.1067 - val_accuracy: 0.9755\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 18s 232ms/step - loss: 0.0498 - accuracy: 0.9820 - val_loss: 0.2316 - val_accuracy: 0.9445\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 18s 239ms/step - loss: 0.0393 - accuracy: 0.9837 - val_loss: 0.1602 - val_accuracy: 0.9625\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 19s 245ms/step - loss: 0.0467 - accuracy: 0.9820 - val_loss: 0.1732 - val_accuracy: 0.9299\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 19s 248ms/step - loss: 0.0340 - accuracy: 0.9886 - val_loss: 0.1994 - val_accuracy: 0.9560\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 19s 243ms/step - loss: 0.0352 - accuracy: 0.9857 - val_loss: 0.1865 - val_accuracy: 0.9396\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 18s 237ms/step - loss: 0.0263 - accuracy: 0.9890 - val_loss: 0.2230 - val_accuracy: 0.9331\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 18s 231ms/step - loss: 0.0454 - accuracy: 0.9853 - val_loss: 0.3099 - val_accuracy: 0.9168\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 18s 231ms/step - loss: 0.0522 - accuracy: 0.9784 - val_loss: 0.1758 - val_accuracy: 0.9396\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 18s 238ms/step - loss: 0.0381 - accuracy: 0.9849 - val_loss: 0.1815 - val_accuracy: 0.9511\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 18s 229ms/step - loss: 0.0363 - accuracy: 0.9882 - val_loss: 0.2171 - val_accuracy: 0.9364\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 18s 230ms/step - loss: 0.0253 - accuracy: 0.9910 - val_loss: 0.1950 - val_accuracy: 0.9576\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 18s 230ms/step - loss: 0.0969 - accuracy: 0.9690 - val_loss: 0.1402 - val_accuracy: 0.9543\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 19s 252ms/step - loss: 0.0415 - accuracy: 0.9861 - val_loss: 0.1877 - val_accuracy: 0.9429\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 21s 267ms/step - loss: 0.0226 - accuracy: 0.9931 - val_loss: 0.1186 - val_accuracy: 0.9755\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 19s 253ms/step - loss: 0.0296 - accuracy: 0.9898 - val_loss: 0.3727 - val_accuracy: 0.9184\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 19s 241ms/step - loss: 0.0446 - accuracy: 0.9845 - val_loss: 0.1841 - val_accuracy: 0.9429\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 18s 239ms/step - loss: 0.0192 - accuracy: 0.9931 - val_loss: 0.1562 - val_accuracy: 0.9625\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 18s 235ms/step - loss: 0.0217 - accuracy: 0.9922 - val_loss: 0.1751 - val_accuracy: 0.9625\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 19s 245ms/step - loss: 0.0308 - accuracy: 0.9873 - val_loss: 0.1067 - val_accuracy: 0.9723\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 18s 239ms/step - loss: 0.0507 - accuracy: 0.9816 - val_loss: 0.2099 - val_accuracy: 0.9364\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 20s 261ms/step - loss: 0.0553 - accuracy: 0.9784 - val_loss: 0.1843 - val_accuracy: 0.9511\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 22s 281ms/step - loss: 0.0343 - accuracy: 0.9869 - val_loss: 0.1458 - val_accuracy: 0.9592\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 22s 289ms/step - loss: 0.0225 - accuracy: 0.9931 - val_loss: 0.1437 - val_accuracy: 0.9608\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 20s 264ms/step - loss: 0.0224 - accuracy: 0.9943 - val_loss: 0.2653 - val_accuracy: 0.9494\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 19s 252ms/step - loss: 0.0263 - accuracy: 0.9898 - val_loss: 0.2040 - val_accuracy: 0.9543\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 19s 252ms/step - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.2399 - val_accuracy: 0.9560\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 20s 255ms/step - loss: 0.0254 - accuracy: 0.9935 - val_loss: 0.1165 - val_accuracy: 0.9674\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 21s 272ms/step - loss: 0.0217 - accuracy: 0.9922 - val_loss: 0.3541 - val_accuracy: 0.9135\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 22s 284ms/step - loss: 0.0095 - accuracy: 0.9967 - val_loss: 0.2000 - val_accuracy: 0.9592\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 22s 280ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.3045 - val_accuracy: 0.9396\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 25s 323ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.1675 - val_accuracy: 0.9690\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 21s 271ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.2961 - val_accuracy: 0.9462\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 19s 251ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.3924 - val_accuracy: 0.9233\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 21s 270ms/step - loss: 0.0317 - accuracy: 0.9882 - val_loss: 0.1970 - val_accuracy: 0.9527\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 23s 304ms/step - loss: 0.0278 - accuracy: 0.9910 - val_loss: 0.1452 - val_accuracy: 0.9478\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 21s 272ms/step - loss: 0.0328 - accuracy: 0.9869 - val_loss: 0.2063 - val_accuracy: 0.9494\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 22s 280ms/step - loss: 0.0192 - accuracy: 0.9922 - val_loss: 0.2061 - val_accuracy: 0.9478\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 21s 268ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.1822 - val_accuracy: 0.9592\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 21s 280ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.3892 - val_accuracy: 0.9233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263ce4cf6a0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3abe3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1103,    0],\n",
       "       [   6,  201]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786c9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852941176470589 0.9972801450589301 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87100053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_3 (PatchEncoder)  (None, 6, 32)        75488       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 6, 32)        64          patch_encoder_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 6, 32)        8416        layer_normalization_25[0][0]     \n",
      "                                                                 layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 6, 32)        0           multi_head_attention_10[0][0]    \n",
      "                                                                 patch_encoder_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 6, 32)        64          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 6, 32)        1056        layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 32)        0           sequential_14[0][0]              \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 6, 32)        64          add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 6, 32)        8416        layer_normalization_27[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 6, 32)        0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 6, 32)        64          add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 6, 32)        1056        layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 6, 32)        0           sequential_15[0][0]              \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 6, 32)        64          add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 94,818\n",
      "Trainable params: 94,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 12s 47ms/step - loss: 0.2790 - accuracy: 0.8783 - val_loss: 0.0928 - val_accuracy: 0.9543\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1682 - accuracy: 0.9294 - val_loss: 0.2265 - val_accuracy: 0.8907\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1701 - accuracy: 0.9302 - val_loss: 0.0314 - val_accuracy: 0.9918\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1472 - accuracy: 0.9445 - val_loss: 0.1172 - val_accuracy: 0.9494\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1489 - accuracy: 0.9383 - val_loss: 0.1258 - val_accuracy: 0.9429\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1472 - accuracy: 0.9379 - val_loss: 0.2192 - val_accuracy: 0.9070\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1544 - accuracy: 0.9339 - val_loss: 0.1849 - val_accuracy: 0.9086\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1405 - accuracy: 0.9437 - val_loss: 0.1909 - val_accuracy: 0.9119\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1257 - accuracy: 0.9481 - val_loss: 0.0254 - val_accuracy: 0.9902\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1425 - accuracy: 0.9367 - val_loss: 0.0262 - val_accuracy: 0.9935\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1400 - accuracy: 0.9441 - val_loss: 0.1889 - val_accuracy: 0.9217\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1164 - accuracy: 0.9510 - val_loss: 0.1038 - val_accuracy: 0.9527\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1273 - accuracy: 0.9490 - val_loss: 0.0457 - val_accuracy: 0.9772\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1126 - accuracy: 0.9563 - val_loss: 0.1330 - val_accuracy: 0.9445\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1025 - accuracy: 0.9563 - val_loss: 0.0641 - val_accuracy: 0.9674\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1129 - accuracy: 0.9514 - val_loss: 0.1119 - val_accuracy: 0.9527\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.1048 - accuracy: 0.9604 - val_loss: 0.0834 - val_accuracy: 0.9592\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.1052 - accuracy: 0.9551 - val_loss: 0.0691 - val_accuracy: 0.9674\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.1115 - accuracy: 0.9535 - val_loss: 0.1969 - val_accuracy: 0.9184\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0936 - accuracy: 0.9641 - val_loss: 0.0389 - val_accuracy: 0.9804\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0909 - accuracy: 0.9645 - val_loss: 0.2659 - val_accuracy: 0.9054\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0863 - accuracy: 0.9645 - val_loss: 0.1013 - val_accuracy: 0.9592\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 35ms/step - loss: 0.0809 - accuracy: 0.9698 - val_loss: 0.1611 - val_accuracy: 0.9494\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0750 - accuracy: 0.9694 - val_loss: 0.0934 - val_accuracy: 0.9657\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0820 - accuracy: 0.9694 - val_loss: 0.1012 - val_accuracy: 0.9592\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0690 - accuracy: 0.9735 - val_loss: 0.1078 - val_accuracy: 0.9641\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0729 - accuracy: 0.9710 - val_loss: 0.0679 - val_accuracy: 0.9755\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0737 - accuracy: 0.9702 - val_loss: 0.1705 - val_accuracy: 0.9478\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0576 - accuracy: 0.9784 - val_loss: 0.1849 - val_accuracy: 0.9445\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0729 - accuracy: 0.9702 - val_loss: 0.1034 - val_accuracy: 0.9608\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0610 - accuracy: 0.9767 - val_loss: 0.0533 - val_accuracy: 0.9837\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0546 - accuracy: 0.9784 - val_loss: 0.1672 - val_accuracy: 0.9576\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0552 - accuracy: 0.9759 - val_loss: 0.2031 - val_accuracy: 0.9429\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0543 - accuracy: 0.9792 - val_loss: 0.0950 - val_accuracy: 0.9657\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0763 - accuracy: 0.9702 - val_loss: 0.1132 - val_accuracy: 0.9657\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0512 - accuracy: 0.9784 - val_loss: 0.1733 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0503 - accuracy: 0.9784 - val_loss: 0.0733 - val_accuracy: 0.9739\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0430 - accuracy: 0.9837 - val_loss: 0.1604 - val_accuracy: 0.9543\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0402 - accuracy: 0.9829 - val_loss: 0.2186 - val_accuracy: 0.9478\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0656 - accuracy: 0.9747 - val_loss: 0.0952 - val_accuracy: 0.9527\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0471 - accuracy: 0.9808 - val_loss: 0.0967 - val_accuracy: 0.9723\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0544 - accuracy: 0.9792 - val_loss: 0.2420 - val_accuracy: 0.9462\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0475 - accuracy: 0.9800 - val_loss: 0.1309 - val_accuracy: 0.9592\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0377 - accuracy: 0.9861 - val_loss: 0.1681 - val_accuracy: 0.9576\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0482 - accuracy: 0.9824 - val_loss: 0.1371 - val_accuracy: 0.9608\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0247 - accuracy: 0.9902 - val_loss: 0.1664 - val_accuracy: 0.9560\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0281 - accuracy: 0.9894 - val_loss: 0.2423 - val_accuracy: 0.9462\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0458 - accuracy: 0.9816 - val_loss: 0.1290 - val_accuracy: 0.9592\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0252 - accuracy: 0.9910 - val_loss: 0.1359 - val_accuracy: 0.9592\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0167 - accuracy: 0.9959 - val_loss: 0.2052 - val_accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0403 - accuracy: 0.9869 - val_loss: 0.1929 - val_accuracy: 0.9511\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0227 - accuracy: 0.9922 - val_loss: 0.2314 - val_accuracy: 0.9527\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0373 - accuracy: 0.9873 - val_loss: 0.2695 - val_accuracy: 0.9364\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.3173 - val_accuracy: 0.9462\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0185 - accuracy: 0.9931 - val_loss: 0.2376 - val_accuracy: 0.9396\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0295 - accuracy: 0.9882 - val_loss: 0.1306 - val_accuracy: 0.9706\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0337 - accuracy: 0.9861 - val_loss: 0.1834 - val_accuracy: 0.9560\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0169 - accuracy: 0.9935 - val_loss: 0.3142 - val_accuracy: 0.9429\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0264 - accuracy: 0.9902 - val_loss: 0.2932 - val_accuracy: 0.9494\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.1400 - val_accuracy: 0.9608\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.1906 - val_accuracy: 0.9543\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0145 - accuracy: 0.9959 - val_loss: 0.1673 - val_accuracy: 0.9592\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.2367 - val_accuracy: 0.9527\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.3590 - val_accuracy: 0.9364\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0279 - accuracy: 0.9890 - val_loss: 0.1551 - val_accuracy: 0.9608\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0436 - accuracy: 0.9841 - val_loss: 0.2427 - val_accuracy: 0.9527\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.3569 - val_accuracy: 0.9429\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0091 - accuracy: 0.9984 - val_loss: 0.3479 - val_accuracy: 0.9478\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.2849 - val_accuracy: 0.9380\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0267 - accuracy: 0.9902 - val_loss: 0.0687 - val_accuracy: 0.9723\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0372 - accuracy: 0.9845 - val_loss: 0.1943 - val_accuracy: 0.9576\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.2390 - val_accuracy: 0.9543\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 3s 33ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.3710 - val_accuracy: 0.9413\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.3887 - val_accuracy: 0.9135\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.2744 - val_accuracy: 0.9462\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.3364 - val_accuracy: 0.9462\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 3s 34ms/step - loss: 0.0290 - accuracy: 0.9918 - val_loss: 0.3349 - val_accuracy: 0.9331\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 2s 27ms/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 0.3601 - val_accuracy: 0.9380\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0286 - accuracy: 0.9894 - val_loss: 0.1759 - val_accuracy: 0.9592\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.3238 - val_accuracy: 0.9380\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.3573 - val_accuracy: 0.9413\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0213 - accuracy: 0.9914 - val_loss: 0.4856 - val_accuracy: 0.9021\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0257 - accuracy: 0.9910 - val_loss: 0.5060 - val_accuracy: 0.9201\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0541 - accuracy: 0.9796 - val_loss: 0.2363 - val_accuracy: 0.9413\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0446 - accuracy: 0.9837 - val_loss: 0.2589 - val_accuracy: 0.9478\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0171 - accuracy: 0.9943 - val_loss: 0.2900 - val_accuracy: 0.9347\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0252 - accuracy: 0.9922 - val_loss: 0.2186 - val_accuracy: 0.9511\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9984 - val_loss: 0.1806 - val_accuracy: 0.9657\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 2s 24ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.2190 - val_accuracy: 0.9608\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0083 - accuracy: 0.9963 - val_loss: 0.2750 - val_accuracy: 0.9527\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0174 - accuracy: 0.9931 - val_loss: 0.4152 - val_accuracy: 0.9250\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0560 - accuracy: 0.9784 - val_loss: 0.2111 - val_accuracy: 0.9511\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0107 - accuracy: 0.9959 - val_loss: 0.1770 - val_accuracy: 0.9576\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9984 - val_loss: 0.2556 - val_accuracy: 0.9527\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0142 - accuracy: 0.9943 - val_loss: 0.1129 - val_accuracy: 0.9723\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.2598 - val_accuracy: 0.9560\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.2788 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.3667 - val_accuracy: 0.9511\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0092 - accuracy: 0.9971 - val_loss: 0.4830 - val_accuracy: 0.9233\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 2s 26ms/step - loss: 0.0139 - accuracy: 0.9947 - val_loss: 0.2799 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2090ca48f70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7ea818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995049504950495 0.999096657633243 0.9901477832512315\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd475d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 6, 32)        75488       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 6, 32)        64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 6, 32)        50336       layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 32)        0           multi_head_attention[0][0]       \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 6, 32)        64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 6, 32)        1056        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 32)        0           sequential_13[0][0]              \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 6, 32)        64          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 6, 32)        50336       layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 32)        0           multi_head_attention_1[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 6, 32)        64          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 6, 32)        1056        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 32)        0           sequential_14[0][0]              \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 6, 32)        64          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 6, 32)        50336       layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 32)        0           multi_head_attention_2[0][0]     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 6, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 6, 32)        1056        layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 6, 32)        0           sequential_15[0][0]              \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 6, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 6, 32)        50336       layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 6, 32)        0           multi_head_attention_3[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 6, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)      (None, 6, 32)        1056        layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 6, 32)        0           sequential_16[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 6, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 6, 32)        50336       layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 6, 32)        0           multi_head_attention_4[0][0]     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 6, 32)        64          add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 6, 32)        1056        layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 6, 32)        0           sequential_17[0][0]              \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 6, 32)        64          add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 6, 32)        50336       layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 6, 32)        0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 6, 32)        64          add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 6, 32)        1056        layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 6, 32)        0           sequential_18[0][0]              \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 6, 32)        64          add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 6, 32)        50336       layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 6, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 6, 32)        64          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 6, 32)        1056        layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 32)        0           sequential_19[0][0]              \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 6, 32)        64          add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 6, 32)        50336       layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 6, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 6, 32)        64          add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 6, 32)        1056        layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 6, 32)        0           sequential_20[0][0]              \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 6, 32)        64          add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_8 (MultiHe (None, 6, 32)        50336       layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 6, 32)        0           multi_head_attention_8[0][0]     \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 6, 32)        64          add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 6, 32)        1056        layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 6, 32)        0           sequential_21[0][0]              \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 6, 32)        64          add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_9 (MultiHe (None, 6, 32)        50336       layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 6, 32)        0           multi_head_attention_9[0][0]     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 6, 32)        64          add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 6, 32)        1056        layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 6, 32)        0           sequential_22[0][0]              \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 6, 32)        64          add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 6, 32)        50336       layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 6, 32)        0           multi_head_attention_10[0][0]    \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 6, 32)        64          add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 6, 32)        1056        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 6, 32)        0           sequential_23[0][0]              \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 6, 32)        64          add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 6, 32)        50336       layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 6, 32)        0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 6, 32)        64          add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_24 (Sequential)      (None, 6, 32)        1056        layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 6, 32)        0           sequential_24[0][0]              \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 6, 32)        64          add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 693,858\n",
      "Trainable params: 693,858\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 23s 103ms/step - loss: 0.5219 - accuracy: 0.7946 - val_loss: 0.2336 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.3663 - accuracy: 0.8432 - val_loss: 0.4138 - val_accuracy: 0.8206\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.2030 - accuracy: 0.9151 - val_loss: 0.2323 - val_accuracy: 0.8842\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.1755 - accuracy: 0.9294 - val_loss: 0.2131 - val_accuracy: 0.9086\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1551 - accuracy: 0.9326 - val_loss: 0.1142 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1516 - accuracy: 0.9347 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1491 - accuracy: 0.9379 - val_loss: 0.1124 - val_accuracy: 0.9494\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1667 - accuracy: 0.9343 - val_loss: 0.1455 - val_accuracy: 0.9347\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.1355 - accuracy: 0.9461 - val_loss: 0.1634 - val_accuracy: 0.9233\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1162 - accuracy: 0.9559 - val_loss: 0.0763 - val_accuracy: 0.9674\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1389 - accuracy: 0.9539 - val_loss: 0.0910 - val_accuracy: 0.9608\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.1209 - accuracy: 0.9522 - val_loss: 0.2704 - val_accuracy: 0.8989\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1106 - accuracy: 0.9575 - val_loss: 0.0810 - val_accuracy: 0.9608\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1089 - accuracy: 0.9563 - val_loss: 0.1549 - val_accuracy: 0.9299\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.1170 - accuracy: 0.9563 - val_loss: 0.0509 - val_accuracy: 0.9788\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.1228 - accuracy: 0.9514 - val_loss: 0.1363 - val_accuracy: 0.9396\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1019 - accuracy: 0.9604 - val_loss: 0.0798 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0915 - accuracy: 0.9653 - val_loss: 0.1441 - val_accuracy: 0.9413\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1009 - accuracy: 0.9608 - val_loss: 0.2379 - val_accuracy: 0.9233\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0921 - accuracy: 0.9661 - val_loss: 0.1586 - val_accuracy: 0.9478\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0931 - accuracy: 0.9653 - val_loss: 0.2354 - val_accuracy: 0.9038\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0979 - accuracy: 0.9600 - val_loss: 0.0736 - val_accuracy: 0.9674\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0867 - accuracy: 0.9673 - val_loss: 0.1006 - val_accuracy: 0.9560\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0817 - accuracy: 0.9714 - val_loss: 0.0889 - val_accuracy: 0.9706\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.1131 - val_accuracy: 0.9527\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0778 - accuracy: 0.9755 - val_loss: 0.1503 - val_accuracy: 0.9315\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0736 - accuracy: 0.9743 - val_loss: 0.3570 - val_accuracy: 0.8940\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0810 - accuracy: 0.9641 - val_loss: 0.3989 - val_accuracy: 0.8499\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0914 - accuracy: 0.9628 - val_loss: 0.1788 - val_accuracy: 0.9315\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.1003 - val_accuracy: 0.9674\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0675 - accuracy: 0.9722 - val_loss: 0.2169 - val_accuracy: 0.9282\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0621 - accuracy: 0.9739 - val_loss: 0.1146 - val_accuracy: 0.9511\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0536 - accuracy: 0.9767 - val_loss: 0.1284 - val_accuracy: 0.9543\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.1891 - val_accuracy: 0.9282\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0566 - accuracy: 0.9763 - val_loss: 0.1903 - val_accuracy: 0.9331\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0715 - accuracy: 0.9722 - val_loss: 0.3347 - val_accuracy: 0.9462\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0535 - accuracy: 0.9816 - val_loss: 0.1838 - val_accuracy: 0.9462\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0659 - accuracy: 0.9755 - val_loss: 0.2889 - val_accuracy: 0.8972\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0868 - accuracy: 0.9661 - val_loss: 0.1128 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0629 - accuracy: 0.9759 - val_loss: 0.2529 - val_accuracy: 0.9413\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0442 - accuracy: 0.9829 - val_loss: 0.0514 - val_accuracy: 0.9821\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0566 - accuracy: 0.9755 - val_loss: 0.2314 - val_accuracy: 0.9413\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0364 - accuracy: 0.9878 - val_loss: 0.2113 - val_accuracy: 0.9315\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0368 - accuracy: 0.9861 - val_loss: 0.3788 - val_accuracy: 0.8874\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.1981 - val_accuracy: 0.9347\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 0.0533 - accuracy: 0.9792 - val_loss: 0.1110 - val_accuracy: 0.9641\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0373 - accuracy: 0.9861 - val_loss: 0.1416 - val_accuracy: 0.9576\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0775 - accuracy: 0.9710 - val_loss: 0.1295 - val_accuracy: 0.9511\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0339 - accuracy: 0.9853 - val_loss: 0.2037 - val_accuracy: 0.9478\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0259 - accuracy: 0.9890 - val_loss: 0.1323 - val_accuracy: 0.9625\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0455 - accuracy: 0.9849 - val_loss: 0.0739 - val_accuracy: 0.9723\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0415 - accuracy: 0.9833 - val_loss: 0.1021 - val_accuracy: 0.9608\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0333 - accuracy: 0.9857 - val_loss: 0.1161 - val_accuracy: 0.9560\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0295 - accuracy: 0.9878 - val_loss: 0.1552 - val_accuracy: 0.9560\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.2156 - val_accuracy: 0.9608\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0389 - accuracy: 0.9849 - val_loss: 0.2261 - val_accuracy: 0.9527\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0262 - accuracy: 0.9894 - val_loss: 0.3019 - val_accuracy: 0.9364\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0304 - accuracy: 0.9853 - val_loss: 0.2947 - val_accuracy: 0.9299\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.3137 - val_accuracy: 0.9396\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0398 - accuracy: 0.9849 - val_loss: 0.1475 - val_accuracy: 0.9576\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.1855 - val_accuracy: 0.9445\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0304 - accuracy: 0.9869 - val_loss: 0.1199 - val_accuracy: 0.9706\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0410 - accuracy: 0.9837 - val_loss: 0.2752 - val_accuracy: 0.9445\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.2091 - val_accuracy: 0.9608\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.2158 - val_accuracy: 0.9592\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0369 - accuracy: 0.9878 - val_loss: 0.2110 - val_accuracy: 0.9462\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1017 - accuracy: 0.9641 - val_loss: 0.1465 - val_accuracy: 0.9560\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0837 - accuracy: 0.9690 - val_loss: 0.1615 - val_accuracy: 0.9560\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0396 - accuracy: 0.9873 - val_loss: 0.1601 - val_accuracy: 0.9560\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.1442 - val_accuracy: 0.9592\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0739 - accuracy: 0.9718 - val_loss: 0.1142 - val_accuracy: 0.9723\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0166 - accuracy: 0.9931 - val_loss: 0.1481 - val_accuracy: 0.9657\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0219 - accuracy: 0.9914 - val_loss: 0.1104 - val_accuracy: 0.9739\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.2892 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.1324 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0399 - accuracy: 0.9857 - val_loss: 0.2416 - val_accuracy: 0.9266\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 0.1394 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.1475 - val_accuracy: 0.9690\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.1367 - val_accuracy: 0.9739\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.2944 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.4960 - val_accuracy: 0.9184\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 7s 84ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0156 - accuracy: 0.9943 - val_loss: 0.1937 - val_accuracy: 0.9592\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.2368 - val_accuracy: 0.9413\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.1489 - val_accuracy: 0.9641\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0144 - accuracy: 0.9943 - val_loss: 0.1541 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0129 - accuracy: 0.9939 - val_loss: 0.2583 - val_accuracy: 0.9576\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.1197 - val_accuracy: 0.9739\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2260 - val_accuracy: 0.9608\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 6.6339e-04 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 6s 85ms/step - loss: 2.4115e-04 - accuracy: 1.0000 - val_loss: 0.2826 - val_accuracy: 0.9576\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 2.8148e-04 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9657\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 2.0978e-04 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9511\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.1025 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.2442 - val_accuracy: 0.9576\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0086 - accuracy: 0.9963 - val_loss: 0.3115 - val_accuracy: 0.9429\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 0.9608\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.2760 - val_accuracy: 0.9625\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0537 - accuracy: 0.9792 - val_loss: 0.1679 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263c237ab50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(12):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output =layers.MultiHeadAttention (  num_heads=12, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c6c53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771 0.9863415555924266 0.98989898989899\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26057200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 6, 32)        11096       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, 6, 32)        64          patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_34 (MultiH (None, 6, 32)        25184       layer_normalization_73[0][0]     \n",
      "                                                                 layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                  (None, 6, 32)        8320        layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 6, 32)        0           multi_head_attention_34[0][0]    \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "                                                                 lstm_34[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, 6, 32)        64          add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_40 (Sequential)      (None, 6, 32)        1056        layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 6, 32)        0           sequential_40[0][0]              \n",
      "                                                                 add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, 6, 32)        64          add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_35 (MultiH (None, 6, 32)        25184       layer_normalization_75[0][0]     \n",
      "                                                                 layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                  (None, 6, 32)        8320        layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 6, 32)        0           multi_head_attention_35[0][0]    \n",
      "                                                                 add_69[0][0]                     \n",
      "                                                                 lstm_35[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, 6, 32)        64          add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_41 (Sequential)      (None, 6, 32)        1056        layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 6, 32)        0           sequential_41[0][0]              \n",
      "                                                                 add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, 6, 32)        64          add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_36 (MultiH (None, 6, 32)        25184       layer_normalization_77[0][0]     \n",
      "                                                                 layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                  (None, 6, 32)        8320        layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 6, 32)        0           multi_head_attention_36[0][0]    \n",
      "                                                                 add_71[0][0]                     \n",
      "                                                                 lstm_36[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, 6, 32)        64          add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_42 (Sequential)      (None, 6, 32)        1056        layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 6, 32)        0           sequential_42[0][0]              \n",
      "                                                                 add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, 6, 32)        64          add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_37 (MultiH (None, 6, 32)        25184       layer_normalization_79[0][0]     \n",
      "                                                                 layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                  (None, 6, 32)        8320        layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 6, 32)        0           multi_head_attention_37[0][0]    \n",
      "                                                                 add_73[0][0]                     \n",
      "                                                                 lstm_37[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, 6, 32)        64          add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_43 (Sequential)      (None, 6, 32)        1056        layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 6, 32)        0           sequential_43[0][0]              \n",
      "                                                                 add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, 6, 32)        64          add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_38 (MultiH (None, 6, 32)        25184       layer_normalization_81[0][0]     \n",
      "                                                                 layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                  (None, 6, 32)        8320        layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 6, 32)        0           multi_head_attention_38[0][0]    \n",
      "                                                                 add_75[0][0]                     \n",
      "                                                                 lstm_38[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, 6, 32)        64          add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_44 (Sequential)      (None, 6, 32)        1056        layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 6, 32)        0           sequential_44[0][0]              \n",
      "                                                                 add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, 6, 32)        64          add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_39 (MultiH (None, 6, 32)        25184       layer_normalization_83[0][0]     \n",
      "                                                                 layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                  (None, 6, 32)        8320        layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 6, 32)        0           multi_head_attention_39[0][0]    \n",
      "                                                                 add_77[0][0]                     \n",
      "                                                                 lstm_39[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, 6, 32)        64          add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_45 (Sequential)      (None, 6, 32)        1056        layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 6, 32)        0           sequential_45[0][0]              \n",
      "                                                                 add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 6, 32)        64          add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 219,354\n",
      "Trainable params: 219,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2676f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 198s 674ms/step - loss: 0.5134 - accuracy: 0.7942 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 26s 341ms/step - loss: 0.2471 - accuracy: 0.8934 - val_loss: 0.2189 - val_accuracy: 0.8825\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 26s 344ms/step - loss: 0.2021 - accuracy: 0.9106 - val_loss: 0.1020 - val_accuracy: 0.9364\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 30s 393ms/step - loss: 0.1864 - accuracy: 0.9204 - val_loss: 0.1285 - val_accuracy: 0.9478\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.1807 - accuracy: 0.9339 - val_loss: 0.1759 - val_accuracy: 0.9217\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 25s 321ms/step - loss: 0.1410 - accuracy: 0.9481 - val_loss: 0.1612 - val_accuracy: 0.9233\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 24s 309ms/step - loss: 0.1318 - accuracy: 0.9514 - val_loss: 0.0987 - val_accuracy: 0.9511\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.1213 - accuracy: 0.9535 - val_loss: 0.0875 - val_accuracy: 0.9511\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 25s 327ms/step - loss: 0.1006 - accuracy: 0.9624 - val_loss: 0.1159 - val_accuracy: 0.9690\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 25s 332ms/step - loss: 0.1111 - accuracy: 0.9600 - val_loss: 0.0732 - val_accuracy: 0.9723\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.1043 - accuracy: 0.9600 - val_loss: 0.0636 - val_accuracy: 0.9755\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 26s 335ms/step - loss: 0.0912 - accuracy: 0.9637 - val_loss: 0.0797 - val_accuracy: 0.9706\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 28s 367ms/step - loss: 0.0900 - accuracy: 0.9682 - val_loss: 0.0880 - val_accuracy: 0.9723\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 24s 312ms/step - loss: 0.0942 - accuracy: 0.9657 - val_loss: 0.1551 - val_accuracy: 0.9380\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 26s 344ms/step - loss: 0.0844 - accuracy: 0.9677 - val_loss: 0.1802 - val_accuracy: 0.9217\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 26s 332ms/step - loss: 0.0619 - accuracy: 0.9788 - val_loss: 0.1200 - val_accuracy: 0.9560\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 28s 371ms/step - loss: 0.0593 - accuracy: 0.9792 - val_loss: 0.0898 - val_accuracy: 0.9625\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 0.0555 - accuracy: 0.9796 - val_loss: 0.1353 - val_accuracy: 0.9625\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 27s 353ms/step - loss: 0.0711 - accuracy: 0.9743 - val_loss: 0.1284 - val_accuracy: 0.9560\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 29s 381ms/step - loss: 0.0449 - accuracy: 0.9857 - val_loss: 0.1709 - val_accuracy: 0.9576\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 33s 427ms/step - loss: 0.0512 - accuracy: 0.9833 - val_loss: 0.1534 - val_accuracy: 0.9560\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 28s 367ms/step - loss: 0.0318 - accuracy: 0.9902 - val_loss: 0.2941 - val_accuracy: 0.9429\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 0.0514 - accuracy: 0.9829 - val_loss: 0.1828 - val_accuracy: 0.9527\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 28s 366ms/step - loss: 0.0299 - accuracy: 0.9914 - val_loss: 0.1971 - val_accuracy: 0.9511\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 27s 351ms/step - loss: 0.0363 - accuracy: 0.9878 - val_loss: 0.3014 - val_accuracy: 0.9299\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 29s 380ms/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 0.3126 - val_accuracy: 0.9086\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 24s 311ms/step - loss: 0.0428 - accuracy: 0.9841 - val_loss: 0.2312 - val_accuracy: 0.9413\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 24s 315ms/step - loss: 0.0342 - accuracy: 0.9898 - val_loss: 0.1636 - val_accuracy: 0.9625\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 23s 294ms/step - loss: 0.0169 - accuracy: 0.9959 - val_loss: 0.2124 - val_accuracy: 0.9592\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 23s 300ms/step - loss: 0.0407 - accuracy: 0.9882 - val_loss: 0.2657 - val_accuracy: 0.9331\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 25s 319ms/step - loss: 0.0224 - accuracy: 0.9947 - val_loss: 0.2234 - val_accuracy: 0.9511\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 27s 345ms/step - loss: 0.0278 - accuracy: 0.9910 - val_loss: 0.1733 - val_accuracy: 0.9576\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 27s 353ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 0.1619 - val_accuracy: 0.9674\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 23s 305ms/step - loss: 0.0195 - accuracy: 0.9931 - val_loss: 0.2014 - val_accuracy: 0.9560\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 26s 336ms/step - loss: 0.0233 - accuracy: 0.9931 - val_loss: 0.1182 - val_accuracy: 0.9739\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 28s 364ms/step - loss: 0.0259 - accuracy: 0.9918 - val_loss: 0.1113 - val_accuracy: 0.9657\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 25s 323ms/step - loss: 0.0229 - accuracy: 0.9935 - val_loss: 0.1915 - val_accuracy: 0.9674\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 24s 317ms/step - loss: 0.0510 - accuracy: 0.9829 - val_loss: 0.2956 - val_accuracy: 0.9250\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 24s 310ms/step - loss: 0.0449 - accuracy: 0.9837 - val_loss: 0.2169 - val_accuracy: 0.9511\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 24s 309ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.2251 - val_accuracy: 0.9625\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 23s 306ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.2994 - val_accuracy: 0.9478\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 0.0080 - accuracy: 0.9971 - val_loss: 0.2290 - val_accuracy: 0.9592\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 27s 348ms/step - loss: 0.0179 - accuracy: 0.9943 - val_loss: 0.2625 - val_accuracy: 0.9494\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 28s 365ms/step - loss: 0.0328 - accuracy: 0.9865 - val_loss: 0.1594 - val_accuracy: 0.9608\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 30s 388ms/step - loss: 0.0203 - accuracy: 0.9927 - val_loss: 0.2659 - val_accuracy: 0.9429\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 31s 409ms/step - loss: 0.0258 - accuracy: 0.9935 - val_loss: 0.1611 - val_accuracy: 0.9429\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 30s 389ms/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.1656 - val_accuracy: 0.9641\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 29s 381ms/step - loss: 0.0170 - accuracy: 0.9931 - val_loss: 0.2286 - val_accuracy: 0.9576\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 29s 380ms/step - loss: 0.0115 - accuracy: 0.9955 - val_loss: 0.2846 - val_accuracy: 0.9445\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 31s 399ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.2176 - val_accuracy: 0.9576\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 28s 359ms/step - loss: 0.0423 - accuracy: 0.9878 - val_loss: 0.1684 - val_accuracy: 0.9543\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 28s 361ms/step - loss: 0.0412 - accuracy: 0.9865 - val_loss: 0.1633 - val_accuracy: 0.9674\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 27s 348ms/step - loss: 0.0495 - accuracy: 0.9841 - val_loss: 0.0663 - val_accuracy: 0.9772\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 28s 364ms/step - loss: 0.0118 - accuracy: 0.9955 - val_loss: 0.1823 - val_accuracy: 0.9625\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.2935 - val_accuracy: 0.9462\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 26s 344ms/step - loss: 0.0337 - accuracy: 0.9878 - val_loss: 0.0979 - val_accuracy: 0.9706\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 28s 367ms/step - loss: 0.0154 - accuracy: 0.9943 - val_loss: 0.2854 - val_accuracy: 0.9413\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 29s 376ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.2639 - val_accuracy: 0.9494\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.0020 - accuracy: 0.9996 - val_loss: 0.2742 - val_accuracy: 0.9494\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 29s 378ms/step - loss: 5.5904e-04 - accuracy: 1.0000 - val_loss: 0.3360 - val_accuracy: 0.9429\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 32s 412ms/step - loss: 0.0551 - accuracy: 0.9824 - val_loss: 0.1544 - val_accuracy: 0.9625\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 29s 383ms/step - loss: 0.0139 - accuracy: 0.9955 - val_loss: 0.2543 - val_accuracy: 0.9478\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.1667 - val_accuracy: 0.9739\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 31s 397ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.2325 - val_accuracy: 0.9543\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 31s 409ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.2663 - val_accuracy: 0.9560\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.2763 - val_accuracy: 0.9576\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 32s 414ms/step - loss: 2.6153e-04 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.9494\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 31s 408ms/step - loss: 1.6481e-04 - accuracy: 1.0000 - val_loss: 0.3197 - val_accuracy: 0.9494\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 29s 383ms/step - loss: 1.4644e-04 - accuracy: 1.0000 - val_loss: 0.3273 - val_accuracy: 0.9478\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 27s 351ms/step - loss: 1.3043e-04 - accuracy: 1.0000 - val_loss: 0.3346 - val_accuracy: 0.9478\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 30s 393ms/step - loss: 1.0916e-04 - accuracy: 1.0000 - val_loss: 0.3470 - val_accuracy: 0.9478\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 25s 329ms/step - loss: 1.0511e-04 - accuracy: 1.0000 - val_loss: 0.3468 - val_accuracy: 0.9478\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 28s 360ms/step - loss: 8.4382e-05 - accuracy: 1.0000 - val_loss: 0.3503 - val_accuracy: 0.9478\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 27s 354ms/step - loss: 7.9529e-05 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9478\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 29s 376ms/step - loss: 7.6150e-05 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 0.9478\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 29s 376ms/step - loss: 7.0301e-05 - accuracy: 1.0000 - val_loss: 0.3616 - val_accuracy: 0.9478\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 27s 354ms/step - loss: 6.4474e-05 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9478\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 29s 377ms/step - loss: 6.2019e-05 - accuracy: 1.0000 - val_loss: 0.3674 - val_accuracy: 0.9478\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 27s 344ms/step - loss: 5.9657e-05 - accuracy: 1.0000 - val_loss: 0.3595 - val_accuracy: 0.9478\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 25s 322ms/step - loss: 9.8886e-05 - accuracy: 1.0000 - val_loss: 0.3495 - val_accuracy: 0.9576\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 24s 318ms/step - loss: 5.1741e-05 - accuracy: 1.0000 - val_loss: 0.3432 - val_accuracy: 0.9592\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 24s 308ms/step - loss: 1.2685e-04 - accuracy: 1.0000 - val_loss: 0.3416 - val_accuracy: 0.9576\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 24s 314ms/step - loss: 5.5980e-05 - accuracy: 1.0000 - val_loss: 0.3515 - val_accuracy: 0.9560\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 27s 350ms/step - loss: 4.0204e-05 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9543\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 29s 380ms/step - loss: 3.5069e-05 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9543\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 28s 370ms/step - loss: 3.5076e-05 - accuracy: 1.0000 - val_loss: 0.3720 - val_accuracy: 0.9543\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 26s 333ms/step - loss: 3.2774e-05 - accuracy: 1.0000 - val_loss: 0.3780 - val_accuracy: 0.9527\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 30s 388ms/step - loss: 2.9266e-05 - accuracy: 1.0000 - val_loss: 0.3830 - val_accuracy: 0.9527\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 2.8690e-05 - accuracy: 1.0000 - val_loss: 0.3860 - val_accuracy: 0.9527\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 25s 323ms/step - loss: 2.7231e-05 - accuracy: 1.0000 - val_loss: 0.3875 - val_accuracy: 0.9527\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 26s 333ms/step - loss: 2.5177e-05 - accuracy: 1.0000 - val_loss: 0.3898 - val_accuracy: 0.9527\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 25s 328ms/step - loss: 2.4615e-05 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 0.9527\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 27s 345ms/step - loss: 2.2731e-05 - accuracy: 1.0000 - val_loss: 0.3958 - val_accuracy: 0.9527\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 26s 334ms/step - loss: 2.2959e-05 - accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.9527\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 25s 329ms/step - loss: 2.0937e-05 - accuracy: 1.0000 - val_loss: 0.4043 - val_accuracy: 0.9511\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 26s 341ms/step - loss: 2.2745e-05 - accuracy: 1.0000 - val_loss: 0.3998 - val_accuracy: 0.9527\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 25s 329ms/step - loss: 1.9242e-05 - accuracy: 1.0000 - val_loss: 0.4031 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 27s 351ms/step - loss: 1.7633e-05 - accuracy: 1.0000 - val_loss: 0.4068 - val_accuracy: 0.9527\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 26s 335ms/step - loss: 1.6633e-05 - accuracy: 1.0000 - val_loss: 0.4091 - val_accuracy: 0.9527\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 1.5808e-05 - accuracy: 1.0000 - val_loss: 0.4119 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2640c33ad60>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef7bac82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1109,    0],\n",
       "       [   0,  201]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b4d89f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c36be08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514281067975519"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3afd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0cf3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( train_df,YY_Train , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cc7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(X_train , y_train , \"train\")\n",
    "validloader = prepare_dataloader(X_val, y_val, \"valid\")\n",
    "testloader = prepare_dataloader(test_df,YY_Test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0523831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6b82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28803bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tubelet_embedding (TubeletEmbed (None, 9, 32)        24608       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoder (PositionalE (None, 9, 32)        288         tubelet_embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 9, 32)        64          positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 9, 32)        4224        layer_normalization_15[0][0]     \n",
      "                                                                 layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 9, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 positional_encoder[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 9, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 9, 32)        8352        layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 9, 32)        0           sequential_9[0][0]               \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 9, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 9, 32)        4224        layer_normalization_17[0][0]     \n",
      "                                                                 layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 9, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 9, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 9, 32)        8352        layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 9, 32)        0           sequential_10[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 9, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 2)            66          global_average_pooling1d_3[0][0] \n",
      "==================================================================================================\n",
      "Total params: 50,434\n",
      "Trainable params: 50,434\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "PROJECTION_DIM = 32\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf326e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6aa5b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 20s 78ms/step - loss: 0.3926 - accuracy: 0.8436 - top-5-accuracy: 1.0000 - val_loss: 0.3380 - val_accuracy: 0.8254 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.2927 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.8418 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2794 - accuracy: 0.8653 - top-5-accuracy: 1.0000 - val_loss: 0.2698 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.2298 - accuracy: 0.8881 - top-5-accuracy: 1.0000 - val_loss: 0.2940 - val_accuracy: 0.8499 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.2393 - accuracy: 0.8877 - top-5-accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.8760 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.2201 - accuracy: 0.8918 - top-5-accuracy: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.8646 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.2056 - accuracy: 0.9028 - top-5-accuracy: 1.0000 - val_loss: 0.2771 - val_accuracy: 0.8711 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1891 - accuracy: 0.9110 - top-5-accuracy: 1.0000 - val_loss: 0.2182 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.1907 - accuracy: 0.9138 - top-5-accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.8989 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1848 - accuracy: 0.9167 - top-5-accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1675 - accuracy: 0.9261 - top-5-accuracy: 1.0000 - val_loss: 0.2019 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1725 - accuracy: 0.9306 - top-5-accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1630 - accuracy: 0.9290 - top-5-accuracy: 1.0000 - val_loss: 0.1966 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1800 - accuracy: 0.9220 - top-5-accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1566 - accuracy: 0.9355 - top-5-accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1476 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1799 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1507 - accuracy: 0.9359 - top-5-accuracy: 1.0000 - val_loss: 0.1821 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1488 - accuracy: 0.9367 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1385 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.1394 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1794 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.1332 - accuracy: 0.9453 - top-5-accuracy: 1.0000 - val_loss: 0.1667 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.1340 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1256 - accuracy: 0.9490 - top-5-accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1275 - accuracy: 0.9449 - top-5-accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1226 - accuracy: 0.9502 - top-5-accuracy: 1.0000 - val_loss: 0.1687 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1257 - accuracy: 0.9441 - top-5-accuracy: 1.0000 - val_loss: 0.1666 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.1256 - accuracy: 0.9461 - top-5-accuracy: 1.0000 - val_loss: 0.1774 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1153 - accuracy: 0.9539 - top-5-accuracy: 1.0000 - val_loss: 0.1668 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1192 - accuracy: 0.9510 - top-5-accuracy: 1.0000 - val_loss: 0.2215 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.1188 - accuracy: 0.9535 - top-5-accuracy: 1.0000 - val_loss: 0.1698 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.1002 - accuracy: 0.9575 - top-5-accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0950 - accuracy: 0.9645 - top-5-accuracy: 1.0000 - val_loss: 0.2260 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1041 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.1014 - accuracy: 0.9588 - top-5-accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0927 - accuracy: 0.9624 - top-5-accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0882 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1617 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0882 - accuracy: 0.9628 - top-5-accuracy: 1.0000 - val_loss: 0.1690 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0879 - accuracy: 0.9600 - top-5-accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0828 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.1662 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0872 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0748 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0807 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0703 - accuracy: 0.9686 - top-5-accuracy: 1.0000 - val_loss: 0.1479 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0779 - accuracy: 0.9702 - top-5-accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 4s 53ms/step - loss: 0.0776 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 4s 57ms/step - loss: 0.0707 - accuracy: 0.9677 - top-5-accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0759 - accuracy: 0.9690 - top-5-accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0656 - accuracy: 0.9735 - top-5-accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0671 - accuracy: 0.9714 - top-5-accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0598 - accuracy: 0.9755 - top-5-accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9527 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0708 - accuracy: 0.9698 - top-5-accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 4s 55ms/step - loss: 0.0569 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.2003 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0569 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1723 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 4s 54ms/step - loss: 0.0515 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.1930 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0562 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1689 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0487 - accuracy: 0.9820 - top-5-accuracy: 1.0000 - val_loss: 0.1747 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0623 - accuracy: 0.9751 - top-5-accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0609 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0538 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1722 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0446 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.1707 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0544 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0602 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.1778 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0460 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0383 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2105 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0504 - accuracy: 0.9784 - top-5-accuracy: 1.0000 - val_loss: 0.1686 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0451 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2012 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0452 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0495 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9478 - val_top-5-accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 3s 40ms/step - loss: 0.0388 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.1932 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0435 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 3s 41ms/step - loss: 0.0436 - accuracy: 0.9804 - top-5-accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9511 - val_top-5-accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0387 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0437 - accuracy: 0.9824 - top-5-accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 4s 49ms/step - loss: 0.0354 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2141 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 4s 47ms/step - loss: 0.0331 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2421 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 4s 51ms/step - loss: 0.0396 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.2242 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 4s 52ms/step - loss: 0.0461 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0346 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.2330 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.0306 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0453 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.2375 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0390 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.2326 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0405 - accuracy: 0.9837 - top-5-accuracy: 1.0000 - val_loss: 0.2045 - val_accuracy: 0.9494 - val_top-5-accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 3s 45ms/step - loss: 0.0337 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2055 - val_accuracy: 0.9445 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 4s 46ms/step - loss: 0.0243 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.2828 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0212 - accuracy: 0.9947 - top-5-accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0521 - accuracy: 0.9808 - top-5-accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 4s 56ms/step - loss: 0.0250 - accuracy: 0.9939 - top-5-accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9462 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.0295 - accuracy: 0.9890 - top-5-accuracy: 1.0000 - val_loss: 0.2660 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 5s 64ms/step - loss: 0.0298 - accuracy: 0.9882 - top-5-accuracy: 1.0000 - val_loss: 0.2464 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 3s 44ms/step - loss: 0.0415 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.2331 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 3s 42ms/step - loss: 0.0401 - accuracy: 0.9816 - top-5-accuracy: 1.0000 - val_loss: 0.2336 - val_accuracy: 0.9380 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 3s 43ms/step - loss: 0.0265 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.3482 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0365 - accuracy: 0.9869 - top-5-accuracy: 1.0000 - val_loss: 0.3200 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0349 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9396 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0632 - accuracy: 0.9747 - top-5-accuracy: 1.0000 - val_loss: 0.2011 - val_accuracy: 0.9413 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 3s 39ms/step - loss: 0.0355 - accuracy: 0.9857 - top-5-accuracy: 1.0000 - val_loss: 0.2601 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0273 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0286 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.3082 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 3s 38ms/step - loss: 0.0280 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.2955 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "41/41 [==============================] - 1s 17ms/step - loss: 0.0741 - accuracy: 0.9794 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 97.94%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c6df61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1104,   22],\n",
       "       [   5,  179]], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY_Test = YY_Test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = YY_Test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7d70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298701298701298 0.9362829932798964 0.9728260869565217\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3620fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398567119155354"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f75542c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "199466a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train_df[19]\n",
    "y = train_df[3]\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [x[0], x[1], x[2], x[3],y[0], y[1], y[2], y[3]]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3eb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027188ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea8a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 6, 28, 28, 64)     5248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 6, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 6, 14, 14, 128)    221312    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 3, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 3, 7, 7, 256)      884992    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 1, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 1, 3, 3, 512)      3539456   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 9,372,674\n",
      "Trainable params: 9,372,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3, 3, 3), activation=\"relu\",name=\"conv1\",   input_shape=(6,28,28,3), strides=(1, 1, 1), padding=\"same\"))  \n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
    "model.add(Conv3D(128, (3, 3, 3), activation=\"relu\",name=\"conv2\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
    "model.add(Conv3D(256, (3, 3, 3), activation=\"relu\",name=\"conv3a\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
    "model.add(Conv3D(512, (3, 3, 3), activation=\"relu\",name=\"conv4a\", strides=(1, 1, 1), padding=\"same\"))   \n",
    "\n",
    "model.add(Flatten())\n",
    "                     \n",
    "    # FC layers group\n",
    "model.add(Dense(1024, activation='relu', name='fc6'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax', name='fc8'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d1b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 357s 5s/step - loss: 0.3074 - accuracy: 0.8591 - val_loss: 0.1795 - val_accuracy: 0.9184\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 356s 5s/step - loss: 0.2036 - accuracy: 0.9224 - val_loss: 0.0635 - val_accuracy: 0.9723\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 338s 4s/step - loss: 0.1969 - accuracy: 0.9216 - val_loss: 0.0981 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 360s 5s/step - loss: 0.1657 - accuracy: 0.9306 - val_loss: 0.0700 - val_accuracy: 0.9674\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 358s 5s/step - loss: 0.1690 - accuracy: 0.9310 - val_loss: 0.0765 - val_accuracy: 0.9723\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 56s - loss: 0.1179 - accuracy: 0.9495 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19660/1501325532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73fd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_29 (TimeDis (None, 6, 26, 26, 2)      56        \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 6, 13, 13, 2)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 6, 11, 11, 4)      76        \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 6, 5, 5, 4)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 6, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 3,638\n",
      "Trainable params: 3,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 9s 63ms/step - loss: 0.5996 - accuracy: 0.7999 - val_loss: 0.1954 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.3871 - accuracy: 0.8109 - val_loss: 0.0698 - val_accuracy: 0.9951\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.2653 - accuracy: 0.8775 - val_loss: 0.1443 - val_accuracy: 0.9119\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2197 - accuracy: 0.9036 - val_loss: 0.1015 - val_accuracy: 0.9396\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 2s 22ms/step - loss: 0.2255 - accuracy: 0.9036 - val_loss: 0.0931 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1977 - accuracy: 0.9216 - val_loss: 0.1024 - val_accuracy: 0.9413\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1912 - accuracy: 0.9192 - val_loss: 0.1007 - val_accuracy: 0.9413\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1736 - accuracy: 0.9249 - val_loss: 0.0728 - val_accuracy: 0.9625\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1723 - accuracy: 0.9285 - val_loss: 0.0595 - val_accuracy: 0.9723\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1685 - accuracy: 0.9281 - val_loss: 0.0819 - val_accuracy: 0.9560\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1617 - accuracy: 0.9334 - val_loss: 0.0673 - val_accuracy: 0.9690\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1567 - accuracy: 0.9367 - val_loss: 0.0865 - val_accuracy: 0.9560\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1526 - accuracy: 0.9330 - val_loss: 0.1450 - val_accuracy: 0.9217\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1517 - accuracy: 0.9355 - val_loss: 0.0741 - val_accuracy: 0.9608\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1458 - accuracy: 0.9371 - val_loss: 0.0587 - val_accuracy: 0.9723\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1495 - accuracy: 0.9388 - val_loss: 0.1112 - val_accuracy: 0.9462\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1493 - accuracy: 0.9351 - val_loss: 0.0823 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1511 - accuracy: 0.9404 - val_loss: 0.0887 - val_accuracy: 0.9592\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1392 - accuracy: 0.9432 - val_loss: 0.0730 - val_accuracy: 0.9608\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1403 - accuracy: 0.9424 - val_loss: 0.0548 - val_accuracy: 0.9706\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1383 - accuracy: 0.9416 - val_loss: 0.0401 - val_accuracy: 0.9821\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1377 - accuracy: 0.9383 - val_loss: 0.0796 - val_accuracy: 0.9625\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1334 - accuracy: 0.9449 - val_loss: 0.0729 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1263 - accuracy: 0.9514 - val_loss: 0.0650 - val_accuracy: 0.9657\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1363 - accuracy: 0.9424 - val_loss: 0.0694 - val_accuracy: 0.9674\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1304 - accuracy: 0.9453 - val_loss: 0.0843 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1290 - accuracy: 0.9449 - val_loss: 0.0952 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1299 - accuracy: 0.9465 - val_loss: 0.0770 - val_accuracy: 0.9657\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1281 - accuracy: 0.9477 - val_loss: 0.0804 - val_accuracy: 0.9674\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 0.0688 - val_accuracy: 0.9690\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1204 - accuracy: 0.9526 - val_loss: 0.0862 - val_accuracy: 0.9641\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1332 - accuracy: 0.9449 - val_loss: 0.0644 - val_accuracy: 0.9690\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1205 - accuracy: 0.9457 - val_loss: 0.0761 - val_accuracy: 0.9674\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1197 - accuracy: 0.9473 - val_loss: 0.0930 - val_accuracy: 0.9641\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1345 - accuracy: 0.9388 - val_loss: 0.0513 - val_accuracy: 0.9772\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1221 - accuracy: 0.9469 - val_loss: 0.0652 - val_accuracy: 0.9690\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1235 - accuracy: 0.9473 - val_loss: 0.0506 - val_accuracy: 0.9755\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1137 - accuracy: 0.9518 - val_loss: 0.0591 - val_accuracy: 0.9706\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1185 - accuracy: 0.9494 - val_loss: 0.1011 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1152 - accuracy: 0.9526 - val_loss: 0.0479 - val_accuracy: 0.9772\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1166 - accuracy: 0.9522 - val_loss: 0.0808 - val_accuracy: 0.9674\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1106 - accuracy: 0.9530 - val_loss: 0.0765 - val_accuracy: 0.9657\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1124 - accuracy: 0.9555 - val_loss: 0.0914 - val_accuracy: 0.9641\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1097 - accuracy: 0.9526 - val_loss: 0.1018 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1180 - accuracy: 0.9477 - val_loss: 0.0502 - val_accuracy: 0.9755\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1083 - accuracy: 0.9514 - val_loss: 0.0792 - val_accuracy: 0.9657\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1104 - accuracy: 0.9522 - val_loss: 0.0804 - val_accuracy: 0.9657\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1078 - accuracy: 0.9530 - val_loss: 0.0793 - val_accuracy: 0.9674\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1058 - accuracy: 0.9543 - val_loss: 0.0647 - val_accuracy: 0.9706\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1088 - accuracy: 0.9506 - val_loss: 0.0543 - val_accuracy: 0.9723\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1102 - accuracy: 0.9526 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1052 - accuracy: 0.9604 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1075 - accuracy: 0.9530 - val_loss: 0.0544 - val_accuracy: 0.9723\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1023 - accuracy: 0.9588 - val_loss: 0.0930 - val_accuracy: 0.9657\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1092 - accuracy: 0.9584 - val_loss: 0.1430 - val_accuracy: 0.9429\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1212 - accuracy: 0.9469 - val_loss: 0.0491 - val_accuracy: 0.9788\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0981 - accuracy: 0.9559 - val_loss: 0.0854 - val_accuracy: 0.9657\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1032 - accuracy: 0.9518 - val_loss: 0.1032 - val_accuracy: 0.9576\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1055 - accuracy: 0.9547 - val_loss: 0.1238 - val_accuracy: 0.9560\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1008 - accuracy: 0.9596 - val_loss: 0.0652 - val_accuracy: 0.9706\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1004 - accuracy: 0.9575 - val_loss: 0.0691 - val_accuracy: 0.9674\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1131 - accuracy: 0.9539 - val_loss: 0.0863 - val_accuracy: 0.9657\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0976 - accuracy: 0.9608 - val_loss: 0.1033 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9600 - val_loss: 0.1076 - val_accuracy: 0.9641\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0962 - accuracy: 0.9592 - val_loss: 0.0723 - val_accuracy: 0.9674\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9559 - val_loss: 0.1495 - val_accuracy: 0.9494\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1030 - accuracy: 0.9551 - val_loss: 0.1109 - val_accuracy: 0.9641\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0991 - accuracy: 0.9559 - val_loss: 0.1028 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1012 - accuracy: 0.9588 - val_loss: 0.0904 - val_accuracy: 0.9657\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1014 - accuracy: 0.9551 - val_loss: 0.1130 - val_accuracy: 0.9576\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1081 - accuracy: 0.9539 - val_loss: 0.1036 - val_accuracy: 0.9641\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0987 - accuracy: 0.9579 - val_loss: 0.0856 - val_accuracy: 0.9641\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.1028 - val_accuracy: 0.9576\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0906 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9641\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0969 - accuracy: 0.9547 - val_loss: 0.0551 - val_accuracy: 0.9804\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0920 - accuracy: 0.9592 - val_loss: 0.1260 - val_accuracy: 0.9543\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0985 - accuracy: 0.9563 - val_loss: 0.0661 - val_accuracy: 0.9690\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0886 - accuracy: 0.9608 - val_loss: 0.0898 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0961 - accuracy: 0.9592 - val_loss: 0.1046 - val_accuracy: 0.9625\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0889 - accuracy: 0.9612 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0872 - accuracy: 0.9624 - val_loss: 0.0987 - val_accuracy: 0.9608\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0914 - accuracy: 0.9608 - val_loss: 0.0799 - val_accuracy: 0.9674\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0948 - accuracy: 0.9596 - val_loss: 0.1392 - val_accuracy: 0.9429\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9637 - val_loss: 0.0783 - val_accuracy: 0.9674\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.0971 - accuracy: 0.9575 - val_loss: 0.1097 - val_accuracy: 0.9592\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0962 - accuracy: 0.9563 - val_loss: 0.0723 - val_accuracy: 0.9690\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0826 - accuracy: 0.9641 - val_loss: 0.0919 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0796 - accuracy: 0.9645 - val_loss: 0.0800 - val_accuracy: 0.9690\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0905 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9674\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0873 - accuracy: 0.9608 - val_loss: 0.1020 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0776 - accuracy: 0.9690 - val_loss: 0.0982 - val_accuracy: 0.9690\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0839 - accuracy: 0.9641 - val_loss: 0.0813 - val_accuracy: 0.9674\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0857 - accuracy: 0.9633 - val_loss: 0.1204 - val_accuracy: 0.9608\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9665 - val_loss: 0.0947 - val_accuracy: 0.9657\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0865 - accuracy: 0.9641 - val_loss: 0.1054 - val_accuracy: 0.9657\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0940 - accuracy: 0.9612 - val_loss: 0.0821 - val_accuracy: 0.9690\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0846 - accuracy: 0.9641 - val_loss: 0.1317 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0773 - accuracy: 0.9682 - val_loss: 0.1421 - val_accuracy: 0.9576\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0958 - accuracy: 0.9584 - val_loss: 0.1490 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0880 - accuracy: 0.9653 - val_loss: 0.0884 - val_accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263d82cdd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "model= models.Sequential()\n",
    "model.add(TimeDistributed(Conv2D(2, (3, 3), strides=(1,1),activation='relu'),input_shape=(6, 28, 28, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "model.add(TimeDistributed(Conv2D(4, (3, 3), strides=(1,1),activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(8,return_sequences=False,dropout=0.2)) # used 32 units\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23f6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253731343283582 0.9528218434864658 0.9253731343283582\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5b058ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e43530db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2643d22cdf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99db6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "742d452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 6, 32)        11096       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 6, 32)        64          patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_36 (MultiH (None, 6, 32)        8416        layer_normalization_85[0][0]     \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 6, 32)        8320        layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 6, 32)        0           multi_head_attention_36[0][0]    \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "                                                                 lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 6, 32)        64          add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_52 (Sequential)      (None, 6, 32)        1056        layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 6, 32)        0           sequential_52[0][0]              \n",
      "                                                                 add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 6, 32)        64          add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_37 (MultiH (None, 6, 32)        8416        layer_normalization_87[0][0]     \n",
      "                                                                 layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 6, 32)        8320        layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 6, 32)        0           multi_head_attention_37[0][0]    \n",
      "                                                                 add_81[0][0]                     \n",
      "                                                                 lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 6, 32)        64          add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_53 (Sequential)      (None, 6, 32)        1056        layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 6, 32)        0           sequential_53[0][0]              \n",
      "                                                                 add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 6, 32)        64          add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 47,066\n",
      "Trainable params: 47,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 45s 182ms/step - loss: 0.3119 - accuracy: 0.8730 - val_loss: 0.1708 - val_accuracy: 0.9331\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 9s 119ms/step - loss: 0.1945 - accuracy: 0.9253 - val_loss: 0.1656 - val_accuracy: 0.9396\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.1590 - accuracy: 0.9379 - val_loss: 0.1555 - val_accuracy: 0.9380\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 10s 137ms/step - loss: 0.1506 - accuracy: 0.9420 - val_loss: 0.2573 - val_accuracy: 0.9119\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.1288 - accuracy: 0.9535 - val_loss: 0.2039 - val_accuracy: 0.9331\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.1138 - accuracy: 0.9588 - val_loss: 0.1660 - val_accuracy: 0.9347\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 12s 150ms/step - loss: 0.1093 - accuracy: 0.9624 - val_loss: 0.1294 - val_accuracy: 0.9511\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 11s 147ms/step - loss: 0.0845 - accuracy: 0.9694 - val_loss: 0.2172 - val_accuracy: 0.9413\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0719 - accuracy: 0.9767 - val_loss: 0.1852 - val_accuracy: 0.9250\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0763 - accuracy: 0.9690 - val_loss: 0.2777 - val_accuracy: 0.9054\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0587 - accuracy: 0.9775 - val_loss: 0.3214 - val_accuracy: 0.8923\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.1472 - val_accuracy: 0.9380\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.1741 - val_accuracy: 0.9478\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0469 - accuracy: 0.9829 - val_loss: 0.4499 - val_accuracy: 0.8825\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0551 - accuracy: 0.9792 - val_loss: 0.2270 - val_accuracy: 0.9380\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 13s 164ms/step - loss: 0.0378 - accuracy: 0.9869 - val_loss: 0.2365 - val_accuracy: 0.9331\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 11s 141ms/step - loss: 0.0393 - accuracy: 0.9865 - val_loss: 0.3281 - val_accuracy: 0.9070\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 13s 166ms/step - loss: 0.0336 - accuracy: 0.9886 - val_loss: 0.5056 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 12s 151ms/step - loss: 0.0563 - accuracy: 0.9804 - val_loss: 0.2307 - val_accuracy: 0.9299\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0328 - accuracy: 0.9886 - val_loss: 0.2689 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0313 - accuracy: 0.9886 - val_loss: 0.0974 - val_accuracy: 0.9674\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0393 - accuracy: 0.9853 - val_loss: 0.1747 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0449 - accuracy: 0.9853 - val_loss: 0.1142 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0167 - accuracy: 0.9931 - val_loss: 0.4680 - val_accuracy: 0.9021\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0479 - accuracy: 0.9816 - val_loss: 0.1475 - val_accuracy: 0.9608\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 0.0168 - accuracy: 0.9931 - val_loss: 0.2110 - val_accuracy: 0.9511\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.3497 - val_accuracy: 0.9282\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.3747 - val_accuracy: 0.9233\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.3154 - val_accuracy: 0.9380\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 0.2614 - val_accuracy: 0.9396\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3280 - val_accuracy: 0.9331\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.3375 - val_accuracy: 0.9347\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.4055 - val_accuracy: 0.9299\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0205 - accuracy: 0.9955 - val_loss: 0.1498 - val_accuracy: 0.9576\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0625 - accuracy: 0.9788 - val_loss: 0.3064 - val_accuracy: 0.9266\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.2326 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.3434 - val_accuracy: 0.9364\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3835 - val_accuracy: 0.9299\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0364 - accuracy: 0.9857 - val_loss: 0.4883 - val_accuracy: 0.8793\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 10s 132ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.2401 - val_accuracy: 0.9445\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9396\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.9396\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 8.8423e-04 - accuracy: 1.0000 - val_loss: 0.3626 - val_accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.3936 - val_accuracy: 0.9299\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3034 - val_accuracy: 0.9478\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.1240 - accuracy: 0.9608 - val_loss: 0.1293 - val_accuracy: 0.9592\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.2322 - val_accuracy: 0.9429\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1845 - val_accuracy: 0.9641\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0163 - accuracy: 0.9935 - val_loss: 0.2686 - val_accuracy: 0.9445\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.2194 - val_accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9396\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.6788e-04 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.9429\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.2790 - val_accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2257 - val_accuracy: 0.9576\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0542 - accuracy: 0.9812 - val_loss: 0.1926 - val_accuracy: 0.9543\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0236 - accuracy: 0.9890 - val_loss: 0.2859 - val_accuracy: 0.9413\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2832 - val_accuracy: 0.9494\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 11s 148ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.3047 - val_accuracy: 0.9413\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2930 - val_accuracy: 0.9511\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 6.1541e-04 - accuracy: 1.0000 - val_loss: 0.3011 - val_accuracy: 0.9527\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 6.8710e-04 - accuracy: 1.0000 - val_loss: 0.3303 - val_accuracy: 0.9478\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.9433e-04 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9462\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 2.6167e-04 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9445\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 2.9263e-04 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9478\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.7604e-04 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9462\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 2.8471e-04 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9478\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 1.3727e-04 - accuracy: 1.0000 - val_loss: 0.3786 - val_accuracy: 0.9445\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.4250 - val_accuracy: 0.9331\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.2091e-04 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 0.9494\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0477 - accuracy: 0.9873 - val_loss: 0.2780 - val_accuracy: 0.9527\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.2241 - val_accuracy: 0.9478\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.3305 - val_accuracy: 0.9364\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 0.3235 - val_accuracy: 0.9478\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 0.9478\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 9.5974e-04 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 10s 133ms/step - loss: 2.8283e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9494\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 2.5996e-04 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9445\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 1.1576e-04 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 0.9429\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.1305e-04 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.9445\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 9.8853e-05 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9462\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.3591e-04 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.6559e-04 - accuracy: 1.0000 - val_loss: 0.3077 - val_accuracy: 0.9543\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 0.2865 - val_accuracy: 0.9315\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0272 - accuracy: 0.9886 - val_loss: 0.5347 - val_accuracy: 0.8842\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0279 - accuracy: 0.9918 - val_loss: 0.2224 - val_accuracy: 0.9462\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.2061 - val_accuracy: 0.9527\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2713 - val_accuracy: 0.9478\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 9.6324e-04 - accuracy: 0.9996 - val_loss: 0.3461 - val_accuracy: 0.9429\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.2564e-04 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9445\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 5.9142e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9462\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 3.6335e-04 - accuracy: 1.0000 - val_loss: 0.3490 - val_accuracy: 0.9462\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 1.9334e-04 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9478\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.6028e-04 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.9429\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.2040e-04 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.9462\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 1.7867e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9494\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 1.2447e-04 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.9478\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.7745e-05 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 0.9445\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 9.9591e-05 - accuracy: 1.0000 - val_loss: 0.4102 - val_accuracy: 0.9445\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 8.3640e-05 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.6141e-05 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2643d486f40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2545595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2643d2d6f10>,\n",
       " <__main__.PatchEncoder at 0x2643d2d67f0>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d2d6a60>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2644020d220>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2640c307fd0>,\n",
       " <keras.layers.merge.Add at 0x2643d33f850>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2640b258eb0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d28c8b0>,\n",
       " <keras.layers.merge.Add at 0x2644020e730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d27bd00>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2643d3004f0>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2643d28c820>,\n",
       " <keras.layers.merge.Add at 0x2643d3b9970>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d362dc0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d40a670>,\n",
       " <keras.layers.merge.Add at 0x2643d27b730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d3002e0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling1D at 0x2643d3d6a30>,\n",
       " <keras.layers.core.Dense at 0x2643d42e9d0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5724c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000264554A5310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "attn = Model(inputs=model.input, outputs = model.layers[10].output)\n",
    "\n",
    "pred = attn.predict(test_df[:2], steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41a95b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.4132207 , -1.2195096 ,  0.8252611 ,  0.2068735 ,\n",
       "         -1.9345852 , -0.3024784 ,  0.68288636,  0.24726246,\n",
       "          1.4878579 , -0.38394576,  1.9602623 , -1.2115494 ,\n",
       "          0.7494351 , -1.2607741 ,  1.5064203 ,  0.4506906 ,\n",
       "         -0.10477968,  0.67580473,  0.11815078,  0.5772801 ,\n",
       "         -0.5792286 , -2.167793  ,  0.25299546,  0.17493363,\n",
       "         -1.3997712 ,  0.21966185, -0.42484006,  0.366758  ,\n",
       "         -0.8750425 , -0.6389731 , -1.5038117 ,  0.5427528 ],\n",
       "        [-0.49964952, -1.3981657 ,  0.9291378 ,  0.33286896,\n",
       "         -2.1367726 , -0.33195415,  0.7276804 ,  0.17592141,\n",
       "          1.6696632 , -0.3734444 ,  2.1794388 , -1.3467876 ,\n",
       "          0.8988415 , -1.5041916 ,  1.6590377 ,  0.6519078 ,\n",
       "         -0.22443868,  0.88728726,  0.08924537,  0.5581587 ,\n",
       "         -0.6181777 , -2.513119  ,  0.44085464,  0.16715612,\n",
       "         -1.6275524 ,  0.24514648, -0.36834878,  0.35315546,\n",
       "         -1.0476707 , -0.6746873 , -1.7237133 ,  0.67369545],\n",
       "        [-0.5054516 , -1.4051408 ,  0.931937  ,  0.33905593,\n",
       "         -2.1503923 , -0.32643858,  0.72040987,  0.16062592,\n",
       "          1.6761467 , -0.37332964,  2.192372  , -1.3548307 ,\n",
       "          0.91177434, -1.5271318 ,  1.674906  ,  0.67372906,\n",
       "         -0.23302507,  0.90983194,  0.08345299,  0.550993  ,\n",
       "         -0.6208706 , -2.5409727 ,  0.45941108,  0.17093034,\n",
       "         -1.6397773 ,  0.24939248, -0.35014296,  0.3488802 ,\n",
       "         -1.0668329 , -0.6731022 , -1.7361139 ,  0.685371  ],\n",
       "        [-0.45331264, -1.306415  ,  0.8866546 ,  0.25975394,\n",
       "         -2.0246832 , -0.34005788,  0.7407966 ,  0.26629165,\n",
       "          1.5913628 , -0.38598648,  2.0556822 , -1.2800528 ,\n",
       "          0.79983205, -1.3376297 ,  1.550189  ,  0.49013677,\n",
       "         -0.15583493,  0.7203113 ,  0.11949462,  0.58744025,\n",
       "         -0.58959144, -2.295015  ,  0.2989272 ,  0.1614392 ,\n",
       "         -1.4926645 ,  0.22279164, -0.45505288,  0.36787757,\n",
       "         -0.9085372 , -0.6712196 , -1.6083722 ,  0.58423537],\n",
       "        [-0.46676803, -1.3375354 ,  0.89921296,  0.2818547 ,\n",
       "         -2.0564473 , -0.3375323 ,  0.72806275,  0.22456515,\n",
       "          1.6079518 , -0.3822405 ,  2.0922425 , -1.2935742 ,\n",
       "          0.83231705, -1.3891375 ,  1.5930897 ,  0.543874  ,\n",
       "         -0.16311598,  0.7822279 ,  0.11047427,  0.5792245 ,\n",
       "         -0.6069935 , -2.3648562 ,  0.34867156,  0.16283749,\n",
       "         -1.5390068 ,  0.23143686, -0.41973758,  0.36701533,\n",
       "         -0.9627736 , -0.66932124, -1.6384935 ,  0.61658263],\n",
       "        [-0.47220904, -1.3485214 ,  0.9062314 ,  0.28910938,\n",
       "         -2.0689666 , -0.34121707,  0.73520386,  0.22686623,\n",
       "          1.6213392 , -0.38222903,  2.104739  , -1.3030748 ,\n",
       "          0.8397041 , -1.4010637 ,  1.5984538 ,  0.551494  ,\n",
       "         -0.17236249,  0.7889492 ,  0.10953354,  0.57948005,\n",
       "         -0.60737693, -2.3821776 ,  0.35537735,  0.16158032,\n",
       "         -1.5513701 ,  0.2319126 , -0.42227972,  0.36642838,\n",
       "         -0.967269  , -0.67334425, -1.6527948 ,  0.6217404 ]],\n",
       "\n",
       "       [[-0.42649883, -1.07714   ,  1.1484387 ,  0.08490907,\n",
       "         -1.6851516 , -0.46723652,  0.5061661 , -0.5605585 ,\n",
       "          1.0461252 , -0.49998504,  1.0360165 , -0.6946146 ,\n",
       "          0.97356695, -1.060988  ,  1.7463284 ,  0.32441568,\n",
       "          0.33397505,  1.1660689 ,  0.3562874 ,  0.45945892,\n",
       "         -0.47635746, -1.5587056 ,  0.7094144 , -0.17435731,\n",
       "         -1.5678124 ,  0.21163751,  0.17598131,  0.34968036,\n",
       "         -1.2812881 , -0.66485435, -1.0910249 ,  0.6593576 ],\n",
       "        [-0.42786318, -1.0868777 ,  1.1333342 ,  0.09772161,\n",
       "         -1.6886925 , -0.47518152,  0.5093493 , -0.56699693,\n",
       "          1.0518668 , -0.47610977,  1.0434015 , -0.7050921 ,\n",
       "          0.9808329 , -1.0820138 ,  1.7231802 ,  0.33740744,\n",
       "          0.35213426,  1.1791012 ,  0.36592385,  0.45341754,\n",
       "         -0.47471106, -1.5531352 ,  0.7377216 , -0.1596379 ,\n",
       "         -1.5623512 ,  0.20603526,  0.16128173,  0.33179644,\n",
       "         -1.2726437 , -0.669748  , -1.0993601 ,  0.6809811 ],\n",
       "        [-0.46682185, -1.154356  ,  1.1586797 ,  0.13679314,\n",
       "         -1.6567787 , -0.49478176,  0.52970284, -0.6290013 ,\n",
       "          1.0179622 , -0.5025515 ,  0.99508554, -0.7152843 ,\n",
       "          0.97520405, -1.1048411 ,  1.7275623 ,  0.32937735,\n",
       "          0.36986497,  1.1806638 ,  0.43615305,  0.43307212,\n",
       "         -0.48267272, -1.4823427 ,  0.75971186, -0.19307967,\n",
       "         -1.5393314 ,  0.22102971,  0.18088883,  0.34285673,\n",
       "         -1.2822428 , -0.71883744, -1.0900193 ,  0.6960212 ],\n",
       "        [-0.4363001 , -1.0829927 ,  1.1493462 ,  0.0923662 ,\n",
       "         -1.6651357 , -0.48290762,  0.5026756 , -0.5942865 ,\n",
       "          1.0257357 , -0.49765265,  0.9899746 , -0.69200104,\n",
       "          0.9832234 , -1.0667776 ,  1.7409017 ,  0.3290018 ,\n",
       "          0.35922012,  1.1847463 ,  0.38116938,  0.435992  ,\n",
       "         -0.4662827 , -1.517116  ,  0.7349468 , -0.183203  ,\n",
       "         -1.5593985 ,  0.21721348,  0.19318488,  0.33741537,\n",
       "         -1.2820184 , -0.67617387, -1.0793917 ,  0.6741583 ],\n",
       "        [-0.49830765, -1.1531427 ,  1.2187309 ,  0.10719223,\n",
       "         -1.587356  , -0.54115945,  0.49432966, -0.7039962 ,\n",
       "          0.9452714 , -0.57704806,  0.8339936 , -0.6744776 ,\n",
       "          0.9837212 , -1.0368228 ,  1.7723382 ,  0.28157845,\n",
       "          0.40617546,  1.2058271 ,  0.47497472,  0.3932675 ,\n",
       "         -0.44723678, -1.3788327 ,  0.7450216 , -0.2586214 ,\n",
       "         -1.5238549 ,  0.25344393,  0.27242252,  0.34878147,\n",
       "         -1.2884816 , -0.73330414, -1.0293506 ,  0.6805313 ],\n",
       "        [-0.4841547 , -1.1146897 ,  1.2149087 ,  0.08220633,\n",
       "         -1.5884408 , -0.53472805,  0.4779762 , -0.6898922 ,\n",
       "          0.942987  , -0.5763631 ,  0.8229502 , -0.6605422 ,\n",
       "          0.9893767 , -1.0167508 ,  1.7826542 ,  0.28118014,\n",
       "          0.4056453 ,  1.2089812 ,  0.44735393,  0.39052385,\n",
       "         -0.43789554, -1.3930148 ,  0.73019046, -0.25510353,\n",
       "         -1.5354162 ,  0.25418186,  0.28484488,  0.3454959 ,\n",
       "         -1.2892004 , -0.71183616, -1.0225661 ,  0.6692119 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0c239b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4260804453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[:3]\n",
    "y = attention_layer.predict(test_df, test_df, return_attention_scores=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e844434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51cf7614",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected input 0 to have rank 3 but got: 2 [Op:Einsum]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4266437127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_attention_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take one sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridspec_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\multi_head_attention.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, value, key, attention_mask, return_attention_scores, training)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;31m#   H = `size_per_head`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# `query` = [B, T, N ,H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;31m# `key` = [B, S, N, H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\einsum_dense.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7186\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected input 0 to have rank 3 but got: 2 [Op:Einsum]"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[10]\n",
    "_, attention_scores = attention_layer(Y_test[:1], test_df[:1], return_attention_scores=True) # take one sample\n",
    "fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[5,5,0.2]))\n",
    "sb.heatmap(attention_scores[0, 0, :, :], annot=True, cbar=False, ax=axs[0])\n",
    "sb.heatmap(attention_scores[0, 1, :, :], annot=True, yticklabels=False, cbar=False, ax=axs[1])\n",
    "fig.colorbar(axs[1].collections[0], cax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b30013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196759ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250da400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
