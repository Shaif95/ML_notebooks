{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acba090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ffa13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "del tr1,tr2,tr3\n",
    "\n",
    "trn1='D:/data/invasive-aquatic-species-data/noninvasive/*/'\n",
    "trn2='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Ostracod Image1/*/'\n",
    "trn3='D:/data/invasive-aquatic-species-data/invasive/*/'\n",
    "\n",
    "tr1= glob(trn1)\n",
    "tr2= glob(trn2)\n",
    "tr3= glob(trn3)\n",
    "\n",
    "trn11='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Not Veligers/*/'\n",
    "trn33='D:/data/Veligers/To Baylor 2023-01-30/To Baylor 2023-01-30/Zebra Pediveliger Image1a/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "#tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trn11='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Not/*/'\n",
    "trn33='D:/data/Veligers/Preserved Zebra Ped 1 To Baylor/Preserved Zebra Ped 1 To Baylor/Sorted Images/Pedi-Zebra Veligers/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "#tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trn11='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Not/*/'\n",
    "trn33='D:/data/Veligers/Preserved Zebra Ped 1a To Baylor/Preserved Zebra Ped 1a To Baylor/Sorted Images/Preserved Zebra Ped 1a/*/'\n",
    "tr11= glob(trn11)\n",
    "tr33= glob(trn33)\n",
    "#tr1.extend(tr11)\n",
    "tr3.extend(tr33)\n",
    "\n",
    "\n",
    "trn111='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/NonVeligers/Images_001/*/'\n",
    "trn333='D:/data/Veligers/Baylor 2022-03-21/Baylor 2022-03-21/Davis Dam 2019-07-24/Manually Reviewed/Veligers/Images_001/*/'\n",
    "tr111= glob(trn111)\n",
    "tr333= glob(trn333)\n",
    "tr1.extend(tr111)\n",
    "tr3.extend(tr333)\n",
    "\n",
    "trnl1='D:/data/Ostracod/Ostracod Day 2 Image12 Short To Baylor/Ostracod Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl2='D:/data/Ostracod/Ostracods Day 2 Image1 To Baylor/Ostracods Day 2 Image1 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl3='D:/data/Ostracod/Ostracods Day 2 Image2 To Baylor/Ostracods Day 2 Image2 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl4='D:/data/Ostracod/Ostracods Day 2 Image3 To Baylor/Ostracods Day 2 Image3 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl5='D:/data/Ostracod/Ostracods Day 2 Image12 To Baylor/Ostracods Day 2 Image12 To Baylor/Sorted Images/Ostracods/*/'\n",
    "trnl6='D:/data/Ostracod/Preserved Ostracods 1 To Baylor/Preserved Ostracods 1 To Baylor/Sorted Images/Preserve Ostracods/*/'\n",
    "trnl7='D:/data/Ostracod/Preserved Ostracods 1a To Baylor/Preserved Ostracods 1a To Baylor/Sorted Images/Preserved Ostracods 1a/*/'\n",
    "\n",
    "trl1= glob(trnl1)\n",
    "trl2= glob(trnl2)\n",
    "trl3= glob(trnl3)\n",
    "trl4= glob(trnl4)\n",
    "trl5= glob(trnl5)\n",
    "trl6= glob(trnl6)\n",
    "trl7= glob(trnl7)\n",
    "\n",
    "\n",
    "tr2.extend(trl1)\n",
    "tr2.extend(trl2)\n",
    "tr2.extend(trl3)\n",
    "tr2.extend(trl4)\n",
    "tr2.extend(trl5)\n",
    "tr2.extend(trl6)\n",
    "tr2.extend(trl7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf0822e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeab214c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e8a504b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3317.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tr1= shuffle(tr1)\n",
    "tr2= shuffle(tr2)\n",
    "tr3= shuffle(tr3)\n",
    "\n",
    "tran_index_noninv = np.round( len(tr1)* .7  )\n",
    "tran_index_osc = np.round( len(tr2)* .7  )\n",
    "tran_index_inv = np.round( len(tr3)* .7  )\n",
    "tran_index_noninv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981f5942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b9c51980",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[:(int) (tran_index_inv)]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[:(int) (tran_index_noninv)]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr2[:(int) (tran_index_osc)]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in range(0,len(tr3[:(int) (tran_index_inv)])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[:(int) (tran_index_noninv)])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])        \n",
    "\n",
    "for j in range(0,len(tr2[:(int) (tran_index_osc)])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k]) \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 28, 28)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_train = idata\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_train = np.reshape(X_train, (len(X_train),28,28,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "079c6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4287, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "train_df= []\n",
    "breath = 5\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*5+k)\n",
    "        \n",
    "        deff.append(X_train[index])\n",
    "        \n",
    "    train_df.append(deff)\n",
    "\n",
    "Y_train = to_categorical(label)\n",
    "train_df = np.array(train_df)\n",
    "YY_Train = label\n",
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "104123f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24d262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "label = []\n",
    "breath = []\n",
    "total = 0\n",
    "\n",
    "for j in tr3[(int) (tran_index_inv) + 1 :]:\n",
    "    label.append(1)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a))\n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in tr1[ (int)(tran_index_noninv) + 1:]:\n",
    "    label.append(0)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "\n",
    "for j in tr2[ (int)(tran_index_osc) + 1:]:\n",
    "    label.append(2)\n",
    "    a = glob(j+'/*')\n",
    "    breath.append(len(a)) \n",
    "    total = total + len(a)\n",
    "    \n",
    "for j in range(0,len(tr3[(int) (tran_index_inv) + 1 :])):\n",
    "    a = glob(tr3[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])\n",
    "\n",
    "for j in range(0,len(tr1[ (int)(tran_index_noninv) + 1:])):\n",
    "    a = glob(tr1[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  \n",
    "        \n",
    "for j in range(0,len(tr2[ (int)(tran_index_osc) + 1:])):\n",
    "    a = glob(tr2[j]+'/*')\n",
    "    for k in range(0,5):\n",
    "        data.append(a[k])  \n",
    "        \n",
    "\n",
    "imgdata=[]\n",
    "for i in range(len(data)):\n",
    "    a = Image.open(data[i])\n",
    "    b = tf.image.resize_with_crop_or_pad(tf.keras.preprocessing.image.img_to_array(a), 28, 28)\n",
    "    c = np.array(b)\n",
    "    imgdata.append(c.reshape(28,28,3))\n",
    "    \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "idata = np.array(imgdata)\n",
    "X_test = idata\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "X_test = np.reshape(X_test, (len(X_test),28,28,3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "661c2759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1835, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end= 0\n",
    "test_df= []\n",
    "breath = 5\n",
    "\n",
    "i = 0\n",
    "for i in range(0, len(label)):\n",
    "    deff = []\n",
    "    for k in range(0, (breath)):\n",
    "        \n",
    "        index = (i*5 + k)\n",
    "        \n",
    "        deff.append(X_test[index])\n",
    "        \n",
    "    test_df.append(deff)\n",
    "    \n",
    "Y_test = to_categorical(label)\n",
    "test_df = np.array(test_df)\n",
    "YY_Test = label\n",
    "np.shape(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56c507ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52b5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = ( 5, 28, 28, 3 )\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 32\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae32102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88acaa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c674b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6123, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31947b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc77872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6, 2352)]    0           []                               \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 6, 32)        75488       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 6, 32)       64          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 32)       25184       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 6, 32)        8320        ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6, 32)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]',          \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 6, 32)       64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 32)        0           ['sequential_1[0][0]',           \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 32)       64          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 32)       25184       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 32)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]',                  \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 32)       64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6, 32)        0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 32)       64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 6, 32)       25184       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 6, 32)        0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]',                  \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 6, 32)       64          ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 6, 32)        0           ['sequential_3[0][0]',           \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 6, 32)       64          ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 6, 32)       25184       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 6, 32)        0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]',                  \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 6, 32)       64          ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 6, 32)        0           ['sequential_4[0][0]',           \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 6, 32)       64          ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 6, 32)       25184       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 6, 32)        0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]',                  \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 6, 32)       64          ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 6, 32)        0           ['sequential_5[0][0]',           \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 6, 32)       64          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 6, 32)       25184       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 6, 32)        8320        ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 6, 32)        0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]',                  \n",
      "                                                                  'lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 6, 32)       64          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 6, 32)        0           ['sequential_6[0][0]',           \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 6, 32)       64          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_12[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 2)            66          ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 283,746\n",
      "Trainable params: 283,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c833aaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_df = train_df.reshape( train_df.shape[0] , train_df.shape[1],( train_df.shape[2] * train_df.shape[3] * 3)  )\n",
    "#tt_df = test_df.reshape(test_df.shape[0] ,test_df.shape[1],( test_df.shape[2] * test_df.shape[3] * 3)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e83af007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_84197]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1388\\2042756921.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[model/lstm/PartitionedCall]] [Op:__inference_train_function_84197]\n\nFunction call stack:\ntrain_function -> train_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3abe3d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1103,    0],\n",
       "       [   6,  201]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "786c9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852941176470589 0.9972801450589301 0.9710144927536232\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87100053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 6, 2352)]    0           []                               \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 6, 32)        75488       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 6, 32)       64          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 6, 32)       8416        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 6, 32)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 6, 32)       64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 6, 32)        0           ['sequential_1[0][0]',           \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 6, 32)       64          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 6, 32)       8416        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 6, 32)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 6, 32)       64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 6, 32)        1056        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 6, 32)        0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 6, 32)       64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_4[0][0]']  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            66          ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 94,818\n",
      "Trainable params: 94,818\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tra_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14576\\1200473485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtra_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tra_df' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7ea818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995049504950495 0.999096657633243 0.9901477832512315\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dd475d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 6, 2352)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 6, 32)        75488       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 6, 32)        64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention (MultiHead (None, 6, 32)        50336       layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 6, 32)        0           multi_head_attention[0][0]       \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 6, 32)        64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 6, 32)        1056        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 6, 32)        0           sequential_13[0][0]              \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 6, 32)        64          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_1 (MultiHe (None, 6, 32)        50336       layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 6, 32)        0           multi_head_attention_1[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 6, 32)        64          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 6, 32)        1056        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 6, 32)        0           sequential_14[0][0]              \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 6, 32)        64          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_2 (MultiHe (None, 6, 32)        50336       layer_normalization_14[0][0]     \n",
      "                                                                 layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 6, 32)        0           multi_head_attention_2[0][0]     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, 6, 32)        64          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_15 (Sequential)      (None, 6, 32)        1056        layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 6, 32)        0           sequential_15[0][0]              \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, 6, 32)        64          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_3 (MultiHe (None, 6, 32)        50336       layer_normalization_16[0][0]     \n",
      "                                                                 layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 6, 32)        0           multi_head_attention_3[0][0]     \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, 6, 32)        64          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_16 (Sequential)      (None, 6, 32)        1056        layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 6, 32)        0           sequential_16[0][0]              \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, 6, 32)        64          add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 6, 32)        50336       layer_normalization_18[0][0]     \n",
      "                                                                 layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 6, 32)        0           multi_head_attention_4[0][0]     \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, 6, 32)        64          add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_17 (Sequential)      (None, 6, 32)        1056        layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 6, 32)        0           sequential_17[0][0]              \n",
      "                                                                 add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, 6, 32)        64          add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 6, 32)        50336       layer_normalization_20[0][0]     \n",
      "                                                                 layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 6, 32)        0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, 6, 32)        64          add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_18 (Sequential)      (None, 6, 32)        1056        layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 6, 32)        0           sequential_18[0][0]              \n",
      "                                                                 add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, 6, 32)        64          add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_6 (MultiHe (None, 6, 32)        50336       layer_normalization_22[0][0]     \n",
      "                                                                 layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 6, 32)        0           multi_head_attention_6[0][0]     \n",
      "                                                                 add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, 6, 32)        64          add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_19 (Sequential)      (None, 6, 32)        1056        layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 32)        0           sequential_19[0][0]              \n",
      "                                                                 add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, 6, 32)        64          add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_7 (MultiHe (None, 6, 32)        50336       layer_normalization_24[0][0]     \n",
      "                                                                 layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 6, 32)        0           multi_head_attention_7[0][0]     \n",
      "                                                                 add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, 6, 32)        64          add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_20 (Sequential)      (None, 6, 32)        1056        layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 6, 32)        0           sequential_20[0][0]              \n",
      "                                                                 add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 6, 32)        64          add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_8 (MultiHe (None, 6, 32)        50336       layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 6, 32)        0           multi_head_attention_8[0][0]     \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 6, 32)        64          add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_21 (Sequential)      (None, 6, 32)        1056        layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 6, 32)        0           sequential_21[0][0]              \n",
      "                                                                 add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 6, 32)        64          add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_9 (MultiHe (None, 6, 32)        50336       layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 6, 32)        0           multi_head_attention_9[0][0]     \n",
      "                                                                 add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 6, 32)        64          add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_22 (Sequential)      (None, 6, 32)        1056        layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 6, 32)        0           sequential_22[0][0]              \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 6, 32)        64          add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_10 (MultiH (None, 6, 32)        50336       layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 6, 32)        0           multi_head_attention_10[0][0]    \n",
      "                                                                 add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 6, 32)        64          add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_23 (Sequential)      (None, 6, 32)        1056        layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 6, 32)        0           sequential_23[0][0]              \n",
      "                                                                 add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 6, 32)        64          add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_11 (MultiH (None, 6, 32)        50336       layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 6, 32)        0           multi_head_attention_11[0][0]    \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 6, 32)        64          add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_24 (Sequential)      (None, 6, 32)        1056        layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 6, 32)        0           sequential_24[0][0]              \n",
      "                                                                 add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 6, 32)        64          add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 2)            66          global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 693,858\n",
      "Trainable params: 693,858\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 23s 103ms/step - loss: 0.5219 - accuracy: 0.7946 - val_loss: 0.2336 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.3663 - accuracy: 0.8432 - val_loss: 0.4138 - val_accuracy: 0.8206\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.2030 - accuracy: 0.9151 - val_loss: 0.2323 - val_accuracy: 0.8842\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.1755 - accuracy: 0.9294 - val_loss: 0.2131 - val_accuracy: 0.9086\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1551 - accuracy: 0.9326 - val_loss: 0.1142 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1516 - accuracy: 0.9347 - val_loss: 0.0847 - val_accuracy: 0.9625\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1491 - accuracy: 0.9379 - val_loss: 0.1124 - val_accuracy: 0.9494\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1667 - accuracy: 0.9343 - val_loss: 0.1455 - val_accuracy: 0.9347\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.1355 - accuracy: 0.9461 - val_loss: 0.1634 - val_accuracy: 0.9233\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1162 - accuracy: 0.9559 - val_loss: 0.0763 - val_accuracy: 0.9674\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.1389 - accuracy: 0.9539 - val_loss: 0.0910 - val_accuracy: 0.9608\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.1209 - accuracy: 0.9522 - val_loss: 0.2704 - val_accuracy: 0.8989\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1106 - accuracy: 0.9575 - val_loss: 0.0810 - val_accuracy: 0.9608\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1089 - accuracy: 0.9563 - val_loss: 0.1549 - val_accuracy: 0.9299\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.1170 - accuracy: 0.9563 - val_loss: 0.0509 - val_accuracy: 0.9788\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.1228 - accuracy: 0.9514 - val_loss: 0.1363 - val_accuracy: 0.9396\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1019 - accuracy: 0.9604 - val_loss: 0.0798 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0915 - accuracy: 0.9653 - val_loss: 0.1441 - val_accuracy: 0.9413\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.1009 - accuracy: 0.9608 - val_loss: 0.2379 - val_accuracy: 0.9233\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0921 - accuracy: 0.9661 - val_loss: 0.1586 - val_accuracy: 0.9478\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0931 - accuracy: 0.9653 - val_loss: 0.2354 - val_accuracy: 0.9038\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0979 - accuracy: 0.9600 - val_loss: 0.0736 - val_accuracy: 0.9674\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0867 - accuracy: 0.9673 - val_loss: 0.1006 - val_accuracy: 0.9560\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0817 - accuracy: 0.9714 - val_loss: 0.0889 - val_accuracy: 0.9706\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0728 - accuracy: 0.9718 - val_loss: 0.1131 - val_accuracy: 0.9527\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0778 - accuracy: 0.9755 - val_loss: 0.1503 - val_accuracy: 0.9315\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0736 - accuracy: 0.9743 - val_loss: 0.3570 - val_accuracy: 0.8940\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0810 - accuracy: 0.9641 - val_loss: 0.3989 - val_accuracy: 0.8499\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0914 - accuracy: 0.9628 - val_loss: 0.1788 - val_accuracy: 0.9315\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.1003 - val_accuracy: 0.9674\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0675 - accuracy: 0.9722 - val_loss: 0.2169 - val_accuracy: 0.9282\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0621 - accuracy: 0.9739 - val_loss: 0.1146 - val_accuracy: 0.9511\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0536 - accuracy: 0.9767 - val_loss: 0.1284 - val_accuracy: 0.9543\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 0.1891 - val_accuracy: 0.9282\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0566 - accuracy: 0.9763 - val_loss: 0.1903 - val_accuracy: 0.9331\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0715 - accuracy: 0.9722 - val_loss: 0.3347 - val_accuracy: 0.9462\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0535 - accuracy: 0.9816 - val_loss: 0.1838 - val_accuracy: 0.9462\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0659 - accuracy: 0.9755 - val_loss: 0.2889 - val_accuracy: 0.8972\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0868 - accuracy: 0.9661 - val_loss: 0.1128 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0629 - accuracy: 0.9759 - val_loss: 0.2529 - val_accuracy: 0.9413\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0442 - accuracy: 0.9829 - val_loss: 0.0514 - val_accuracy: 0.9821\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0566 - accuracy: 0.9755 - val_loss: 0.2314 - val_accuracy: 0.9413\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0364 - accuracy: 0.9878 - val_loss: 0.2113 - val_accuracy: 0.9315\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0368 - accuracy: 0.9861 - val_loss: 0.3788 - val_accuracy: 0.8874\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.1981 - val_accuracy: 0.9347\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 0.0533 - accuracy: 0.9792 - val_loss: 0.1110 - val_accuracy: 0.9641\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0373 - accuracy: 0.9861 - val_loss: 0.1416 - val_accuracy: 0.9576\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0775 - accuracy: 0.9710 - val_loss: 0.1295 - val_accuracy: 0.9511\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0339 - accuracy: 0.9853 - val_loss: 0.2037 - val_accuracy: 0.9478\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0259 - accuracy: 0.9890 - val_loss: 0.1323 - val_accuracy: 0.9625\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0455 - accuracy: 0.9849 - val_loss: 0.0739 - val_accuracy: 0.9723\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0415 - accuracy: 0.9833 - val_loss: 0.1021 - val_accuracy: 0.9608\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0333 - accuracy: 0.9857 - val_loss: 0.1161 - val_accuracy: 0.9560\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0295 - accuracy: 0.9878 - val_loss: 0.1552 - val_accuracy: 0.9560\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.2156 - val_accuracy: 0.9608\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0389 - accuracy: 0.9849 - val_loss: 0.2261 - val_accuracy: 0.9527\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0262 - accuracy: 0.9894 - val_loss: 0.3019 - val_accuracy: 0.9364\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0304 - accuracy: 0.9853 - val_loss: 0.2947 - val_accuracy: 0.9299\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.3137 - val_accuracy: 0.9396\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0398 - accuracy: 0.9849 - val_loss: 0.1475 - val_accuracy: 0.9576\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.1855 - val_accuracy: 0.9445\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0304 - accuracy: 0.9869 - val_loss: 0.1199 - val_accuracy: 0.9706\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0410 - accuracy: 0.9837 - val_loss: 0.2752 - val_accuracy: 0.9445\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0123 - accuracy: 0.9967 - val_loss: 0.2091 - val_accuracy: 0.9608\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0153 - accuracy: 0.9939 - val_loss: 0.2158 - val_accuracy: 0.9592\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0369 - accuracy: 0.9878 - val_loss: 0.2110 - val_accuracy: 0.9462\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.1017 - accuracy: 0.9641 - val_loss: 0.1465 - val_accuracy: 0.9560\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0837 - accuracy: 0.9690 - val_loss: 0.1615 - val_accuracy: 0.9560\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0396 - accuracy: 0.9873 - val_loss: 0.1601 - val_accuracy: 0.9560\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0287 - accuracy: 0.9902 - val_loss: 0.1442 - val_accuracy: 0.9592\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0739 - accuracy: 0.9718 - val_loss: 0.1142 - val_accuracy: 0.9723\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0166 - accuracy: 0.9931 - val_loss: 0.1481 - val_accuracy: 0.9657\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0219 - accuracy: 0.9914 - val_loss: 0.1104 - val_accuracy: 0.9739\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.2892 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.1324 - val_accuracy: 0.9625\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0399 - accuracy: 0.9857 - val_loss: 0.2416 - val_accuracy: 0.9266\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0546 - accuracy: 0.9812 - val_loss: 0.1394 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0190 - accuracy: 0.9935 - val_loss: 0.1475 - val_accuracy: 0.9690\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0166 - accuracy: 0.9943 - val_loss: 0.1367 - val_accuracy: 0.9739\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 7s 86ms/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.2944 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.4960 - val_accuracy: 0.9184\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 7s 84ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.2138 - val_accuracy: 0.9462\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0156 - accuracy: 0.9943 - val_loss: 0.1937 - val_accuracy: 0.9592\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0141 - accuracy: 0.9959 - val_loss: 0.2368 - val_accuracy: 0.9413\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.1489 - val_accuracy: 0.9641\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0144 - accuracy: 0.9943 - val_loss: 0.1541 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0129 - accuracy: 0.9939 - val_loss: 0.2583 - val_accuracy: 0.9576\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 7s 89ms/step - loss: 0.0124 - accuracy: 0.9955 - val_loss: 0.1197 - val_accuracy: 0.9739\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.2260 - val_accuracy: 0.9608\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 6.6339e-04 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9625\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 6s 85ms/step - loss: 2.4115e-04 - accuracy: 1.0000 - val_loss: 0.2826 - val_accuracy: 0.9576\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 7s 85ms/step - loss: 2.8148e-04 - accuracy: 1.0000 - val_loss: 0.2401 - val_accuracy: 0.9657\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 2.0978e-04 - accuracy: 1.0000 - val_loss: 0.3223 - val_accuracy: 0.9511\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0210 - accuracy: 0.9922 - val_loss: 0.1025 - val_accuracy: 0.9625\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.2442 - val_accuracy: 0.9576\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0086 - accuracy: 0.9963 - val_loss: 0.3115 - val_accuracy: 0.9429\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 0.9608\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.2760 - val_accuracy: 0.9625\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.0537 - accuracy: 0.9792 - val_loss: 0.1679 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263c237ab50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,2352) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(12):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output =layers.MultiHeadAttention (  num_heads=12, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    #lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(tra_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c6c53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824561403508771 0.9863415555924266 0.98989898989899\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(tt_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325fe64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a6163a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26057200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5, 28, 28,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " patch_encoder_3 (PatchEncoder)  (None, 5, 32)       11064       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_39 (LayerN  (None, 5, 32)       64          ['patch_encoder_3[0][0]']        \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_18 (Multi  (None, 5, 32)       25184       ['layer_normalization_39[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_18 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 5, 32)        0           ['multi_head_attention_18[0][0]',\n",
      "                                                                  'patch_encoder_3[0][0]',        \n",
      "                                                                  'lstm_18[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_40 (LayerN  (None, 5, 32)       64          ['add_36[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_22 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 5, 32)        0           ['sequential_22[0][0]',          \n",
      "                                                                  'add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_41 (LayerN  (None, 5, 32)       64          ['add_37[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_19 (Multi  (None, 5, 32)       25184       ['layer_normalization_41[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_19 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 5, 32)        0           ['multi_head_attention_19[0][0]',\n",
      "                                                                  'add_37[0][0]',                 \n",
      "                                                                  'lstm_19[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_42 (LayerN  (None, 5, 32)       64          ['add_38[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_23 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 5, 32)        0           ['sequential_23[0][0]',          \n",
      "                                                                  'add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_43 (LayerN  (None, 5, 32)       64          ['add_39[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_20 (Multi  (None, 5, 32)       25184       ['layer_normalization_43[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_20 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 5, 32)        0           ['multi_head_attention_20[0][0]',\n",
      "                                                                  'add_39[0][0]',                 \n",
      "                                                                  'lstm_20[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_44 (LayerN  (None, 5, 32)       64          ['add_40[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_24 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 5, 32)        0           ['sequential_24[0][0]',          \n",
      "                                                                  'add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_45 (LayerN  (None, 5, 32)       64          ['add_41[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_21 (Multi  (None, 5, 32)       25184       ['layer_normalization_45[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_21 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 5, 32)        0           ['multi_head_attention_21[0][0]',\n",
      "                                                                  'add_41[0][0]',                 \n",
      "                                                                  'lstm_21[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_46 (LayerN  (None, 5, 32)       64          ['add_42[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_25 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 5, 32)        0           ['sequential_25[0][0]',          \n",
      "                                                                  'add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_47 (LayerN  (None, 5, 32)       64          ['add_43[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (Multi  (None, 5, 32)       25184       ['layer_normalization_47[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_22 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 5, 32)        0           ['multi_head_attention_22[0][0]',\n",
      "                                                                  'add_43[0][0]',                 \n",
      "                                                                  'lstm_22[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_48 (LayerN  (None, 5, 32)       64          ['add_44[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_26 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, 5, 32)        0           ['sequential_26[0][0]',          \n",
      "                                                                  'add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_49 (LayerN  (None, 5, 32)       64          ['add_45[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_23 (Multi  (None, 5, 32)       25184       ['layer_normalization_49[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_23 (LSTM)                 (None, 5, 32)        8320        ['layer_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, 5, 32)        0           ['multi_head_attention_23[0][0]',\n",
      "                                                                  'add_45[0][0]',                 \n",
      "                                                                  'lstm_23[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_50 (LayerN  (None, 5, 32)       64          ['add_46[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_27 (Sequential)     (None, 5, 32)        1056        ['layer_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 5, 32)        0           ['sequential_27[0][0]',          \n",
      "                                                                  'add_46[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_51 (LayerN  (None, 5, 32)       64          ['add_47[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_3 (Gl  (None, 32)          0           ['layer_normalization_51[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3)            99          ['global_average_pooling1d_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 219,355\n",
      "Trainable params: 219,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (5,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(5, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=3, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262812d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2676f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "108/108 [==============================] - 32s 113ms/step - loss: 0.0759 - accuracy: 0.9787 - val_loss: 1.5787 - val_accuracy: 0.8951\n",
      "Epoch 2/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0390 - accuracy: 0.9878 - val_loss: 1.5695 - val_accuracy: 0.9044\n",
      "Epoch 3/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0276 - accuracy: 0.9918 - val_loss: 1.5558 - val_accuracy: 0.9056\n",
      "Epoch 4/50\n",
      "108/108 [==============================] - 9s 79ms/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 1.5367 - val_accuracy: 0.9009\n",
      "Epoch 5/50\n",
      "108/108 [==============================] - 9s 79ms/step - loss: 0.0280 - accuracy: 0.9918 - val_loss: 1.4911 - val_accuracy: 0.8904\n",
      "Epoch 6/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0210 - accuracy: 0.9942 - val_loss: 1.3963 - val_accuracy: 0.9091\n",
      "Epoch 7/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0366 - accuracy: 0.9886 - val_loss: 1.4292 - val_accuracy: 0.8974\n",
      "Epoch 8/50\n",
      "108/108 [==============================] - 8s 78ms/step - loss: 0.0098 - accuracy: 0.9983 - val_loss: 1.4519 - val_accuracy: 0.8998\n",
      "Epoch 9/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0060 - accuracy: 0.9994 - val_loss: 1.4691 - val_accuracy: 0.9033\n",
      "Epoch 10/50\n",
      "108/108 [==============================] - 8s 78ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 1.4765 - val_accuracy: 0.9044\n",
      "Epoch 11/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0194 - accuracy: 0.9953 - val_loss: 1.4154 - val_accuracy: 0.9044\n",
      "Epoch 12/50\n",
      "108/108 [==============================] - 8s 76ms/step - loss: 0.0196 - accuracy: 0.9939 - val_loss: 1.4662 - val_accuracy: 0.9033\n",
      "Epoch 13/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0273 - accuracy: 0.9913 - val_loss: 1.5619 - val_accuracy: 0.8636\n",
      "Epoch 14/50\n",
      "108/108 [==============================] - 9s 84ms/step - loss: 0.0228 - accuracy: 0.9936 - val_loss: 1.4903 - val_accuracy: 0.9009\n",
      "Epoch 15/50\n",
      "108/108 [==============================] - 8s 76ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 1.4939 - val_accuracy: 0.9091\n",
      "Epoch 16/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0076 - accuracy: 0.9983 - val_loss: 1.4797 - val_accuracy: 0.8986\n",
      "Epoch 17/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 1.4672 - val_accuracy: 0.9021\n",
      "Epoch 18/50\n",
      "108/108 [==============================] - 8s 79ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 1.4610 - val_accuracy: 0.8998\n",
      "Epoch 19/50\n",
      "108/108 [==============================] - 8s 78ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 1.4696 - val_accuracy: 0.8998\n",
      "Epoch 20/50\n",
      "108/108 [==============================] - 9s 80ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 1.4834 - val_accuracy: 0.9009\n",
      "Epoch 21/50\n",
      "108/108 [==============================] - 9s 79ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 1.5101 - val_accuracy: 0.9009\n",
      "Epoch 22/50\n",
      "108/108 [==============================] - 8s 76ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 1.4698 - val_accuracy: 0.9009\n",
      "Epoch 23/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0202 - accuracy: 0.9927 - val_loss: 1.5053 - val_accuracy: 0.9033\n",
      "Epoch 24/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 1.5633 - val_accuracy: 0.8963\n",
      "Epoch 25/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0312 - accuracy: 0.9910 - val_loss: 1.4719 - val_accuracy: 0.8939\n",
      "Epoch 26/50\n",
      "108/108 [==============================] - 8s 73ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 1.4990 - val_accuracy: 0.8939\n",
      "Epoch 27/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0043 - accuracy: 0.9997 - val_loss: 1.4828 - val_accuracy: 0.8974\n",
      "Epoch 28/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 1.5092 - val_accuracy: 0.8998\n",
      "Epoch 29/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0127 - accuracy: 0.9968 - val_loss: 1.5750 - val_accuracy: 0.8776\n",
      "Epoch 30/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0139 - accuracy: 0.9962 - val_loss: 1.5111 - val_accuracy: 0.8986\n",
      "Epoch 31/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 1.5268 - val_accuracy: 0.8928\n",
      "Epoch 32/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 1.4717 - val_accuracy: 0.9103\n",
      "Epoch 33/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0158 - accuracy: 0.9953 - val_loss: 1.5245 - val_accuracy: 0.8939\n",
      "Epoch 34/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0171 - accuracy: 0.9948 - val_loss: 1.5704 - val_accuracy: 0.8928\n",
      "Epoch 35/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 0.0218 - accuracy: 0.9930 - val_loss: 1.5846 - val_accuracy: 0.8636\n",
      "Epoch 36/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0092 - accuracy: 0.9965 - val_loss: 1.4375 - val_accuracy: 0.9021\n",
      "Epoch 37/50\n",
      "108/108 [==============================] - 8s 73ms/step - loss: 8.9861e-04 - accuracy: 1.0000 - val_loss: 1.4590 - val_accuracy: 0.9033\n",
      "Epoch 38/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 1.4646 - val_accuracy: 0.8963\n",
      "Epoch 39/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0083 - accuracy: 0.9977 - val_loss: 1.4779 - val_accuracy: 0.8998\n",
      "Epoch 40/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 1.4985 - val_accuracy: 0.8893\n",
      "Epoch 41/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 1.4810 - val_accuracy: 0.9009\n",
      "Epoch 42/50\n",
      "108/108 [==============================] - 8s 73ms/step - loss: 6.0797e-04 - accuracy: 1.0000 - val_loss: 1.4809 - val_accuracy: 0.8974\n",
      "Epoch 43/50\n",
      "108/108 [==============================] - 8s 79ms/step - loss: 2.6375e-04 - accuracy: 1.0000 - val_loss: 1.4746 - val_accuracy: 0.8986\n",
      "Epoch 44/50\n",
      "108/108 [==============================] - 8s 78ms/step - loss: 1.7919e-04 - accuracy: 1.0000 - val_loss: 1.4860 - val_accuracy: 0.8998\n",
      "Epoch 45/50\n",
      "108/108 [==============================] - 8s 78ms/step - loss: 1.7252e-04 - accuracy: 1.0000 - val_loss: 1.5056 - val_accuracy: 0.8974\n",
      "Epoch 46/50\n",
      "108/108 [==============================] - 8s 76ms/step - loss: 9.3043e-05 - accuracy: 1.0000 - val_loss: 1.5080 - val_accuracy: 0.8986\n",
      "Epoch 47/50\n",
      "108/108 [==============================] - 8s 77ms/step - loss: 6.2619e-05 - accuracy: 1.0000 - val_loss: 1.5211 - val_accuracy: 0.8986\n",
      "Epoch 48/50\n",
      "108/108 [==============================] - 8s 74ms/step - loss: 9.9343e-05 - accuracy: 1.0000 - val_loss: 1.5313 - val_accuracy: 0.9009\n",
      "Epoch 49/50\n",
      "108/108 [==============================] - 8s 75ms/step - loss: 4.8609e-05 - accuracy: 1.0000 - val_loss: 1.5417 - val_accuracy: 0.8974\n",
      "Epoch 50/50\n",
      "108/108 [==============================] - 8s 76ms/step - loss: 4.4255e-05 - accuracy: 1.0000 - val_loss: 1.5503 - val_accuracy: 0.8974\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "history = model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "af7c1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ef7bac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.37%\n",
      "Balanced accuracy: 66.67%\n",
      "F1 score for class 1: 99.10%\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, p)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(Y_test, p)\n",
    "print(\"Balanced accuracy: {:.2f}%\".format(balanced_accuracy * 100))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for class 1\n",
    "f1_class1 = f1_score(Y_test, p, labels=[1], average='macro')\n",
    "print(\"F1 score for class 1: {:.2f}%\".format(f1_class1 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b4d89f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(Y_test, p)\n",
    "\n",
    "# Create a heatmap of the confusion matrix using seaborn\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"], yticklabels=[\"Class 1\", \"Class 2\", \"Class 3\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c36be08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514281067975519"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bb2f5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy over time\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506427c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3afd50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0cf3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( train_df,YY_Train , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cc7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(X_train , y_train , \"train\")\n",
    "validloader = prepare_dataloader(X_val, y_val, \"valid\")\n",
    "testloader = prepare_dataloader(test_df,YY_Test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0523831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e6b82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=3, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "28803bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 5, 28, 28,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " tubelet_embedding_5 (TubeletEm  (None, 9, 32)       24608       ['input_10[0][0]']               \n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " positional_encoder_5 (Position  (None, 9, 32)       288         ['tubelet_embedding_5[0][0]']    \n",
      " alEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_83 (LayerN  (None, 9, 32)       64          ['positional_encoder_5[0][0]']   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_37 (Multi  (None, 9, 32)       3962        ['layer_normalization_83[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " add_74 (Add)                   (None, 9, 32)        0           ['multi_head_attention_37[0][0]',\n",
      "                                                                  'positional_encoder_5[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_84 (LayerN  (None, 9, 32)       64          ['add_74[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_41 (Sequential)     (None, 9, 32)        8352        ['layer_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " add_75 (Add)                   (None, 9, 32)        0           ['sequential_41[0][0]',          \n",
      "                                                                  'add_74[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_85 (LayerN  (None, 9, 32)       64          ['add_75[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_38 (Multi  (None, 9, 32)       3962        ['layer_normalization_85[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " add_76 (Add)                   (None, 9, 32)        0           ['multi_head_attention_38[0][0]',\n",
      "                                                                  'add_75[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_86 (LayerN  (None, 9, 32)       64          ['add_76[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_42 (Sequential)     (None, 9, 32)        8352        ['layer_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " add_77 (Add)                   (None, 9, 32)        0           ['sequential_42[0][0]',          \n",
      "                                                                  'add_76[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_87 (LayerN  (None, 9, 32)       64          ['add_77[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_39 (Multi  (None, 9, 32)       3962        ['layer_normalization_87[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " add_78 (Add)                   (None, 9, 32)        0           ['multi_head_attention_39[0][0]',\n",
      "                                                                  'add_77[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_88 (LayerN  (None, 9, 32)       64          ['add_78[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_43 (Sequential)     (None, 9, 32)        8352        ['layer_normalization_88[0][0]'] \n",
      "                                                                                                  \n",
      " add_79 (Add)                   (None, 9, 32)        0           ['sequential_43[0][0]',          \n",
      "                                                                  'add_78[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_89 (LayerN  (None, 9, 32)       64          ['add_79[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_40 (Multi  (None, 9, 32)       3962        ['layer_normalization_89[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " add_80 (Add)                   (None, 9, 32)        0           ['multi_head_attention_40[0][0]',\n",
      "                                                                  'add_79[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_90 (LayerN  (None, 9, 32)       64          ['add_80[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_44 (Sequential)     (None, 9, 32)        8352        ['layer_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " add_81 (Add)                   (None, 9, 32)        0           ['sequential_44[0][0]',          \n",
      "                                                                  'add_80[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_91 (LayerN  (None, 9, 32)       64          ['add_81[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_41 (Multi  (None, 9, 32)       3962        ['layer_normalization_91[0][0]', \n",
      " HeadAttention)                                                   'layer_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " add_82 (Add)                   (None, 9, 32)        0           ['multi_head_attention_41[0][0]',\n",
      "                                                                  'add_81[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_92 (LayerN  (None, 9, 32)       64          ['add_82[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_45 (Sequential)     (None, 9, 32)        8352        ['layer_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " add_83 (Add)                   (None, 9, 32)        0           ['sequential_45[0][0]',          \n",
      "                                                                  'add_82[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_93 (LayerN  (None, 9, 32)       64          ['add_83[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d_9 (Gl  (None, 32)          0           ['layer_normalization_93[0][0]'] \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_73 (Dense)               (None, 3)            99          ['global_average_pooling1d_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 87,269\n",
      "Trainable params: 87,269\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 6\n",
    "NUM_LAYERS = 6\n",
    "# TRAINING\n",
    "EPOCHS = 50\n",
    "PROJECTION_DIM = 32\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf326e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6aa5b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "108/108 [==============================] - 13s 56ms/step - loss: 0.5367 - accuracy: 0.7845 - top-5-accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.7879 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.4612 - accuracy: 0.8008 - top-5-accuracy: 1.0000 - val_loss: 0.4707 - val_accuracy: 0.7960 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.4161 - accuracy: 0.8075 - top-5-accuracy: 1.0000 - val_loss: 0.4108 - val_accuracy: 0.8193 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.3934 - accuracy: 0.8198 - top-5-accuracy: 1.0000 - val_loss: 0.4090 - val_accuracy: 0.8252 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.3567 - accuracy: 0.8358 - top-5-accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.8368 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.3419 - accuracy: 0.8408 - top-5-accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 0.8357 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.3279 - accuracy: 0.8577 - top-5-accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.8427 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.3061 - accuracy: 0.8629 - top-5-accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.8520 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/50\n",
      "108/108 [==============================] - 6s 51ms/step - loss: 0.2979 - accuracy: 0.8693 - top-5-accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.8438 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.2899 - accuracy: 0.8702 - top-5-accuracy: 1.0000 - val_loss: 0.3606 - val_accuracy: 0.8566 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.2585 - accuracy: 0.8906 - top-5-accuracy: 1.0000 - val_loss: 0.3012 - val_accuracy: 0.8753 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.2465 - accuracy: 0.8988 - top-5-accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.8683 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.2473 - accuracy: 0.9008 - top-5-accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.8741 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.2304 - accuracy: 0.9105 - top-5-accuracy: 1.0000 - val_loss: 0.3559 - val_accuracy: 0.8741 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.2154 - accuracy: 0.9157 - top-5-accuracy: 1.0000 - val_loss: 0.2964 - val_accuracy: 0.8858 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.2148 - accuracy: 0.9157 - top-5-accuracy: 1.0000 - val_loss: 0.3358 - val_accuracy: 0.8718 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1929 - accuracy: 0.9265 - top-5-accuracy: 1.0000 - val_loss: 0.3090 - val_accuracy: 0.8800 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.1953 - accuracy: 0.9227 - top-5-accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.8660 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.1912 - accuracy: 0.9248 - top-5-accuracy: 1.0000 - val_loss: 0.2789 - val_accuracy: 0.8939 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.1829 - accuracy: 0.9323 - top-5-accuracy: 1.0000 - val_loss: 0.2958 - val_accuracy: 0.8951 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1881 - accuracy: 0.9274 - top-5-accuracy: 1.0000 - val_loss: 0.3199 - val_accuracy: 0.8765 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "108/108 [==============================] - 6s 55ms/step - loss: 0.1671 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 0.2619 - val_accuracy: 0.9114 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "108/108 [==============================] - 6s 56ms/step - loss: 0.1626 - accuracy: 0.9370 - top-5-accuracy: 1.0000 - val_loss: 0.2770 - val_accuracy: 0.8928 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.1622 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 0.2636 - val_accuracy: 0.9044 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1660 - accuracy: 0.9373 - top-5-accuracy: 1.0000 - val_loss: 0.2658 - val_accuracy: 0.9126 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1470 - accuracy: 0.9443 - top-5-accuracy: 1.0000 - val_loss: 0.3002 - val_accuracy: 0.8974 - val_top-5-accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1383 - accuracy: 0.9481 - top-5-accuracy: 1.0000 - val_loss: 0.2986 - val_accuracy: 0.9021 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1323 - accuracy: 0.9554 - top-5-accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 0.8951 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.1452 - accuracy: 0.9469 - top-5-accuracy: 1.0000 - val_loss: 0.2978 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1367 - accuracy: 0.9504 - top-5-accuracy: 1.0000 - val_loss: 0.2561 - val_accuracy: 0.9138 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.1487 - accuracy: 0.9408 - top-5-accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9079 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.1374 - accuracy: 0.9475 - top-5-accuracy: 1.0000 - val_loss: 0.2897 - val_accuracy: 0.9068 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.1308 - accuracy: 0.9498 - top-5-accuracy: 1.0000 - val_loss: 0.2838 - val_accuracy: 0.9044 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.1192 - accuracy: 0.9577 - top-5-accuracy: 1.0000 - val_loss: 0.2785 - val_accuracy: 0.9056 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1135 - accuracy: 0.9571 - top-5-accuracy: 1.0000 - val_loss: 0.3056 - val_accuracy: 0.9009 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 0.1029 - accuracy: 0.9656 - top-5-accuracy: 1.0000 - val_loss: 0.2765 - val_accuracy: 0.9009 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "108/108 [==============================] - 5s 50ms/step - loss: 0.1214 - accuracy: 0.9539 - top-5-accuracy: 1.0000 - val_loss: 0.2631 - val_accuracy: 0.9161 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "108/108 [==============================] - 6s 54ms/step - loss: 0.1070 - accuracy: 0.9606 - top-5-accuracy: 1.0000 - val_loss: 0.2463 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0953 - accuracy: 0.9662 - top-5-accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9149 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0940 - accuracy: 0.9697 - top-5-accuracy: 1.0000 - val_loss: 0.2727 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1003 - accuracy: 0.9653 - top-5-accuracy: 1.0000 - val_loss: 0.2592 - val_accuracy: 0.9126 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0985 - accuracy: 0.9624 - top-5-accuracy: 1.0000 - val_loss: 0.2852 - val_accuracy: 0.9079 - val_top-5-accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1086 - accuracy: 0.9574 - top-5-accuracy: 1.0000 - val_loss: 0.2810 - val_accuracy: 0.9172 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.0974 - accuracy: 0.9630 - top-5-accuracy: 1.0000 - val_loss: 0.3268 - val_accuracy: 0.9009 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "108/108 [==============================] - 5s 47ms/step - loss: 0.1008 - accuracy: 0.9635 - top-5-accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 0.9091 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "108/108 [==============================] - 5s 49ms/step - loss: 0.0902 - accuracy: 0.9656 - top-5-accuracy: 1.0000 - val_loss: 0.3022 - val_accuracy: 0.9091 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0947 - accuracy: 0.9659 - top-5-accuracy: 1.0000 - val_loss: 0.2971 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.1037 - accuracy: 0.9589 - top-5-accuracy: 1.0000 - val_loss: 0.2760 - val_accuracy: 0.9091 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0746 - accuracy: 0.9764 - top-5-accuracy: 1.0000 - val_loss: 0.3113 - val_accuracy: 0.9161 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "108/108 [==============================] - 5s 48ms/step - loss: 0.0782 - accuracy: 0.9717 - top-5-accuracy: 1.0000 - val_loss: 0.3646 - val_accuracy: 0.9033 - val_top-5-accuracy: 1.0000\n",
      "58/58 [==============================] - 1s 18ms/step - loss: 0.1427 - accuracy: 0.9504 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 95.04%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "PROJECTION_DIM = 32\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c6df61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.04%\n",
      "Balanced accuracy: 87.63%\n",
      "F1 score for class 1: 89.44%\n"
     ]
    }
   ],
   "source": [
    "#Y_test = np.argmax(YY_test, axis=1)\n",
    "Y_test = Y_test\n",
    "\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, p)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(Y_test, p)\n",
    "print(\"Balanced accuracy: {:.2f}%\".format(balanced_accuracy * 100))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 score for class 1\n",
    "f1_class1 = f1_score(Y_test, p, labels=[1], average='macro')\n",
    "print(\"F1 score for class 1: {:.2f}%\".format(f1_class1 * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f7d70cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298701298701298 0.9362829932798964 0.9728260869565217\n"
     ]
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3620fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398567119155354"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f75542c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "199466a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train_df[19]\n",
    "y = train_df[3]\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [x[0], x[1], x[2], x[3],y[0], y[1], y[2], y[3]]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3eb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027188ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ea8a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 6, 28, 28, 64)     5248      \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 6, 14, 14, 64)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 6, 14, 14, 128)    221312    \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 3, 7, 7, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 3, 7, 7, 256)      884992    \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 1, 3, 3, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 1, 3, 3, 512)      3539456   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 9,372,674\n",
      "Trainable params: 9,372,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(64, (3, 3, 3), activation=\"relu\",name=\"conv1\",   input_shape=(6,28,28,3), strides=(1, 1, 1), padding=\"same\"))  \n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
    "model.add(Conv3D(128, (3, 3, 3), activation=\"relu\",name=\"conv2\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
    "model.add(Conv3D(256, (3, 3, 3), activation=\"relu\",name=\"conv3a\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
    "model.add(Conv3D(512, (3, 3, 3), activation=\"relu\",name=\"conv4a\", strides=(1, 1, 1), padding=\"same\"))   \n",
    "\n",
    "model.add(Flatten())\n",
    "                     \n",
    "    # FC layers group\n",
    "model.add(Dense(1024, activation='relu', name='fc6'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax', name='fc8'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d1b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 357s 5s/step - loss: 0.3074 - accuracy: 0.8591 - val_loss: 0.1795 - val_accuracy: 0.9184\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 356s 5s/step - loss: 0.2036 - accuracy: 0.9224 - val_loss: 0.0635 - val_accuracy: 0.9723\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 338s 4s/step - loss: 0.1969 - accuracy: 0.9216 - val_loss: 0.0981 - val_accuracy: 0.9625\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 360s 5s/step - loss: 0.1657 - accuracy: 0.9306 - val_loss: 0.0700 - val_accuracy: 0.9674\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 358s 5s/step - loss: 0.1690 - accuracy: 0.9310 - val_loss: 0.0765 - val_accuracy: 0.9723\n",
      "Epoch 6/100\n",
      "65/77 [========================>.....] - ETA: 56s - loss: 0.1179 - accuracy: 0.9495 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19660/1501325532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c73fd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_29 (TimeDis (None, 6, 26, 26, 2)      56        \n",
      "_________________________________________________________________\n",
      "time_distributed_30 (TimeDis (None, 6, 13, 13, 2)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed_31 (TimeDis (None, 6, 11, 11, 4)      76        \n",
      "_________________________________________________________________\n",
      "time_distributed_32 (TimeDis (None, 6, 5, 5, 4)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_33 (TimeDis (None, 6, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 8)                 3488      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 3,638\n",
      "Trainable params: 3,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 9s 63ms/step - loss: 0.5996 - accuracy: 0.7999 - val_loss: 0.1954 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 4s 50ms/step - loss: 0.3871 - accuracy: 0.8109 - val_loss: 0.0698 - val_accuracy: 0.9951\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 5s 65ms/step - loss: 0.2653 - accuracy: 0.8775 - val_loss: 0.1443 - val_accuracy: 0.9119\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 2s 25ms/step - loss: 0.2197 - accuracy: 0.9036 - val_loss: 0.1015 - val_accuracy: 0.9396\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 2s 22ms/step - loss: 0.2255 - accuracy: 0.9036 - val_loss: 0.0931 - val_accuracy: 0.9494\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1977 - accuracy: 0.9216 - val_loss: 0.1024 - val_accuracy: 0.9413\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1912 - accuracy: 0.9192 - val_loss: 0.1007 - val_accuracy: 0.9413\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1736 - accuracy: 0.9249 - val_loss: 0.0728 - val_accuracy: 0.9625\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1723 - accuracy: 0.9285 - val_loss: 0.0595 - val_accuracy: 0.9723\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1685 - accuracy: 0.9281 - val_loss: 0.0819 - val_accuracy: 0.9560\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1617 - accuracy: 0.9334 - val_loss: 0.0673 - val_accuracy: 0.9690\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.1567 - accuracy: 0.9367 - val_loss: 0.0865 - val_accuracy: 0.9560\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 2s 28ms/step - loss: 0.1526 - accuracy: 0.9330 - val_loss: 0.1450 - val_accuracy: 0.9217\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1517 - accuracy: 0.9355 - val_loss: 0.0741 - val_accuracy: 0.9608\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1458 - accuracy: 0.9371 - val_loss: 0.0587 - val_accuracy: 0.9723\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1495 - accuracy: 0.9388 - val_loss: 0.1112 - val_accuracy: 0.9462\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1493 - accuracy: 0.9351 - val_loss: 0.0823 - val_accuracy: 0.9608\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1511 - accuracy: 0.9404 - val_loss: 0.0887 - val_accuracy: 0.9592\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1392 - accuracy: 0.9432 - val_loss: 0.0730 - val_accuracy: 0.9608\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1403 - accuracy: 0.9424 - val_loss: 0.0548 - val_accuracy: 0.9706\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1383 - accuracy: 0.9416 - val_loss: 0.0401 - val_accuracy: 0.9821\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1377 - accuracy: 0.9383 - val_loss: 0.0796 - val_accuracy: 0.9625\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1334 - accuracy: 0.9449 - val_loss: 0.0729 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1263 - accuracy: 0.9514 - val_loss: 0.0650 - val_accuracy: 0.9657\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1363 - accuracy: 0.9424 - val_loss: 0.0694 - val_accuracy: 0.9674\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1304 - accuracy: 0.9453 - val_loss: 0.0843 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1290 - accuracy: 0.9449 - val_loss: 0.0952 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1299 - accuracy: 0.9465 - val_loss: 0.0770 - val_accuracy: 0.9657\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1281 - accuracy: 0.9477 - val_loss: 0.0804 - val_accuracy: 0.9674\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1228 - accuracy: 0.9547 - val_loss: 0.0688 - val_accuracy: 0.9690\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1204 - accuracy: 0.9526 - val_loss: 0.0862 - val_accuracy: 0.9641\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1332 - accuracy: 0.9449 - val_loss: 0.0644 - val_accuracy: 0.9690\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1205 - accuracy: 0.9457 - val_loss: 0.0761 - val_accuracy: 0.9674\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1197 - accuracy: 0.9473 - val_loss: 0.0930 - val_accuracy: 0.9641\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1345 - accuracy: 0.9388 - val_loss: 0.0513 - val_accuracy: 0.9772\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1221 - accuracy: 0.9469 - val_loss: 0.0652 - val_accuracy: 0.9690\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1235 - accuracy: 0.9473 - val_loss: 0.0506 - val_accuracy: 0.9755\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1137 - accuracy: 0.9518 - val_loss: 0.0591 - val_accuracy: 0.9706\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1185 - accuracy: 0.9494 - val_loss: 0.1011 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1152 - accuracy: 0.9526 - val_loss: 0.0479 - val_accuracy: 0.9772\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1166 - accuracy: 0.9522 - val_loss: 0.0808 - val_accuracy: 0.9674\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1106 - accuracy: 0.9530 - val_loss: 0.0765 - val_accuracy: 0.9657\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1124 - accuracy: 0.9555 - val_loss: 0.0914 - val_accuracy: 0.9641\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1097 - accuracy: 0.9526 - val_loss: 0.1018 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1180 - accuracy: 0.9477 - val_loss: 0.0502 - val_accuracy: 0.9755\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1083 - accuracy: 0.9514 - val_loss: 0.0792 - val_accuracy: 0.9657\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1104 - accuracy: 0.9522 - val_loss: 0.0804 - val_accuracy: 0.9657\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1078 - accuracy: 0.9530 - val_loss: 0.0793 - val_accuracy: 0.9674\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1058 - accuracy: 0.9543 - val_loss: 0.0647 - val_accuracy: 0.9706\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1088 - accuracy: 0.9506 - val_loss: 0.0543 - val_accuracy: 0.9723\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1102 - accuracy: 0.9526 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1052 - accuracy: 0.9604 - val_loss: 0.0945 - val_accuracy: 0.9641\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1075 - accuracy: 0.9530 - val_loss: 0.0544 - val_accuracy: 0.9723\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1023 - accuracy: 0.9588 - val_loss: 0.0930 - val_accuracy: 0.9657\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1092 - accuracy: 0.9584 - val_loss: 0.1430 - val_accuracy: 0.9429\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1212 - accuracy: 0.9469 - val_loss: 0.0491 - val_accuracy: 0.9788\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0981 - accuracy: 0.9559 - val_loss: 0.0854 - val_accuracy: 0.9657\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1032 - accuracy: 0.9518 - val_loss: 0.1032 - val_accuracy: 0.9576\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1055 - accuracy: 0.9547 - val_loss: 0.1238 - val_accuracy: 0.9560\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1008 - accuracy: 0.9596 - val_loss: 0.0652 - val_accuracy: 0.9706\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1004 - accuracy: 0.9575 - val_loss: 0.0691 - val_accuracy: 0.9674\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.1131 - accuracy: 0.9539 - val_loss: 0.0863 - val_accuracy: 0.9657\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0976 - accuracy: 0.9608 - val_loss: 0.1033 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9600 - val_loss: 0.1076 - val_accuracy: 0.9641\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0962 - accuracy: 0.9592 - val_loss: 0.0723 - val_accuracy: 0.9674\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.0956 - accuracy: 0.9559 - val_loss: 0.1495 - val_accuracy: 0.9494\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1030 - accuracy: 0.9551 - val_loss: 0.1109 - val_accuracy: 0.9641\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0991 - accuracy: 0.9559 - val_loss: 0.1028 - val_accuracy: 0.9625\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1012 - accuracy: 0.9588 - val_loss: 0.0904 - val_accuracy: 0.9657\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1014 - accuracy: 0.9551 - val_loss: 0.1130 - val_accuracy: 0.9576\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1081 - accuracy: 0.9539 - val_loss: 0.1036 - val_accuracy: 0.9641\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0987 - accuracy: 0.9579 - val_loss: 0.0856 - val_accuracy: 0.9641\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.1028 - val_accuracy: 0.9576\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0906 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9641\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0969 - accuracy: 0.9547 - val_loss: 0.0551 - val_accuracy: 0.9804\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0920 - accuracy: 0.9592 - val_loss: 0.1260 - val_accuracy: 0.9543\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0985 - accuracy: 0.9563 - val_loss: 0.0661 - val_accuracy: 0.9690\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0886 - accuracy: 0.9608 - val_loss: 0.0898 - val_accuracy: 0.9674\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0961 - accuracy: 0.9592 - val_loss: 0.1046 - val_accuracy: 0.9625\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0889 - accuracy: 0.9612 - val_loss: 0.1196 - val_accuracy: 0.9641\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0872 - accuracy: 0.9624 - val_loss: 0.0987 - val_accuracy: 0.9608\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0914 - accuracy: 0.9608 - val_loss: 0.0799 - val_accuracy: 0.9674\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0948 - accuracy: 0.9596 - val_loss: 0.1392 - val_accuracy: 0.9429\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9637 - val_loss: 0.0783 - val_accuracy: 0.9674\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.0971 - accuracy: 0.9575 - val_loss: 0.1097 - val_accuracy: 0.9592\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0962 - accuracy: 0.9563 - val_loss: 0.0723 - val_accuracy: 0.9690\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0826 - accuracy: 0.9641 - val_loss: 0.0919 - val_accuracy: 0.9674\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0796 - accuracy: 0.9645 - val_loss: 0.0800 - val_accuracy: 0.9690\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0905 - accuracy: 0.9616 - val_loss: 0.0891 - val_accuracy: 0.9674\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0873 - accuracy: 0.9608 - val_loss: 0.1020 - val_accuracy: 0.9625\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0776 - accuracy: 0.9690 - val_loss: 0.0982 - val_accuracy: 0.9690\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0839 - accuracy: 0.9641 - val_loss: 0.0813 - val_accuracy: 0.9674\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0857 - accuracy: 0.9633 - val_loss: 0.1204 - val_accuracy: 0.9608\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0855 - accuracy: 0.9665 - val_loss: 0.0947 - val_accuracy: 0.9657\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0865 - accuracy: 0.9641 - val_loss: 0.1054 - val_accuracy: 0.9657\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 2s 31ms/step - loss: 0.0940 - accuracy: 0.9612 - val_loss: 0.0821 - val_accuracy: 0.9690\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0846 - accuracy: 0.9641 - val_loss: 0.1317 - val_accuracy: 0.9527\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0773 - accuracy: 0.9682 - val_loss: 0.1421 - val_accuracy: 0.9576\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0958 - accuracy: 0.9584 - val_loss: 0.1490 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 2s 30ms/step - loss: 0.0880 - accuracy: 0.9653 - val_loss: 0.0884 - val_accuracy: 0.9674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x263d82cdd30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "model= models.Sequential()\n",
    "model.add(TimeDistributed(Conv2D(2, (3, 3), strides=(1,1),activation='relu'),input_shape=(6, 28, 28, 3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "model.add(TimeDistributed(Conv2D(4, (3, 3), strides=(1,1),activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(2,2)))\n",
    "\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(8,return_sequences=False,dropout=0.2)) # used 32 units\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f23f6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253731343283582 0.9528218434864658 0.9253731343283582\n"
     ]
    }
   ],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T\n",
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r =  (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5b058ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e43530db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2643d22cdf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99db6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "742d452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_5 (PatchEncoder)  (None, 6, 32)        11096       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, 6, 32)        64          patch_encoder_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_36 (MultiH (None, 6, 32)        8416        layer_normalization_85[0][0]     \n",
      "                                                                 layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 6, 32)        8320        layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 6, 32)        0           multi_head_attention_36[0][0]    \n",
      "                                                                 patch_encoder_5[0][0]            \n",
      "                                                                 lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, 6, 32)        64          add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_52 (Sequential)      (None, 6, 32)        1056        layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 6, 32)        0           sequential_52[0][0]              \n",
      "                                                                 add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, 6, 32)        64          add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_37 (MultiH (None, 6, 32)        8416        layer_normalization_87[0][0]     \n",
      "                                                                 layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                  (None, 6, 32)        8320        layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 6, 32)        0           multi_head_attention_37[0][0]    \n",
      "                                                                 add_81[0][0]                     \n",
      "                                                                 lstm_33[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, 6, 32)        64          add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_53 (Sequential)      (None, 6, 32)        1056        layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 6, 32)        0           sequential_53[0][0]              \n",
      "                                                                 add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, 6, 32)        64          add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 2)            66          global_average_pooling1d_5[0][0] \n",
      "==================================================================================================\n",
      "Total params: 47,066\n",
      "Trainable params: 47,066\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "77/77 [==============================] - 45s 182ms/step - loss: 0.3119 - accuracy: 0.8730 - val_loss: 0.1708 - val_accuracy: 0.9331\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 9s 119ms/step - loss: 0.1945 - accuracy: 0.9253 - val_loss: 0.1656 - val_accuracy: 0.9396\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.1590 - accuracy: 0.9379 - val_loss: 0.1555 - val_accuracy: 0.9380\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 10s 137ms/step - loss: 0.1506 - accuracy: 0.9420 - val_loss: 0.2573 - val_accuracy: 0.9119\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.1288 - accuracy: 0.9535 - val_loss: 0.2039 - val_accuracy: 0.9331\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.1138 - accuracy: 0.9588 - val_loss: 0.1660 - val_accuracy: 0.9347\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 12s 150ms/step - loss: 0.1093 - accuracy: 0.9624 - val_loss: 0.1294 - val_accuracy: 0.9511\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 11s 147ms/step - loss: 0.0845 - accuracy: 0.9694 - val_loss: 0.2172 - val_accuracy: 0.9413\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0719 - accuracy: 0.9767 - val_loss: 0.1852 - val_accuracy: 0.9250\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0763 - accuracy: 0.9690 - val_loss: 0.2777 - val_accuracy: 0.9054\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0587 - accuracy: 0.9775 - val_loss: 0.3214 - val_accuracy: 0.8923\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 9s 117ms/step - loss: 0.0529 - accuracy: 0.9820 - val_loss: 0.1472 - val_accuracy: 0.9380\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0520 - accuracy: 0.9816 - val_loss: 0.1741 - val_accuracy: 0.9478\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0469 - accuracy: 0.9829 - val_loss: 0.4499 - val_accuracy: 0.8825\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0551 - accuracy: 0.9792 - val_loss: 0.2270 - val_accuracy: 0.9380\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 13s 164ms/step - loss: 0.0378 - accuracy: 0.9869 - val_loss: 0.2365 - val_accuracy: 0.9331\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 11s 141ms/step - loss: 0.0393 - accuracy: 0.9865 - val_loss: 0.3281 - val_accuracy: 0.9070\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 13s 166ms/step - loss: 0.0336 - accuracy: 0.9886 - val_loss: 0.5056 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 12s 151ms/step - loss: 0.0563 - accuracy: 0.9804 - val_loss: 0.2307 - val_accuracy: 0.9299\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0328 - accuracy: 0.9886 - val_loss: 0.2689 - val_accuracy: 0.9250\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 11s 142ms/step - loss: 0.0313 - accuracy: 0.9886 - val_loss: 0.0974 - val_accuracy: 0.9674\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0393 - accuracy: 0.9853 - val_loss: 0.1747 - val_accuracy: 0.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0449 - accuracy: 0.9853 - val_loss: 0.1142 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0167 - accuracy: 0.9931 - val_loss: 0.4680 - val_accuracy: 0.9021\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0479 - accuracy: 0.9816 - val_loss: 0.1475 - val_accuracy: 0.9608\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 0.0168 - accuracy: 0.9931 - val_loss: 0.2110 - val_accuracy: 0.9511\n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0170 - accuracy: 0.9951 - val_loss: 0.3497 - val_accuracy: 0.9282\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 11s 146ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.3747 - val_accuracy: 0.9233\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.3154 - val_accuracy: 0.9380\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0110 - accuracy: 0.9959 - val_loss: 0.2614 - val_accuracy: 0.9396\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3280 - val_accuracy: 0.9331\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.3375 - val_accuracy: 0.9347\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 11s 140ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.4055 - val_accuracy: 0.9299\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0205 - accuracy: 0.9955 - val_loss: 0.1498 - val_accuracy: 0.9576\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0625 - accuracy: 0.9788 - val_loss: 0.3064 - val_accuracy: 0.9266\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 0.0066 - accuracy: 0.9984 - val_loss: 0.2326 - val_accuracy: 0.9494\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.3434 - val_accuracy: 0.9364\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3835 - val_accuracy: 0.9299\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 0.0364 - accuracy: 0.9857 - val_loss: 0.4883 - val_accuracy: 0.8793\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 10s 132ms/step - loss: 0.0211 - accuracy: 0.9939 - val_loss: 0.2401 - val_accuracy: 0.9445\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2996 - val_accuracy: 0.9396\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3310 - val_accuracy: 0.9396\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 8.8423e-04 - accuracy: 1.0000 - val_loss: 0.3626 - val_accuracy: 0.9380\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.3936 - val_accuracy: 0.9299\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3034 - val_accuracy: 0.9478\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.1240 - accuracy: 0.9608 - val_loss: 0.1293 - val_accuracy: 0.9592\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0338 - accuracy: 0.9894 - val_loss: 0.2322 - val_accuracy: 0.9429\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1845 - val_accuracy: 0.9641\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0163 - accuracy: 0.9935 - val_loss: 0.2686 - val_accuracy: 0.9445\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.2194 - val_accuracy: 0.9543\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3109 - val_accuracy: 0.9396\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.6788e-04 - accuracy: 1.0000 - val_loss: 0.2967 - val_accuracy: 0.9429\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.2790 - val_accuracy: 0.9494\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2257 - val_accuracy: 0.9576\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0542 - accuracy: 0.9812 - val_loss: 0.1926 - val_accuracy: 0.9543\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0236 - accuracy: 0.9890 - val_loss: 0.2859 - val_accuracy: 0.9413\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 10s 131ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.2832 - val_accuracy: 0.9494\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 11s 148ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.3047 - val_accuracy: 0.9413\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2930 - val_accuracy: 0.9511\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 6.1541e-04 - accuracy: 1.0000 - val_loss: 0.3011 - val_accuracy: 0.9527\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 10s 136ms/step - loss: 6.8710e-04 - accuracy: 1.0000 - val_loss: 0.3303 - val_accuracy: 0.9478\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.9433e-04 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9462\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 2.6167e-04 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9445\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 2.9263e-04 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.9478\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.7604e-04 - accuracy: 1.0000 - val_loss: 0.3578 - val_accuracy: 0.9462\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 2.8471e-04 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9478\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 9s 118ms/step - loss: 1.3727e-04 - accuracy: 1.0000 - val_loss: 0.3786 - val_accuracy: 0.9445\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0018 - accuracy: 0.9992 - val_loss: 0.4250 - val_accuracy: 0.9331\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 6.2091e-04 - accuracy: 1.0000 - val_loss: 0.3400 - val_accuracy: 0.9494\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 9s 116ms/step - loss: 0.0477 - accuracy: 0.9873 - val_loss: 0.2780 - val_accuracy: 0.9527\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.2241 - val_accuracy: 0.9478\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0060 - accuracy: 0.9984 - val_loss: 0.3305 - val_accuracy: 0.9364\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0032 - accuracy: 0.9984 - val_loss: 0.3235 - val_accuracy: 0.9478\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3186 - val_accuracy: 0.9478\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 9.5974e-04 - accuracy: 1.0000 - val_loss: 0.4130 - val_accuracy: 0.9380\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 10s 133ms/step - loss: 2.8283e-04 - accuracy: 1.0000 - val_loss: 0.3413 - val_accuracy: 0.9494\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 11s 138ms/step - loss: 2.5996e-04 - accuracy: 1.0000 - val_loss: 0.3759 - val_accuracy: 0.9445\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 10s 127ms/step - loss: 1.1576e-04 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 0.9429\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.1305e-04 - accuracy: 1.0000 - val_loss: 0.3753 - val_accuracy: 0.9445\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 9.8853e-05 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9462\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.3591e-04 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9511\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 1.6559e-04 - accuracy: 1.0000 - val_loss: 0.3077 - val_accuracy: 0.9543\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0719 - accuracy: 0.9743 - val_loss: 0.2865 - val_accuracy: 0.9315\n",
      "Epoch 84/100\n",
      "77/77 [==============================] - 9s 121ms/step - loss: 0.0272 - accuracy: 0.9886 - val_loss: 0.5347 - val_accuracy: 0.8842\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0279 - accuracy: 0.9918 - val_loss: 0.2224 - val_accuracy: 0.9462\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.2061 - val_accuracy: 0.9527\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 10s 129ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.2713 - val_accuracy: 0.9478\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 10s 126ms/step - loss: 9.6324e-04 - accuracy: 0.9996 - val_loss: 0.3461 - val_accuracy: 0.9429\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 10s 134ms/step - loss: 4.2564e-04 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9445\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 10s 128ms/step - loss: 5.9142e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9462\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 3.6335e-04 - accuracy: 1.0000 - val_loss: 0.3490 - val_accuracy: 0.9462\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 9s 124ms/step - loss: 1.9334e-04 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.9478\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 1.6028e-04 - accuracy: 1.0000 - val_loss: 0.3868 - val_accuracy: 0.9429\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 1.2040e-04 - accuracy: 1.0000 - val_loss: 0.3778 - val_accuracy: 0.9462\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 10s 124ms/step - loss: 1.7867e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9494\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 10s 130ms/step - loss: 1.2447e-04 - accuracy: 1.0000 - val_loss: 0.3843 - val_accuracy: 0.9478\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.7745e-05 - accuracy: 1.0000 - val_loss: 0.3941 - val_accuracy: 0.9445\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 9s 122ms/step - loss: 9.9591e-05 - accuracy: 1.0000 - val_loss: 0.4102 - val_accuracy: 0.9445\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 10s 125ms/step - loss: 8.3640e-05 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9445\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 9s 123ms/step - loss: 7.6141e-05 - accuracy: 1.0000 - val_loss: 0.3968 - val_accuracy: 0.9445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2643d486f40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.1)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_df,Y_train,validation_split=0.2,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2545595e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2643d2d6f10>,\n",
       " <__main__.PatchEncoder at 0x2643d2d67f0>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d2d6a60>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2644020d220>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2640c307fd0>,\n",
       " <keras.layers.merge.Add at 0x2643d33f850>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2640b258eb0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d28c8b0>,\n",
       " <keras.layers.merge.Add at 0x2644020e730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d27bd00>,\n",
       " <keras.layers.multi_head_attention.MultiHeadAttention at 0x2643d3004f0>,\n",
       " <keras.layers.recurrent_v2.LSTM at 0x2643d28c820>,\n",
       " <keras.layers.merge.Add at 0x2643d3b9970>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d362dc0>,\n",
       " <keras.engine.sequential.Sequential at 0x2643d40a670>,\n",
       " <keras.layers.merge.Add at 0x2643d27b730>,\n",
       " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x2643d3002e0>,\n",
       " <keras.layers.pooling.GlobalAveragePooling1D at 0x2643d3d6a30>,\n",
       " <keras.layers.core.Dense at 0x2643d42e9d0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5724c4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 169 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000264554A5310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "attn = Model(inputs=model.input, outputs = model.layers[10].output)\n",
    "\n",
    "pred = attn.predict(test_df[:2], steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41a95b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.4132207 , -1.2195096 ,  0.8252611 ,  0.2068735 ,\n",
       "         -1.9345852 , -0.3024784 ,  0.68288636,  0.24726246,\n",
       "          1.4878579 , -0.38394576,  1.9602623 , -1.2115494 ,\n",
       "          0.7494351 , -1.2607741 ,  1.5064203 ,  0.4506906 ,\n",
       "         -0.10477968,  0.67580473,  0.11815078,  0.5772801 ,\n",
       "         -0.5792286 , -2.167793  ,  0.25299546,  0.17493363,\n",
       "         -1.3997712 ,  0.21966185, -0.42484006,  0.366758  ,\n",
       "         -0.8750425 , -0.6389731 , -1.5038117 ,  0.5427528 ],\n",
       "        [-0.49964952, -1.3981657 ,  0.9291378 ,  0.33286896,\n",
       "         -2.1367726 , -0.33195415,  0.7276804 ,  0.17592141,\n",
       "          1.6696632 , -0.3734444 ,  2.1794388 , -1.3467876 ,\n",
       "          0.8988415 , -1.5041916 ,  1.6590377 ,  0.6519078 ,\n",
       "         -0.22443868,  0.88728726,  0.08924537,  0.5581587 ,\n",
       "         -0.6181777 , -2.513119  ,  0.44085464,  0.16715612,\n",
       "         -1.6275524 ,  0.24514648, -0.36834878,  0.35315546,\n",
       "         -1.0476707 , -0.6746873 , -1.7237133 ,  0.67369545],\n",
       "        [-0.5054516 , -1.4051408 ,  0.931937  ,  0.33905593,\n",
       "         -2.1503923 , -0.32643858,  0.72040987,  0.16062592,\n",
       "          1.6761467 , -0.37332964,  2.192372  , -1.3548307 ,\n",
       "          0.91177434, -1.5271318 ,  1.674906  ,  0.67372906,\n",
       "         -0.23302507,  0.90983194,  0.08345299,  0.550993  ,\n",
       "         -0.6208706 , -2.5409727 ,  0.45941108,  0.17093034,\n",
       "         -1.6397773 ,  0.24939248, -0.35014296,  0.3488802 ,\n",
       "         -1.0668329 , -0.6731022 , -1.7361139 ,  0.685371  ],\n",
       "        [-0.45331264, -1.306415  ,  0.8866546 ,  0.25975394,\n",
       "         -2.0246832 , -0.34005788,  0.7407966 ,  0.26629165,\n",
       "          1.5913628 , -0.38598648,  2.0556822 , -1.2800528 ,\n",
       "          0.79983205, -1.3376297 ,  1.550189  ,  0.49013677,\n",
       "         -0.15583493,  0.7203113 ,  0.11949462,  0.58744025,\n",
       "         -0.58959144, -2.295015  ,  0.2989272 ,  0.1614392 ,\n",
       "         -1.4926645 ,  0.22279164, -0.45505288,  0.36787757,\n",
       "         -0.9085372 , -0.6712196 , -1.6083722 ,  0.58423537],\n",
       "        [-0.46676803, -1.3375354 ,  0.89921296,  0.2818547 ,\n",
       "         -2.0564473 , -0.3375323 ,  0.72806275,  0.22456515,\n",
       "          1.6079518 , -0.3822405 ,  2.0922425 , -1.2935742 ,\n",
       "          0.83231705, -1.3891375 ,  1.5930897 ,  0.543874  ,\n",
       "         -0.16311598,  0.7822279 ,  0.11047427,  0.5792245 ,\n",
       "         -0.6069935 , -2.3648562 ,  0.34867156,  0.16283749,\n",
       "         -1.5390068 ,  0.23143686, -0.41973758,  0.36701533,\n",
       "         -0.9627736 , -0.66932124, -1.6384935 ,  0.61658263],\n",
       "        [-0.47220904, -1.3485214 ,  0.9062314 ,  0.28910938,\n",
       "         -2.0689666 , -0.34121707,  0.73520386,  0.22686623,\n",
       "          1.6213392 , -0.38222903,  2.104739  , -1.3030748 ,\n",
       "          0.8397041 , -1.4010637 ,  1.5984538 ,  0.551494  ,\n",
       "         -0.17236249,  0.7889492 ,  0.10953354,  0.57948005,\n",
       "         -0.60737693, -2.3821776 ,  0.35537735,  0.16158032,\n",
       "         -1.5513701 ,  0.2319126 , -0.42227972,  0.36642838,\n",
       "         -0.967269  , -0.67334425, -1.6527948 ,  0.6217404 ]],\n",
       "\n",
       "       [[-0.42649883, -1.07714   ,  1.1484387 ,  0.08490907,\n",
       "         -1.6851516 , -0.46723652,  0.5061661 , -0.5605585 ,\n",
       "          1.0461252 , -0.49998504,  1.0360165 , -0.6946146 ,\n",
       "          0.97356695, -1.060988  ,  1.7463284 ,  0.32441568,\n",
       "          0.33397505,  1.1660689 ,  0.3562874 ,  0.45945892,\n",
       "         -0.47635746, -1.5587056 ,  0.7094144 , -0.17435731,\n",
       "         -1.5678124 ,  0.21163751,  0.17598131,  0.34968036,\n",
       "         -1.2812881 , -0.66485435, -1.0910249 ,  0.6593576 ],\n",
       "        [-0.42786318, -1.0868777 ,  1.1333342 ,  0.09772161,\n",
       "         -1.6886925 , -0.47518152,  0.5093493 , -0.56699693,\n",
       "          1.0518668 , -0.47610977,  1.0434015 , -0.7050921 ,\n",
       "          0.9808329 , -1.0820138 ,  1.7231802 ,  0.33740744,\n",
       "          0.35213426,  1.1791012 ,  0.36592385,  0.45341754,\n",
       "         -0.47471106, -1.5531352 ,  0.7377216 , -0.1596379 ,\n",
       "         -1.5623512 ,  0.20603526,  0.16128173,  0.33179644,\n",
       "         -1.2726437 , -0.669748  , -1.0993601 ,  0.6809811 ],\n",
       "        [-0.46682185, -1.154356  ,  1.1586797 ,  0.13679314,\n",
       "         -1.6567787 , -0.49478176,  0.52970284, -0.6290013 ,\n",
       "          1.0179622 , -0.5025515 ,  0.99508554, -0.7152843 ,\n",
       "          0.97520405, -1.1048411 ,  1.7275623 ,  0.32937735,\n",
       "          0.36986497,  1.1806638 ,  0.43615305,  0.43307212,\n",
       "         -0.48267272, -1.4823427 ,  0.75971186, -0.19307967,\n",
       "         -1.5393314 ,  0.22102971,  0.18088883,  0.34285673,\n",
       "         -1.2822428 , -0.71883744, -1.0900193 ,  0.6960212 ],\n",
       "        [-0.4363001 , -1.0829927 ,  1.1493462 ,  0.0923662 ,\n",
       "         -1.6651357 , -0.48290762,  0.5026756 , -0.5942865 ,\n",
       "          1.0257357 , -0.49765265,  0.9899746 , -0.69200104,\n",
       "          0.9832234 , -1.0667776 ,  1.7409017 ,  0.3290018 ,\n",
       "          0.35922012,  1.1847463 ,  0.38116938,  0.435992  ,\n",
       "         -0.4662827 , -1.517116  ,  0.7349468 , -0.183203  ,\n",
       "         -1.5593985 ,  0.21721348,  0.19318488,  0.33741537,\n",
       "         -1.2820184 , -0.67617387, -1.0793917 ,  0.6741583 ],\n",
       "        [-0.49830765, -1.1531427 ,  1.2187309 ,  0.10719223,\n",
       "         -1.587356  , -0.54115945,  0.49432966, -0.7039962 ,\n",
       "          0.9452714 , -0.57704806,  0.8339936 , -0.6744776 ,\n",
       "          0.9837212 , -1.0368228 ,  1.7723382 ,  0.28157845,\n",
       "          0.40617546,  1.2058271 ,  0.47497472,  0.3932675 ,\n",
       "         -0.44723678, -1.3788327 ,  0.7450216 , -0.2586214 ,\n",
       "         -1.5238549 ,  0.25344393,  0.27242252,  0.34878147,\n",
       "         -1.2884816 , -0.73330414, -1.0293506 ,  0.6805313 ],\n",
       "        [-0.4841547 , -1.1146897 ,  1.2149087 ,  0.08220633,\n",
       "         -1.5884408 , -0.53472805,  0.4779762 , -0.6898922 ,\n",
       "          0.942987  , -0.5763631 ,  0.8229502 , -0.6605422 ,\n",
       "          0.9893767 , -1.0167508 ,  1.7826542 ,  0.28118014,\n",
       "          0.4056453 ,  1.2089812 ,  0.44735393,  0.39052385,\n",
       "         -0.43789554, -1.3930148 ,  0.73019046, -0.25510353,\n",
       "         -1.5354162 ,  0.25418186,  0.28484488,  0.3454959 ,\n",
       "         -1.2892004 , -0.71183616, -1.0225661 ,  0.6692119 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0c239b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4260804453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[:3]\n",
    "y = attention_layer.predict(test_df, test_df, return_attention_scores=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e844434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51cf7614",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected input 0 to have rank 3 but got: 2 [Op:Einsum]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15860/4266437127.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_attention_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# take one sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgridspec_kw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth_ratios\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\multi_head_attention.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, query, value, key, attention_mask, return_attention_scores, training)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;31m#   H = `size_per_head`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# `query` = [B, T, N ,H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m     \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[1;31m# `key` = [B, S, N, H]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\einsum_dense.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m       \u001b[0mret\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7186\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected input 0 to have rank 3 but got: 2 [Op:Einsum]"
     ]
    }
   ],
   "source": [
    "attention_layer = model.layers[10]\n",
    "_, attention_scores = attention_layer(Y_test[:1], test_df[:1], return_attention_scores=True) # take one sample\n",
    "fig, axs = plt.subplots(ncols=3, gridspec_kw=dict(width_ratios=[5,5,0.2]))\n",
    "sb.heatmap(attention_scores[0, 0, :, :], annot=True, cbar=False, ax=axs[0])\n",
    "sb.heatmap(attention_scores[0, 1, :, :], annot=True, yticklabels=False, cbar=False, ax=axs[1])\n",
    "fig.colorbar(axs[1].collections[0], cax=axs[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5f3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b30013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bf04e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196759ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250da400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
