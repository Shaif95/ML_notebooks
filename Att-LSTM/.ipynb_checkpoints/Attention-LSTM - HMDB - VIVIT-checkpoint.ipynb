{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "acba090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa13c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0bf0822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eeab214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a504b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn='E:/HMDB60/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90ff3ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:/HMDB60\\\\brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\April_09_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\atempting_to_brush_my_hair_brush_hair_u_nm_np2_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ba_goo_4\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np1_ri_med_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Hair_II_brush_hair_u_nm_np2_le_med_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_ba_med_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Aussie_Brunette_Brushing_Long_Hair_brush_hair_u_nm_np1_fr_med_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Blonde_being_brushed_brush_hair_f_nm_np2_ri_med_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Blonde_being_brushed_brush_hair_u_cm_np2_ri_med_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_fr_goo_4\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_le_goo_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_f_nm_np1_ri_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_h_cm_np2_ri_goo_6\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brunette_Foxyanya_ultra_silky_long_hair_brushing_hairjob_brush_hair_h_nm_np2_ri_goo_5\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_2_brush_hair_h_nm_np1_ba_med_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_2_brush_hair_h_nm_np1_ba_med_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_2_brush_hair_h_nm_np1_ba_med_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_brush_hair_f_cm_np2_ri_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_brush_hair_f_nm_np2_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_brush_hair_f_nm_np2_ba_goo_4\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_brush_hair_f_nm_np2_ri_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_hair_brush_hair_f_nm_np2_ri_goo_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Hair_with_Beth_brush_hair_h_nm_np1_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Hair_with_Beth_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Hair_with_Beth_brush_hair_u_nm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_hair__the_right_way_brush_hair_u_nm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_Her_Hair__[_NEW_AUDIO_]_UPDATED!!!!_brush_hair_h_cm_np1_le_goo_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_jrs_hair_brush_hair_u_cm_np2_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_jrs_hair_brush_hair_u_cm_np2_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_hair_-_December_2008_brush_hair_u_cm_np1_ba_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_hair_-_December_2008_brush_hair_u_cm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_hair_-_December_2008_brush_hair_u_nm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_long_hair_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_long_hair_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_long_hair_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_ba_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_Long_Hair__February_2009_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Brushing_my_waist_lenth_hair_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brushing_raychel_s_hair_brush_hair_u_cm_np2_ri_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\brush_my_hair_without_wearing_the_glasses_brush_hair_u_nm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\can_somebody_just_brush_my_hair!_brush_hair_u_cm_np2_ri_med_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_cm_np1_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Chanta!____THAT_IS_long_blonde_hair!_brush_hair_u_nm_np1_ri_goo_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_amazing_long_hair_brush_hair_u_cm_np1_ri_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Ella_brushing_her_long_blonde_hair___again_brush_hair_u_cm_np1_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Haarek_mmen_brush_hair_h_cm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Haarek_mmen_brush_hair_h_cm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Haarek_mmen_brush_hair_h_cm_np1_fr_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\indianrapunzels_com---silky_long_hair_brushing_brush_hair_u_cm_np2_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\indianrapunzels_com---silky_long_hair_brushing_brush_hair_u_cm_np2_ri_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_fr_med_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_fr_med_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Lektion_3__-_Das_Haare_b_rsten_brush_hair_h_nm_np1_le_med_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\long_hair_Christa__brush_hair_u_nm_np1_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\me_brushing_my_hair_lol_i_was_being_silly_brush_hair_u_nm_np1_ri_goo_3\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\My_Hair_Routine_brush_hair_h_nm_np1_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Olivia_Brushing_Hair_Playing_with_Evelyn_brush_hair_u_cm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Olivia_Brushing_Hair_Playing_with_Evelyn_brush_hair_u_cm_np2_ri_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Prelinger_HabitPat1954_brush_hair_h_nm_np1_ba_med_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Prelinger_HabitPat1954_brush_hair_h_nm_np1_fr_med_26\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Prelinger_HabitPat1954_brush_hair_u_cm_np1_le_med_10\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Prelinger_HabitPat1954_brush_hair_u_nm_np1_le_goo_11\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\rebecca_golden_hair_brush_hair_h_nm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\red_head_brush_hair_u_cm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\red_head_brush_hair_u_cm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\red_head_brush_hair_u_cm_np1_le_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\sarah_brushing_her_hair_brush_hair_h_cm_np1_ri_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\sarah_brushing_her_hair_brush_hair_h_cm_np1_ri_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Silky_Straight_Hair_Original_brush_hair_h_nm_np1_ba_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Silky_Straight_Hair_Original_brush_hair_h_nm_np1_ba_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Silky_Straight_Hair_Original_brush_hair_u_nm_np1_ba_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Slave_brush_my_hair_brush_hair_u_cm_np2_le_goo_2\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_0\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_1\\\\',\n",
       " 'E:/HMDB60\\\\brush_hair\\\\Trannydude___Brushing_SyntheticHair___OhNOES!__those_fukin_knots!_brush_hair_u_nm_np1_fr_goo_2\\\\']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glob(tr[0]+'/*/')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb9999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0b6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf33ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb778605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3703c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    imgdata=[]\n",
    "    higher = len(x)\n",
    "    import more_itertools as mit\n",
    "    \n",
    "    y = mit.random_combination(range(0, higher), r=20)\n",
    "\n",
    "    for i in y:\n",
    "        \n",
    "        a = Image.open(x[i])\n",
    "        b = a.resize((60, 60))\n",
    "        c = np.array(b)\n",
    "        imgdata.append(c.reshape(60,60,3))\n",
    "        \n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata\n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    #print(np.shape(X_train))\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c98322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4331b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bc375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbe2817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_training_batch_generator.__getitem__(3)\n",
    "#(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fdbdc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = My_Test_Generator(test, batch_size).__getitem__(7)\n",
    "arr = np.array(My_Test_Generator(test, batch_size).__getitem__(0))\n",
    "for i in range(1,(len(x))):\n",
    "    x = My_Test_Generator(test, batch_size).__getitem__(i)\n",
    "    arr = np.concatenate((arr,x),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8275bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e6e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c51980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079c6d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24d262b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c2759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "56c507ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3499, 6, 28, 28, 3)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"organmnist3d\"\n",
    "BATCH_SIZE = 32\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = ( 6, 28, 28, 3 )\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 60\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "# ViViT ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "PROJECTION_DIM = 128\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9ae32102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "88acaa5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c674b1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6123, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31947b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fc77872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (6,28,28,3) )\n",
    "\n",
    "#x =  TimeDistributed( Conv2D(4, (3, 3), strides=(1,1),activation='relu') ) (inputs)\n",
    "\n",
    "#y = TimeDistributed(MaxPooling2D(2,2)) (x)\n",
    "\n",
    "#w = (TimeDistributed( Flatten() )) (y)\n",
    "\n",
    "#z = LSTM(128,return_sequences=True,dropout=0.1) (w)\n",
    "\n",
    "encoded_patches = (PatchEncoder(6, 64 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=64, dropout=0.1 )  (x1, x1)\n",
    "\n",
    "        \n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(LSTM(64,return_sequences=False,dropout=0.1) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = LSTM(80,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=2, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46672d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f07b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a01de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5e73d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1b70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e37073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(32, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                (layers.Conv2D(64, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                (layers.Conv2D(128, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim),\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26057200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 20, 60, 60,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "patch_encoder_2 (PatchEncoder)  (None, 20, 32)       196320      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 20, 32)       64          patch_encoder_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_4 (MultiHe (None, 20, 32)       8416        layer_normalization_10[0][0]     \n",
      "                                                                 layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 20, 32)       8320        layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 20, 32)       0           multi_head_attention_4[0][0]     \n",
      "                                                                 patch_encoder_2[0][0]            \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 20, 32)       64          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 20, 32)       1056        layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 20, 32)       0           sequential_7[0][0]               \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 20, 32)       64          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_5 (MultiHe (None, 20, 32)       8416        layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 20, 32)       8320        layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 20, 32)       0           multi_head_attention_5[0][0]     \n",
      "                                                                 add_9[0][0]                      \n",
      "                                                                 lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 20, 32)       64          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 20, 32)       1056        layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 20, 32)       0           sequential_8[0][0]               \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 20, 32)       64          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 51)           1683        global_average_pooling1d_2[0][0] \n",
      "==================================================================================================\n",
      "Total params: 233,907\n",
      "Trainable params: 233,907\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (20,60,60,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(20, 32 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=32, dropout=0.2 )  (x1, x1)\n",
    "    \n",
    "    lstm_output =  LSTM(32,return_sequences=True,dropout=0.2)(x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches,lstm_output])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "#representation = LSTM(100,return_sequences=False,dropout=0.1)(representation)\n",
    "\n",
    "outputs = layers.Dense(units=51, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8262812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "optimizer = keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2676f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chsha\\AppData\\Local\\Temp/ipykernel_13568/2168406815.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "123/123 [==============================] - 2209s 18s/step - loss: 0.5173 - accuracy: 0.9180 - val_loss: 2.6226 - val_accuracy: 0.4353\n",
      "Epoch 2/2\n",
      "123/123 [==============================] - 2159s 18s/step - loss: 0.4737 - accuracy: 0.9318 - val_loss: 2.6430 - val_accuracy: 0.4246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16fade6f790>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 2,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef7bac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n",
      "C:\\Users\\chsha\\AppData\\Local\\Temp/ipykernel_20108/1543328956.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(My_Test_Generator(test, batch_size), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b4d89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "p = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d68227f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03522504892367906"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, p)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2f5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acfadae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847db778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a0cf3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split( train_df,YY_Train , test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4cc7dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
    "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
    "    # Preprocess images\n",
    "    frames = tf.image.convert_image_dtype(\n",
    "        frames[\n",
    "            ..., tf.newaxis\n",
    "        ],  # The new axis is to help for further processing with Conv3D layers\n",
    "        tf.float32,\n",
    "    )\n",
    "    # Parse label\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return frames, label\n",
    "\n",
    "\n",
    "def prepare_dataloader(\n",
    "    videos: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    loader_type: str = \"train\",\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "):\n",
    "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
    "\n",
    "    if loader_type == \"train\":\n",
    "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
    "\n",
    "    dataloader = (\n",
    "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "trainloader = prepare_dataloader(X_train , y_train , \"train\")\n",
    "validloader = prepare_dataloader(X_val, y_val, \"valid\")\n",
    "testloader = prepare_dataloader(test_df,YY_Test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0523831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TubeletEmbedding(layers.Layer):\n",
    "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.projection = layers.Conv3D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "\n",
    "    def call(self, videos):\n",
    "        projected_patches = self.projection(videos)\n",
    "        flattened_patches = self.flatten(projected_patches)\n",
    "        return flattened_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f4f081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionalEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, num_tokens, _ = input_shape\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_tokens, output_dim=self.embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=num_tokens, delta=1)\n",
    "\n",
    "    def call(self, encoded_tokens):\n",
    "        # Encode the positions and add it to the encoded tokens\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_tokens = encoded_tokens + encoded_positions\n",
    "        return encoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6b82757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vivit_classifier(\n",
    "    tubelet_embedder,\n",
    "    positional_encoder,\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    transformer_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embed_dim=PROJECTION_DIM,\n",
    "    layer_norm_eps=LAYER_NORM_EPS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "):\n",
    "    # Get the input layer\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Create patches.\n",
    "    patches = tubelet_embedder(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = positional_encoder(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization and MHSA\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
    "        )(x1, x1)\n",
    "\n",
    "        # Skip connection\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "        # Layer Normalization and MLP\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=embed_dim * 4, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=embed_dim, activation=tf.nn.gelu),\n",
    "            ]\n",
    "        )(x3)\n",
    "\n",
    "        # Skip connection\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Layer normalization and Global average pooling.\n",
    "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
    "    representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "    # Classify outputs.\n",
    "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28803bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 6, 28, 28, 3 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tubelet_embedding_11 (TubeletEm (None, 9, 128)       98432       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "positional_encoder_11 (Position (None, 9, 128)       1152        tubelet_embedding_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, 9, 128)       256         positional_encoder_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_38 (MultiH (None, 9, 128)       66048       layer_normalization_95[0][0]     \n",
      "                                                                 layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 9, 128)       0           multi_head_attention_38[0][0]    \n",
      "                                                                 positional_encoder_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_96 (LayerNo (None, 9, 128)       256         add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_38 (Sequential)      (None, 9, 128)       131712      layer_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 9, 128)       0           sequential_38[0][0]              \n",
      "                                                                 add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_97 (LayerNo (None, 9, 128)       256         add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multi_head_attention_39 (MultiH (None, 9, 128)       66048       layer_normalization_97[0][0]     \n",
      "                                                                 layer_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 9, 128)       0           multi_head_attention_39[0][0]    \n",
      "                                                                 add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_98 (LayerNo (None, 9, 128)       256         add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "sequential_39 (Sequential)      (None, 9, 128)       131712      layer_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 9, 128)       0           sequential_39[0][0]              \n",
      "                                                                 add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_99 (LayerNo (None, 9, 128)       256         add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           layer_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_76 (Dense)                (None, 2)            258         global_average_pooling1d_11[0][0]\n",
      "==================================================================================================\n",
      "Total params: 496,642\n",
      "Trainable params: 496,642\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TUBELET EMBEDDING\n",
    "PATCH_SIZE = (4, 8, 8)\n",
    "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
    "\n",
    "NUM_HEADS = 2\n",
    "NUM_LAYERS = 2\n",
    "# TRAINING\n",
    "EPOCHS = 100\n",
    "\n",
    "md = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "md.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf326e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60360ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6aa5b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "77/77 [==============================] - 21s 100ms/step - loss: 0.4438 - accuracy: 0.8424 - top-5-accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 0.8206 - val_top-5-accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.3220 - accuracy: 0.8571 - top-5-accuracy: 1.0000 - val_loss: 0.2746 - val_accuracy: 0.8679 - val_top-5-accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.2685 - accuracy: 0.8812 - top-5-accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.8874 - val_top-5-accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.1945 - accuracy: 0.9147 - top-5-accuracy: 1.0000 - val_loss: 0.2213 - val_accuracy: 0.9119 - val_top-5-accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.1727 - accuracy: 0.9265 - top-5-accuracy: 1.0000 - val_loss: 0.2144 - val_accuracy: 0.9086 - val_top-5-accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.1640 - accuracy: 0.9294 - top-5-accuracy: 1.0000 - val_loss: 0.2185 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.1650 - accuracy: 0.9318 - top-5-accuracy: 1.0000 - val_loss: 0.2061 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1682 - accuracy: 0.9326 - top-5-accuracy: 1.0000 - val_loss: 0.1946 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.1533 - accuracy: 0.9347 - top-5-accuracy: 1.0000 - val_loss: 0.1995 - val_accuracy: 0.9086 - val_top-5-accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1515 - accuracy: 0.9392 - top-5-accuracy: 1.0000 - val_loss: 0.1833 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "77/77 [==============================] - 6s 77ms/step - loss: 0.1439 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.2156 - val_accuracy: 0.9038 - val_top-5-accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.1444 - accuracy: 0.9457 - top-5-accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.1286 - accuracy: 0.9502 - top-5-accuracy: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.1529 - accuracy: 0.9400 - top-5-accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.1377 - accuracy: 0.9469 - top-5-accuracy: 1.0000 - val_loss: 0.2025 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.1164 - accuracy: 0.9543 - top-5-accuracy: 1.0000 - val_loss: 0.2137 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1148 - accuracy: 0.9575 - top-5-accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.1081 - accuracy: 0.9543 - top-5-accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.1072 - accuracy: 0.9555 - top-5-accuracy: 1.0000 - val_loss: 0.2117 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0989 - accuracy: 0.9637 - top-5-accuracy: 1.0000 - val_loss: 0.2397 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0997 - accuracy: 0.9608 - top-5-accuracy: 1.0000 - val_loss: 0.2085 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.1015 - accuracy: 0.9563 - top-5-accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9364 - val_top-5-accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0888 - accuracy: 0.9633 - top-5-accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0983 - accuracy: 0.9620 - top-5-accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 0.9429 - val_top-5-accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 0.0908 - accuracy: 0.9592 - top-5-accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "77/77 [==============================] - 7s 84ms/step - loss: 0.0856 - accuracy: 0.9669 - top-5-accuracy: 1.0000 - val_loss: 0.3156 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000 \n",
      "Epoch 27/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.1158 - accuracy: 0.9563 - top-5-accuracy: 1.0000 - val_loss: 0.2438 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0791 - accuracy: 0.9653 - top-5-accuracy: 1.0000 - val_loss: 0.2555 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0864 - accuracy: 0.9641 - top-5-accuracy: 1.0000 - val_loss: 0.2609 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "77/77 [==============================] - 8s 101ms/step - loss: 0.0829 - accuracy: 0.9653 - top-5-accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0760 - accuracy: 0.9694 - top-5-accuracy: 1.0000 - val_loss: 0.2729 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0746 - accuracy: 0.9698 - top-5-accuracy: 1.0000 - val_loss: 0.2657 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0709 - accuracy: 0.9743 - top-5-accuracy: 1.0000 - val_loss: 0.2973 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0623 - accuracy: 0.9780 - top-5-accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "77/77 [==============================] - 7s 87ms/step - loss: 0.0710 - accuracy: 0.9673 - top-5-accuracy: 1.0000 - val_loss: 0.3120 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0865 - accuracy: 0.9628 - top-5-accuracy: 1.0000 - val_loss: 0.2990 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0697 - accuracy: 0.9726 - top-5-accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.0651 - accuracy: 0.9718 - top-5-accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 0.0517 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.3003 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "77/77 [==============================] - 5s 69ms/step - loss: 0.0587 - accuracy: 0.9767 - top-5-accuracy: 1.0000 - val_loss: 0.3270 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0625 - accuracy: 0.9743 - top-5-accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0583 - accuracy: 0.9771 - top-5-accuracy: 1.0000 - val_loss: 0.3304 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0524 - accuracy: 0.9808 - top-5-accuracy: 1.0000 - val_loss: 0.3820 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "77/77 [==============================] - 6s 72ms/step - loss: 0.0595 - accuracy: 0.9780 - top-5-accuracy: 1.0000 - val_loss: 0.3279 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0425 - accuracy: 0.9845 - top-5-accuracy: 1.0000 - val_loss: 0.3356 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0394 - accuracy: 0.9841 - top-5-accuracy: 1.0000 - val_loss: 0.4004 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0437 - accuracy: 0.9820 - top-5-accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0421 - accuracy: 0.9812 - top-5-accuracy: 1.0000 - val_loss: 0.4765 - val_accuracy: 0.9152 - val_top-5-accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.0599 - accuracy: 0.9763 - top-5-accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.0353 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0357 - accuracy: 0.9861 - top-5-accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "77/77 [==============================] - 5s 69ms/step - loss: 0.0357 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0395 - accuracy: 0.9824 - top-5-accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0337 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.5375 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.0495 - accuracy: 0.9808 - top-5-accuracy: 1.0000 - val_loss: 0.3832 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "77/77 [==============================] - 5s 69ms/step - loss: 0.0615 - accuracy: 0.9751 - top-5-accuracy: 1.0000 - val_loss: 0.3939 - val_accuracy: 0.9266 - val_top-5-accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0297 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.3771 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0341 - accuracy: 0.9886 - top-5-accuracy: 1.0000 - val_loss: 0.4383 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0308 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.3892 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0349 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.3463 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0236 - accuracy: 0.9898 - top-5-accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0308 - accuracy: 0.9886 - top-5-accuracy: 1.0000 - val_loss: 0.4533 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0491 - accuracy: 0.9829 - top-5-accuracy: 1.0000 - val_loss: 0.4259 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "77/77 [==============================] - 7s 92ms/step - loss: 0.0249 - accuracy: 0.9914 - top-5-accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "77/77 [==============================] - 8s 98ms/step - loss: 0.0383 - accuracy: 0.9849 - top-5-accuracy: 1.0000 - val_loss: 0.5387 - val_accuracy: 0.9005 - val_top-5-accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0476 - accuracy: 0.9833 - top-5-accuracy: 1.0000 - val_loss: 0.4249 - val_accuracy: 0.9184 - val_top-5-accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "77/77 [==============================] - 6s 75ms/step - loss: 0.0276 - accuracy: 0.9865 - top-5-accuracy: 1.0000 - val_loss: 0.6276 - val_accuracy: 0.8940 - val_top-5-accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0324 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0207 - accuracy: 0.9910 - top-5-accuracy: 1.0000 - val_loss: 0.4485 - val_accuracy: 0.9152 - val_top-5-accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0320 - accuracy: 0.9886 - top-5-accuracy: 1.0000 - val_loss: 0.5643 - val_accuracy: 0.9135 - val_top-5-accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "77/77 [==============================] - 6s 81ms/step - loss: 0.0308 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 0.4870 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0241 - accuracy: 0.9914 - top-5-accuracy: 1.0000 - val_loss: 0.4422 - val_accuracy: 0.9233 - val_top-5-accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0267 - accuracy: 0.9914 - top-5-accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "77/77 [==============================] - 6s 84ms/step - loss: 0.0273 - accuracy: 0.9910 - top-5-accuracy: 1.0000 - val_loss: 0.5979 - val_accuracy: 0.9038 - val_top-5-accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0259 - accuracy: 0.9910 - top-5-accuracy: 1.0000 - val_loss: 0.4108 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0241 - accuracy: 0.9894 - top-5-accuracy: 1.0000 - val_loss: 0.4822 - val_accuracy: 0.9201 - val_top-5-accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0202 - accuracy: 0.9939 - top-5-accuracy: 1.0000 - val_loss: 0.4651 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "77/77 [==============================] - 6s 76ms/step - loss: 0.0246 - accuracy: 0.9922 - top-5-accuracy: 1.0000 - val_loss: 0.6098 - val_accuracy: 0.8956 - val_top-5-accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "77/77 [==============================] - 6s 73ms/step - loss: 0.0440 - accuracy: 0.9849 - top-5-accuracy: 1.0000 - val_loss: 0.5318 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0208 - accuracy: 0.9910 - top-5-accuracy: 1.0000 - val_loss: 0.5259 - val_accuracy: 0.9038 - val_top-5-accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0191 - accuracy: 0.9918 - top-5-accuracy: 1.0000 - val_loss: 0.5169 - val_accuracy: 0.8989 - val_top-5-accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "77/77 [==============================] - 7s 91ms/step - loss: 0.0277 - accuracy: 0.9910 - top-5-accuracy: 1.0000 - val_loss: 0.4289 - val_accuracy: 0.9103 - val_top-5-accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "77/77 [==============================] - 8s 100ms/step - loss: 0.0269 - accuracy: 0.9898 - top-5-accuracy: 1.0000 - val_loss: 0.4557 - val_accuracy: 0.9217 - val_top-5-accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 0.0168 - accuracy: 0.9935 - top-5-accuracy: 1.0000 - val_loss: 0.5888 - val_accuracy: 0.9054 - val_top-5-accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "77/77 [==============================] - 7s 96ms/step - loss: 0.0504 - accuracy: 0.9820 - top-5-accuracy: 1.0000 - val_loss: 0.4264 - val_accuracy: 0.9152 - val_top-5-accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "77/77 [==============================] - 6s 79ms/step - loss: 0.0142 - accuracy: 0.9967 - top-5-accuracy: 1.0000 - val_loss: 0.3633 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0139 - accuracy: 0.9951 - top-5-accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.9347 - val_top-5-accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0079 - accuracy: 0.9976 - top-5-accuracy: 1.0000 - val_loss: 0.4113 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 0.0117 - accuracy: 0.9951 - top-5-accuracy: 1.0000 - val_loss: 0.4026 - val_accuracy: 0.9331 - val_top-5-accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "77/77 [==============================] - 7s 93ms/step - loss: 0.0184 - accuracy: 0.9927 - top-5-accuracy: 1.0000 - val_loss: 0.4382 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "77/77 [==============================] - 6s 83ms/step - loss: 0.0294 - accuracy: 0.9873 - top-5-accuracy: 1.0000 - val_loss: 0.5151 - val_accuracy: 0.9168 - val_top-5-accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "77/77 [==============================] - 6s 82ms/step - loss: 0.0299 - accuracy: 0.9898 - top-5-accuracy: 1.0000 - val_loss: 0.4056 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0106 - accuracy: 0.9967 - top-5-accuracy: 1.0000 - val_loss: 0.4235 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "77/77 [==============================] - 8s 99ms/step - loss: 0.0080 - accuracy: 0.9976 - top-5-accuracy: 1.0000 - val_loss: 0.3944 - val_accuracy: 0.9315 - val_top-5-accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "77/77 [==============================] - 9s 120ms/step - loss: 0.0100 - accuracy: 0.9971 - top-5-accuracy: 1.0000 - val_loss: 0.4739 - val_accuracy: 0.9086 - val_top-5-accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.0189 - accuracy: 0.9931 - top-5-accuracy: 1.0000 - val_loss: 0.3907 - val_accuracy: 0.9250 - val_top-5-accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0087 - accuracy: 0.9959 - top-5-accuracy: 1.0000 - val_loss: 0.4038 - val_accuracy: 0.9282 - val_top-5-accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "77/77 [==============================] - 5s 70ms/step - loss: 0.0153 - accuracy: 0.9935 - top-5-accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "77/77 [==============================] - 5s 71ms/step - loss: 0.0172 - accuracy: 0.9943 - top-5-accuracy: 1.0000 - val_loss: 0.6344 - val_accuracy: 0.9070 - val_top-5-accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "77/77 [==============================] - 6s 74ms/step - loss: 0.0271 - accuracy: 0.9906 - top-5-accuracy: 1.0000 - val_loss: 0.4167 - val_accuracy: 0.9299 - val_top-5-accuracy: 1.0000\n",
      "41/41 [==============================] - 2s 28ms/step - loss: 0.0922 - accuracy: 0.9809 - top-5-accuracy: 1.0000\n",
      "Test accuracy: 98.09%\n",
      "Test top 5 accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment():\n",
    "    # Initialize model\n",
    "    model = create_vivit_classifier(\n",
    "        tubelet_embedder=TubeletEmbedding(\n",
    "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
    "        ),\n",
    "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
    "    )\n",
    "\n",
    "    # Compile the model with the optimizer, loss function\n",
    "    # and the metrics.\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
    "\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4c6df61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1105,   21],\n",
       "       [   4,  180]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY_Test = YY_Test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = YY_Test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f7d70cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935064935064935"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a3620fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398567119155354"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f75542c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "199466a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train_df[19]\n",
    "y = train_df[3]\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "im1 = np.arange(100).reshape((10, 10))\n",
    "im2 = im1.T\n",
    "im3 = np.flipud(im1)\n",
    "im4 = np.fliplr(im2)\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [x[0], x[1], x[2], x[3],y[0], y[1], y[2], y[3]]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3eb90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257102bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6633bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "027188ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, ZeroPadding3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea8a42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv3D)               (None, 10, 43, 40, 2)     164       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 10, 21, 20, 2)     0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv3D)               (None, 10, 21, 20, 4)     220       \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 5, 10, 10, 4)      0         \n",
      "_________________________________________________________________\n",
      "conv3a (Conv3D)              (None, 5, 10, 10, 8)      872       \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 2, 5, 5, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv4a (Conv3D)              (None, 2, 5, 5, 16)       3472      \n",
      "_________________________________________________________________\n",
      "poodfl3 (MaxPooling3D)       (None, 1, 2, 2, 16)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "fc6 (Dense)                  (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 51)                5151      \n",
      "=================================================================\n",
      "Total params: 16,379\n",
      "Trainable params: 16,379\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(2, (3, 3, 3), activation=\"relu\",name=\"conv1\",   input_shape=(10,43,40,3), strides=(1, 1, 1), padding=\"same\"))  \n",
    "model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name=\"pool1\", padding=\"valid\"))\n",
    "model.add(Conv3D(4, (3, 3, 3), activation=\"relu\",name=\"conv2\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool2\", padding=\"valid\"))\n",
    "model.add(Conv3D(8, (3, 3, 3), activation=\"relu\",name=\"conv3a\", strides=(1, 1, 1), padding=\"same\"))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"pool3\", padding=\"valid\"))\n",
    "model.add(Conv3D(16, (3, 3, 3), activation=\"relu\",name=\"conv4a\", strides=(1, 1, 1), padding=\"same\")) \n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), name=\"poodfl3\", padding=\"valid\"))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "                     \n",
    "    # FC layers group\n",
    "model.add(Dense(100, activation='relu', name='fc6'))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(51, activation='softmax', name='fc8'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1b41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chsha\\AppData\\Local\\Temp/ipykernel_19652/2168406815.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "122/122 [==============================] - 580s 5s/step - loss: 3.9375 - accuracy: 0.0643 - val_loss: 3.9140 - val_accuracy: 0.0883\n",
      "Epoch 2/100\n",
      "101/122 [=======================>......] - ETA: 1:23 - loss: 3.9179 - accuracy: 0.0597"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 100,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = Y_test\n",
    "pred = model.predict(test_df)\n",
    "p = np.round(pred)\n",
    "f1 = get_f1(Y_test, p)\n",
    "f1\n",
    "\n",
    "y_p = []\n",
    "for i in range(len(p)):\n",
    "    if ( p[i][0] == 0 ):\n",
    "        y_p.append(1)\n",
    "    else :\n",
    "        y_p.append(0)\n",
    "y_p = np.array(y_p)\n",
    "y_t = []\n",
    "for i in range(len(Y_test)):\n",
    "    if ( Y_test[i][0] == 0 ):\n",
    "        y_t.append(1)\n",
    "    else :\n",
    "        y_t.append(0)\n",
    "y_t = np.array(y_t)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "a=(confusion_matrix(y_t, y_p , labels=[0,1]))\n",
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5845ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = a[1][1]/(((a[0][1]+a[1][0])/2)+a[1][1])\n",
    "b = 1 - ( (a[1][0]/a[1][1] + a[0][1]/a[0][0])/2  )\n",
    "r = 1 - (  a[1][1]  / (a[1][1] + a[0][1])  )\n",
    "print(f,b,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fd8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f6883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c412a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b058ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43530db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99db6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c7b30013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "46bf04e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7480/615670820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mencoded_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mPatchEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7480/456504370.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_patches, projection_dim)\u001b[0m\n\u001b[0;32m      7\u001b[0m                 \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             ]) \n\u001b[1;32m----> 9\u001b[1;33m         self.position_embedding = layers.Embedding(\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprojection_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\embeddings.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, activity_regularizer, embeddings_constraint, mask_zero, input_length, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput_dim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m       raise ValueError('Both `input_dim` and `output_dim` should be positive, '\n\u001b[0;32m    123\u001b[0m                        'found input_dim {} and output_dim {}'.format(\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "inputs = layers.Input(shape= (10,10) )\n",
    "encoded_patches = (PatchEncoder(10, 64 )) (inputs)\n",
    "\n",
    "for _ in range(2):\n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    attention_output = layers.MultiHeadAttention (  num_heads=2, key_dim=64, dropout=0.1 )  (x1, x1)\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "    x3 = keras.Sequential(LSTM(64,return_sequences=False,dropout=0.1) )(x3)\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "outputs = layers.Dense(units=1, activation=\"softmax\") ( representation)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5bac3482",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.ones([50], dtype=float)\n",
    "e = np.zeros([50], dtype=float)\n",
    "f =  np.concatenate((d,e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cdd7ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chsha\\AppData\\Local\\Temp/ipykernel_7480/98547960.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  y = np.array(x)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(0,50):\n",
    "    x.append(np.ones([5,10], dtype=float))\n",
    "for i in range(0,50):\n",
    "    x.append(np.zeros([10,10], dtype=float))\n",
    "y = np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196759ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "84a5bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 853, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 842, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 835, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 787, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1020, in __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 266, in assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model_4: expected shape=(None, 10, 10), found shape=(20, 5, 10)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7480/3369147149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 853, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 842, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 835, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 787, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1020, in __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    File \"C:\\Users\\chsha\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 266, in assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer model_4: expected shape=(None, 10, 10), found shape=(20, 5, 10)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(X,f,validation_split=0.2, epochs = 20,batch_size =20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4f0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250da400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
