{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ae3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acbbce13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn='D:/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3b2ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e3bfee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0b3bdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the resized images array: (239, 10, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# Initialize a numpy array to store the resized images\n",
    "resized_images = np.zeros((len(test), 10, 64, 64, 3), dtype=np.uint8)\n",
    "\n",
    "# Loop through each folder in the \"test\" array\n",
    "for i, folder in enumerate(test):\n",
    "\n",
    "    # Loop through the first 10 images in the current folder\n",
    "    for j, filename in enumerate(os.listdir(folder)[:10]):\n",
    "\n",
    "        # Load the image and resize it to 64x64 pixels\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        img = cv2.resize(img, (64, 64))\n",
    "\n",
    "        # Store the resized image in the numpy array\n",
    "        resized_images[i, j] = img\n",
    "\n",
    "X_test = resized_images\n",
    "print(\"Shape of the resized images array:\", np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011538c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764a45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    imgdata=[]\n",
    "    higher = len(x)\n",
    "    import more_itertools as mit\n",
    "    \n",
    "    y = mit.random_combination(range(0, higher), r=10)\n",
    "\n",
    "    for i in y:\n",
    "        \n",
    "        a = Image.open(x[i])\n",
    "        b = a.resize((64, 64))\n",
    "        c = np.array(b)\n",
    "        imgdata.append(c.reshape(64,64,3))\n",
    "        \n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata\n",
    "    X_train = X_train.astype('float32') / 255.\n",
    "    #print(np.shape(X_train))\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09b055ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "995461c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cb5d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faa3e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = my_training_batch_generator.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe4941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859263e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7f2b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_7 (TimeDis  (None, 10, 62, 62, 32)   896       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 10, 31, 31, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 10, 29, 29, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 10, 14, 14, 64)   0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 10, 12, 12, 128)  73856     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_12 (TimeDi  (None, 10, 6, 6, 128)    0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_13 (TimeDi  (None, 10, 4608)         0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 25)                463400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 557,038\n",
      "Trainable params: 557,038\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "\n",
    "num_frames = 10\n",
    "frame_height = 64\n",
    "frame_width = 64\n",
    "num_channels = 3\n",
    "num_classes = 15  # Change this to the number of classes in your dataset\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# CNN\n",
    "model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(num_frames, frame_height, frame_width, num_channels)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# LSTM\n",
    "model.add(LSTM(25))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "161da374",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "optimizer = keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chowd\\AppData\\Local\\Temp\\ipykernel_4068\\1675710652.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=my_training_batch_generator, epochs = 30,validation_data = my_validation_batch_generator)\n",
      "C:\\Users\\chowd\\AppData\\Local\\Temp\\ipykernel_4068\\2168406815.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "59/59 [==============================] - 104s 2s/step - loss: 2.6161 - accuracy: 0.1242 - val_loss: 2.5594 - val_accuracy: 0.1682\n",
      "Epoch 2/30\n",
      "59/59 [==============================] - 90s 2s/step - loss: 2.3596 - accuracy: 0.2148 - val_loss: 2.3306 - val_accuracy: 0.2430\n",
      "Epoch 3/30\n",
      "59/59 [==============================] - 89s 2s/step - loss: 2.1458 - accuracy: 0.3038 - val_loss: 2.2042 - val_accuracy: 0.2617\n",
      "Epoch 4/30\n",
      "59/59 [==============================] - 88s 1s/step - loss: 1.9733 - accuracy: 0.3737 - val_loss: 1.9606 - val_accuracy: 0.3832\n",
      "Epoch 5/30\n",
      "59/59 [==============================] - 88s 1s/step - loss: 1.7683 - accuracy: 0.4350 - val_loss: 1.8846 - val_accuracy: 0.4112\n",
      "Epoch 6/30\n",
      "59/59 [==============================] - 87s 1s/step - loss: 1.5436 - accuracy: 0.5096 - val_loss: 1.7927 - val_accuracy: 0.4393\n",
      "Epoch 7/30\n",
      "59/59 [==============================] - 85s 1s/step - loss: 1.4046 - accuracy: 0.5741 - val_loss: 1.8585 - val_accuracy: 0.4486\n",
      "Epoch 8/30\n",
      "14/59 [======>.......................] - ETA: 57s - loss: 1.2430 - accuracy: 0.6468"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 30,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d69c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(My_Test_Generator(X_test)\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import Ofrom sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, predictions)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd108b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "neHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "p = np.argmax(predictions, axis=1)\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(test_y, p)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b3b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2105e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super(PatchEncoder, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"projection_dim\": self.projection_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "\n",
    "inputs = layers.Input(shape= (10,64,64,3) )\n",
    "\n",
    "encoded_patches = (PatchEncoder(5, 32 )) (inputs)\n",
    "\n",
    "for _ in range(6):\n",
    "    \n",
    "    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "    attention_output= layers.MultiHeadAttention (  num_heads=6, key_dim=32, dropout=0.1 )  (x1, x1)\n",
    "\n",
    "    x2 = layers.Add()([attention_output, encoded_patches])\n",
    "\n",
    "    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "\n",
    "    x3 = keras.Sequential(layers.Dense(units=32, activation=tf.nn.gelu) )(x3)\n",
    "\n",
    "    encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "\n",
    "representation = layers.GlobalAvgPool1D()(representation)\n",
    "\n",
    "outputs = layers.Dense(units=3, activation=\"softmax\") ( representation)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e73b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "optimizer = keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.fit_generator(generator=my_training_batch_generator, epochs = 30,validation_data = my_validation_batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b131c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93124652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
