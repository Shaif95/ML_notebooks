{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15e2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47503c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-model-optimization tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c11495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c0c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 5\n",
      "x_train shape: (2936, 128, 128, 3)\n",
      "y_train shape: (2936, 5)\n",
      "x_test  shape: (734, 128, 128, 3)\n",
      "y_test  shape: (734, 5)\n",
      "num_classes: 5, nums: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet50,\n",
    "    EfficientNetB0,\n",
    "    resnet50,\n",
    "    efficientnet\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Load the tf_flowers Dataset\n",
    "# ---------------------------------------------------\n",
    "# Create your own train/test split: 80% / 20%\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Number of classes in tf_flowers\n",
    "num_classes = info.features['label'].num_classes\n",
    "nums = num_classes  # keep both variables as you did before\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Convert to NumPy arrays (Optionally Resize)\n",
    "# ---------------------------------------------------\n",
    "IMG_SIZE = 128\n",
    "\n",
    "x_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    # Resize to reduce memory usage\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "x_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "x_train = np.stack(x_train_list).astype(\"float32\")\n",
    "y_train = np.array(y_train_list)\n",
    "x_test  = np.stack(x_test_list).astype(\"float32\")\n",
    "y_test  = np.array(y_test_list)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Normalize & One-Hot Encode Labels\n",
    "# ---------------------------------------------------\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train /= 255.0\n",
    "x_test  /= 255.0\n",
    "\n",
    "# Convert integer labels to one-hot vectors\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test  shape: {x_test.shape}\")\n",
    "print(f\"y_test  shape: {y_test.shape}\")\n",
    "print(f\"num_classes: {num_classes}, nums: {nums}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee5ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8492c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning ResNet50 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "92/92 [==============================] - 48s 95ms/step - loss: 1.6381 - accuracy: 0.3004 - val_loss: 1.5210 - val_accuracy: 0.3351\n",
      "Epoch 2/3\n",
      "92/92 [==============================] - 6s 68ms/step - loss: 1.5180 - accuracy: 0.3532 - val_loss: 1.5176 - val_accuracy: 0.3229\n",
      "Epoch 3/3\n",
      "92/92 [==============================] - 7s 73ms/step - loss: 1.5082 - accuracy: 0.3420 - val_loss: 1.4880 - val_accuracy: 0.3665\n",
      "ResNet50 - CIFAR-100 Accuracy: 0.3665\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll define two separate pipelines:\n",
    "#   - One for ResNet50\n",
    "#   - One for EfficientNetB0\n",
    "\n",
    "# ------------------------------\n",
    "# 2. tf.data Pipeline for ResNet\n",
    "# ------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    # Resize and apply ResNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = resnet50.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Create training dataset\n",
    "train_ds_resnet = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_resnet = train_ds_resnet.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# Create validation (test) dataset\n",
    "val_ds_resnet = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_resnet = val_ds_resnet.map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Define Baseline Models to Fine-Tune\n",
    "# ------------------------------------------\n",
    "def create_resnet50_finetune(input_shape, num_classes):\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5. Fine-Tune ResNet50 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "resnet_model = create_resnet50_finetune((224, 224, 3), num_classes)\n",
    "resnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning ResNet50 on CIFAR-100 ---\")\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_ds_resnet,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds_resnet,\n",
    "    verbose=1\n",
    ")\n",
    "resnet_loss, resnet_acc = resnet_model.evaluate(val_ds_resnet, verbose=0)\n",
    "print(f\"ResNet50 - CIFAR-100 Accuracy: {resnet_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb5f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 109s 67ms/step - loss: 0.5143 - accuracy: 0.8315 - val_loss: 0.3506 - val_accuracy: 0.8838\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 100s 64ms/step - loss: 0.2873 - accuracy: 0.9016 - val_loss: 0.3390 - val_accuracy: 0.8914\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 96s 62ms/step - loss: 0.2076 - accuracy: 0.9289 - val_loss: 0.3440 - val_accuracy: 0.8962\n",
      "EfficientNetB0 - CIFAR-100 Accuracy: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 3. tf.data Pipeline for EfficientNet\n",
    "# ------------------------------\n",
    "def preprocess_efficientnet(image, label):\n",
    "    # Resize and apply EfficientNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds_eff = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_eff = train_ds_eff.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_eff = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_eff = val_ds_eff.map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "def create_efficientnet_finetune(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6. Fine-Tune EfficientNetB0 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "eff_model = create_efficientnet_finetune((224, 224, 3), num_classes)\n",
    "eff_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\")\n",
    "history_eff = eff_model.fit(\n",
    "    train_ds_eff,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds_eff,\n",
    "    verbose=1\n",
    ")\n",
    "eff_loss, eff_acc = eff_model.evaluate(val_ds_eff, verbose=0)\n",
    "print(f\"EfficientNetB0 - CIFAR-100 Accuracy: {eff_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbd444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f129143",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Magnitude-Based Pruning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eda9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_pruned_efficientnet(input_shape, num_classes, pruning_params):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=pruning_params[\"initial_sparsity\"],\n",
    "        final_sparsity=pruning_params[\"final_sparsity\"],\n",
    "        begin_step=pruning_params[\"begin_step\"],\n",
    "        end_step=pruning_params[\"end_step\"]\n",
    "    )\n",
    "\n",
    "    # Apply pruning to the dense layers\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = tfmot.sparsity.keras.prune_low_magnitude(\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        pruning_schedule=pruning_schedule\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define pruning parameters\n",
    "pruning_params = {\n",
    "    \"initial_sparsity\": 0.1,  # Start pruning at 10%\n",
    "    \"final_sparsity\": 0.5,    # End pruning at 50%\n",
    "    \"begin_step\": 100,        # Start pruning after 100 steps\n",
    "    \"end_step\": 3000          # End pruning at step 3000\n",
    "}\n",
    "\n",
    "pruned_model = create_pruned_efficientnet((224, 224, 3), num_classes, pruning_params)\n",
    "pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Train the Pruned Model\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- Fine-Tuning Pruned EfficientNetB0 on TF Flowers ---\")\n",
    "\n",
    "history_pruned = pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Evaluate the Pruned Model\n",
    "# ---------------------------------------------\n",
    "pruned_loss, pruned_acc = pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Pruned EfficientNetB0 - TF Flowers Accuracy: {pruned_acc:.4f}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Strip Pruning for Inference\n",
    "# ---------------------------------------------\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "final_model.save(\"efficientnet_pruned.h5\")  # Save the pruned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4b87a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7afceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Random Pruning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33faee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import random\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Random Pruning Function\n",
    "# ---------------------------------------------\n",
    "def random_prune_weights(weights, prune_ratio):\n",
    "    \"\"\"\n",
    "    Randomly prunes a given percentage of weights.\n",
    "    \"\"\"\n",
    "    mask = np.ones_like(weights)\n",
    "    total_weights = weights.size\n",
    "    num_prune = int(total_weights * prune_ratio)\n",
    "    \n",
    "    indices = random.sample(range(total_weights), num_prune)\n",
    "    np.put(mask, indices, 0)  # Set randomly selected weights to zero\n",
    "    \n",
    "    return weights * mask  # Apply mask\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Define Randomly Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_random_pruned_efficientnet(input_shape, num_classes, prune_ratio=0.5):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    dense1 = layers.Dense(256, activation='relu')\n",
    "    x = dense1(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    # Apply random pruning to the dense layers\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, layers.Dense):\n",
    "            weights, biases = layer.get_weights()\n",
    "            pruned_weights = random_prune_weights(weights, prune_ratio)\n",
    "            layer.set_weights([pruned_weights, biases])\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Train the Randomly Pruned Model\n",
    "# ---------------------------------------------\n",
    "prune_ratio = 0.5  # 50% of the weights are randomly pruned\n",
    "\n",
    "random_pruned_model = create_random_pruned_efficientnet((224, 224, 3), num_classes, prune_ratio)\n",
    "random_pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Randomly Pruned EfficientNetB0 on TF Flowers ---\")\n",
    "\n",
    "history_random_pruned = random_pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Evaluate the Randomly Pruned Model\n",
    "# ---------------------------------------------\n",
    "random_pruned_loss, random_pruned_acc = random_pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Randomly Pruned EfficientNetB0 - TF Flowers Accuracy: {random_pruned_acc:.4f}\")\n",
    "\n",
    "# Save the final pruned model\n",
    "random_pruned_model.save(\"efficientnet_random_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bce09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Gradient-Based Pruning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcdd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Function to Compute Weight Importance\n",
    "# ---------------------------------------------\n",
    "def compute_gradient_importance(model, train_data, prune_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Computes the gradient-based importance of weights.\n",
    "    Prunes weights with lowest gradient magnitudes.\n",
    "    \"\"\"\n",
    "    # Get model weights\n",
    "    layer_weights = [layer.get_weights() for layer in model.layers if isinstance(layer, layers.Dense)]\n",
    "    \n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = sum(model.loss(y, model(x)) for x, y in train_data) / len(train_data)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # Get absolute values of gradients\n",
    "    grad_magnitudes = [tf.abs(g) for g in grads if g is not None]\n",
    "\n",
    "    # Apply pruning\n",
    "    for (weights, biases), grad in zip(layer_weights, grad_magnitudes):\n",
    "        threshold = tf.math.top_k(tf.reshape(grad, [-1]), int(prune_ratio * tf.size(grad).numpy())).values[-1]\n",
    "        mask = tf.cast(grad >= threshold, tf.float32)\n",
    "        pruned_weights = weights * mask.numpy()  # Set less important weights to zero\n",
    "        \n",
    "        # Set new pruned weights\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, layers.Dense):\n",
    "                layer.set_weights([pruned_weights, biases])\n",
    "\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Define EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_efficientnet(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Train the Model Before Pruning\n",
    "# ---------------------------------------------\n",
    "prune_ratio = 0.5  # 50% of weights will be pruned\n",
    "\n",
    "# Create model\n",
    "gradient_pruned_model = create_efficientnet((224, 224, 3), num_classes)\n",
    "gradient_pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Pretraining EfficientNetB0 Before Pruning ---\")\n",
    "\n",
    "history = gradient_pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=1,  # Pre-train for 1 epoch to compute gradients\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Apply Gradient-Based Pruning\n",
    "# ---------------------------------------------\n",
    "gradient_pruned_model = compute_gradient_importance(gradient_pruned_model, train_ds, prune_ratio)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 8. Train the Pruned Model\n",
    "# ---------------------------------------------\n",
    "print(\"\\n--- Fine-Tuning Gradient-Based Pruned EfficientNetB0 ---\")\n",
    "\n",
    "history_pruned = gradient_pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 9. Evaluate the Pruned Model\n",
    "# ---------------------------------------------\n",
    "pruned_loss, pruned_acc = gradient_pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Gradient-Based Pruned EfficientNetB0 - TF Flowers Accuracy: {pruned_acc:.4f}\")\n",
    "\n",
    "# Save the final pruned model\n",
    "gradient_pruned_model.save(\"efficientnet_gradient_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52cce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61abb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Variational / Bayesian Pruning\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd65eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Bayesian Layers for Pruning\n",
    "# ---------------------------------------------\n",
    "def prior(kernel_size, bias_size, dtype=None):\n",
    "    \"\"\"\n",
    "    Defines the prior probability distribution for the weights.\n",
    "    Uses a standard Gaussian prior (zero mean, unit variance).\n",
    "    \"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    return lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n), scale_diag=tf.ones(n))\n",
    "\n",
    "def posterior(kernel_size, bias_size, dtype=None):\n",
    "    \"\"\"\n",
    "    Defines the posterior probability distribution for the weights.\n",
    "    It is a learnable Gaussian distribution with trainable mean and std.\n",
    "    \"\"\"\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "        tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "        tfp.layers.DistributionLambda(lambda t: tfd.MultivariateNormalDiag(loc=t, scale_diag=tf.nn.softplus(t)))\n",
    "    ])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Define Bayesian Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_bayesian_efficientnet(input_shape, num_classes, dropout_rate=0.5):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "\n",
    "    # Bayesian Dense Layer\n",
    "    x = tfp.layers.DenseVariational(\n",
    "        units=256,\n",
    "        make_prior_fn=prior,\n",
    "        make_posterior_fn=posterior,\n",
    "        kl_weight=1 / x_train.shape[0],  # KL divergence weight (scaling factor)\n",
    "        activation='relu'\n",
    "    )(x)\n",
    "\n",
    "    x = layers.Dropout(dropout_rate)(x)  # Bayesian Dropout\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Train the Bayesian Model with Pruning\n",
    "# ---------------------------------------------\n",
    "bayesian_model = create_bayesian_efficientnet((224, 224, 3), num_classes, dropout_rate=0.5)\n",
    "\n",
    "bayesian_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Bayesian Pruned EfficientNetB0 on TF Flowers ---\")\n",
    "\n",
    "history_bayesian = bayesian_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Evaluate the Pruned Bayesian Model\n",
    "# ---------------------------------------------\n",
    "bayesian_loss, bayesian_acc = bayesian_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Bayesian Pruned EfficientNetB0 - TF Flowers Accuracy: {bayesian_acc:.4f}\")\n",
    "\n",
    "# Save the final pruned model\n",
    "bayesian_model.save(\"efficientnet_bayesian_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157799d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#Structured Pruning via Group Regularization\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import EfficientNetB0, efficientnet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load TF Flowers Dataset\n",
    "# ---------------------------------------------\n",
    "(dataset_train, dataset_test), info = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "num_classes = info.features['label'].num_classes\n",
    "IMG_SIZE = 128\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Convert Dataset to NumPy Arrays\n",
    "# ---------------------------------------------\n",
    "x_train_list, y_train_list = [], []\n",
    "x_test_list, y_test_list = [], []\n",
    "\n",
    "for img, label in dataset_train:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_train_list.append(img.numpy())\n",
    "    y_train_list.append(label.numpy())\n",
    "\n",
    "for img, label in dataset_test:\n",
    "    img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    x_test_list.append(img.numpy())\n",
    "    y_test_list.append(label.numpy())\n",
    "\n",
    "x_train = np.stack(x_train_list).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(np.array(y_train_list), num_classes)\n",
    "\n",
    "x_test = np.stack(x_test_list).astype(\"float32\") / 255.0\n",
    "y_test = tf.keras.utils.to_categorical(np.array(y_test_list), num_classes)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Data Pipeline for EfficientNetB0\n",
    "# ---------------------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_efficientnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define Structured Pruned EfficientNetB0 Model\n",
    "# ---------------------------------------------\n",
    "def create_structured_pruned_efficientnet(input_shape, num_classes, l1_lambda=1e-4, l2_lambda=1e-4):\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze base model\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "\n",
    "    # Add Group Lasso Regularization (L1 for sparsity, L2 for structure)\n",
    "    x = layers.Dense(\n",
    "        256,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l1_l2(l1=l1_lambda, l2=l2_lambda)  # Group regularization\n",
    "    )(x)\n",
    "\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Train the Structured Pruned Model\n",
    "# ---------------------------------------------\n",
    "structured_pruned_model = create_structured_pruned_efficientnet((224, 224, 3), num_classes, l1_lambda=1e-4, l2_lambda=1e-4)\n",
    "\n",
    "structured_pruned_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Structured Pruned EfficientNetB0 on TF Flowers ---\")\n",
    "\n",
    "history_structured = structured_pruned_model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Evaluate the Structured Pruned Model\n",
    "# ---------------------------------------------\n",
    "structured_loss, structured_acc = structured_pruned_model.evaluate(val_ds, verbose=0)\n",
    "print(f\"Structured Pruned EfficientNetB0 - TF Flowers Accuracy: {structured_acc:.4f}\")\n",
    "\n",
    "# Save the final pruned model\n",
    "structured_pruned_model.save(\"efficientnet_structured_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344c0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8c290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee374bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc85e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3c07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa5a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fdbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
