{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "379e3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Normalize the pixel values between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 100)\n",
    "y_test = keras.utils.to_categorical(y_test, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e329042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (29009, 32, 32, 3)\n",
      "Y_train shape: (29009,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"E:\\imbcifar\\train\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_train = []\n",
    "Y_train = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((32, 32))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_train.append(img_array)\n",
    "            Y_train.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37702c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ce9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bc75184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_16216\\1555111213.py:18: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((32, 32), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n",
      "Cannot open desktop.ini as an image.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_and_process_images(base_dir):\n",
    "    images = []\n",
    "    # Iterate through all subdirectories\n",
    "    for subdir, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            # Construct the full file path\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            try:\n",
    "                # Open the image\n",
    "                with Image.open(filepath) as img:\n",
    "                    # Convert to RGB\n",
    "                    img = img.convert('RGB')\n",
    "                    # Resize to 32x32\n",
    "                    img = img.resize((32, 32), Image.ANTIALIAS)\n",
    "                    # Convert to numpy array and scale\n",
    "                    img_array = np.asarray(img, dtype=np.float32) / 255\n",
    "                    images.append(img_array)\n",
    "            except IOError:\n",
    "                # Handle the case where the file could not be opened as an image\n",
    "                print(f\"Cannot open {file} as an image.\")\n",
    "                \n",
    "    return images\n",
    "\n",
    "base_dir = r\"D:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\training\"\n",
    "X_train = read_and_process_images(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251b8092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (11276, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define the paths to your dataset folders\n",
    "train_dataset_dir = r\"D:\\datasets\\Underwater_Image\\LIMUC (Labeled Images for Ulcerative Colitis)\\train_and_validation_sets\\train_and_validation_sets\"\n",
    "test_dataset_dir = r\"D:\\datasets\\Underwater_Image\\LIMUC (Labeled Images for Ulcerative Colitis)\\test_set\\test_set\"\n",
    "\n",
    "# Initialize empty lists for X_train, Y_train, X_test, and Y_test\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "# Initialize an empty list to store categorical labels\n",
    "categorical_labels = []\n",
    "\n",
    "# Define a function to read and preprocess images\n",
    "def process_images(folder_path, label, is_train_set=True):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".bmp\"):  # Check if it's a file and ends with .bmp\n",
    "            # Open and resize the image to (32, 32, 3)\n",
    "            img = Image.open(file_path)\n",
    "            img = img.resize((32, 32))\n",
    "            img = img.convert(\"RGB\")\n",
    "            \n",
    "            # Convert image data to a NumPy array\n",
    "            img_array = np.array(img).astype('float32')  # Convert to float\n",
    "            \n",
    "            # Normalize the image data (optional)\n",
    "            img_array /= 255.0  # Normalize pixel values to [0, 1]\n",
    "            \n",
    "            # Append the image data to the appropriate list\n",
    "            if is_train_set:\n",
    "                X_train.append(img_array)\n",
    "                Y_train.append(label)  # Append the numerical label\n",
    "            else:\n",
    "                X_test.append(img_array)\n",
    "                Y_test.append(label)  # Append the numerical label\n",
    "            \n",
    "            # Append the label for categorical encoding\n",
    "            categorical_labels.append(label)  # Append the numerical label\n",
    "\n",
    "# List the folders inside the training dataset directory\n",
    "train_folders = os.listdir(train_dataset_dir)\n",
    "\n",
    "# Create a label encoder for categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Loop through the training folders and process images\n",
    "for label, folder_name in enumerate(train_folders):\n",
    "    folder_path = os.path.join(train_dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        process_images(folder_path, label)\n",
    "\n",
    "# List the folders inside the test dataset directory\n",
    "test_folders = os.listdir(test_dataset_dir)\n",
    "\n",
    "# Loop through the test folders and process images\n",
    "for label, folder_name in enumerate(test_folders):\n",
    "    folder_path = os.path.join(test_dataset_dir, folder_name)\n",
    "    if os.path.isdir(folder_path):  # Check if it's a directory\n",
    "        process_images(folder_path, label, is_train_set=False)\n",
    "        \n",
    "        \n",
    "        \n",
    "x_train = np.concatenate((X_train, X_test), axis=0)\n",
    "# Check the shape of X_train, Y_train_categorical, X_test, and Y_test_categorical\n",
    "print(\"Shape of X_train:\", np.shape(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5d638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|███████████████████████████████████████████████▍                                  | 11/19 [03:50<04:22, 32.87s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"G:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\training\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_train = []\n",
    "Y_train = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((32, 32))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_train.append(img_array)\n",
    "            Y_train.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846c5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\shaif\\tensorflow_datasets\\i_naturalist2018\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005416393280029297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Dl Completed...",
       "rate": null,
       "total": 0,
       "unit": " url",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a02de4aa6b4d378638a3dfeb0c142e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0005040168762207031,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Dl Size...",
       "rate": null,
       "total": 0,
       "unit": " MiB",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df1338bd0a49aa96be0c11b2c30f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Extraction completed...",
       "rate": null,
       "total": 0,
       "unit": " file",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8712c6efe4bf400cbd51034dbbc88d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnknownError",
     "evalue": "Failed to WriteFile: C:\\Users\\shaif\\tensorflow_datasets\\downloads\\ml-inat-competi-dataset.s3_2018_test201y0Id8aFmoOuWp0nWu62p_H0DreGZ8ygzUUVFwolgzZ8.tar.gz.tmp.c6ce1278ee294654a198b9a8699be117\\test2018.tar.gz : There is not enough space on the disk.\r\n; operation in progress",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the iNaturalist dataset\u001b[39;00m\n\u001b[0;32m      6\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_naturalist2018\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m (ds_train, ds_test), ds_info \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_supervised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Get the (image, label) pairs\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Function to preprocess images and labels\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(image, label):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:640\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads the named dataset into a `tf.data.Dataset`.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[38;5;124;03m`tfds.load` is a convenience method that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m    Split-specific information is available in `ds_info.splits`.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    634\u001b[0m dbuilder \u001b[38;5;241m=\u001b[39m _fetch_builder(\n\u001b[0;32m    635\u001b[0m     name,\n\u001b[0;32m    636\u001b[0m     data_dir,\n\u001b[0;32m    637\u001b[0m     builder_kwargs,\n\u001b[0;32m    638\u001b[0m     try_gcs,\n\u001b[0;32m    639\u001b[0m )\n\u001b[1;32m--> 640\u001b[0m \u001b[43m_download_and_prepare_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdbuilder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m as_dataset_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m   as_dataset_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:499\u001b[0m, in \u001b[0;36m_download_and_prepare_builder\u001b[1;34m(dbuilder, download, download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m    498\u001b[0m   download_and_prepare_kwargs \u001b[38;5;241m=\u001b[39m download_and_prepare_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 499\u001b[0m   \u001b[43mdbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:646\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    644\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_dir)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 646\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[0;32m    652\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[0;32m    653\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[0;32m    654\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[0;32m    655\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:1498\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1497\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1498\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[0;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[0;32m   1505\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[0;32m   1506\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[0;32m   1507\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[0;32m   1508\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\image_classification\\i_naturalist2018\\i_naturalist2018.py:81\u001b[0m, in \u001b[0;36mINaturalist2018._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m---> 81\u001b[0m   output_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainval_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m          \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_val2018.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m          \u001b[49m\u001b[43mextract_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExtractMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNO_EXTRACT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_annos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain2018.json.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_annos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval2018.json.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mResource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m          \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest2018.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m          \u001b[49m\u001b[43mextract_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExtractMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNO_EXTRACT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategories.json.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples(\n\u001b[0;32m     96\u001b[0m           images_archive\u001b[38;5;241m=\u001b[39mdl_manager\u001b[38;5;241m.\u001b[39miter_archive(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m       ),\n\u001b[0;32m    120\u001b[0m   }\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:687\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m    686\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extractor\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_extract\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:830\u001b[0m, in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m--> 830\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tree\\__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[0;32m    827\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m    828\u001b[0m     map_fn, all_inputs\n\u001b[0;32m    829\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m    830\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m--> 831\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[0;32m    832\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[0;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[1;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\six.py:703\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\promise\\promise.py:844\u001b[0m, in \u001b[0;36m_process_future_result.<locals>.handle_future_result\u001b[1;34m(future)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_future_result\u001b[39m(future):\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;66;03m# type: (Any) -> None\u001b[39;00m\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 844\u001b[0m         resolve(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    846\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\concurrent\\futures\\_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\concurrent\\futures\\_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\concurrent\\futures\\thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow_datasets\\core\\download\\downloader.py:244\u001b[0m, in \u001b[0;36m_Downloader._sync_download\u001b[1;34m(self, url, destination_path, verify)\u001b[0m\n\u001b[0;32m    242\u001b[0m size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[0;32m    243\u001b[0m checksum\u001b[38;5;241m.\u001b[39mupdate(block)\n\u001b[1;32m--> 244\u001b[0m \u001b[43mfile_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Update the download size progress bar\u001b[39;00m\n\u001b[0;32m    247\u001b[0m size_mb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(block)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:100\u001b[0m, in \u001b[0;36mFileIO.write\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prewrite_check()\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_writable_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to WriteFile: C:\\Users\\shaif\\tensorflow_datasets\\downloads\\ml-inat-competi-dataset.s3_2018_test201y0Id8aFmoOuWp0nWu62p_H0DreGZ8ygzUUVFwolgzZ8.tar.gz.tmp.c6ce1278ee294654a198b9a8699be117\\test2018.tar.gz : There is not enough space on the disk.\r\n; operation in progress"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the iNaturalist dataset\n",
    "dataset_name = 'i_naturalist2018'\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    dataset_name,\n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,  # Get the (image, label) pairs\n",
    "    with_info=True\n",
    ")\n",
    "\n",
    "# Function to preprocess images and labels\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (32, 32))\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Normalize to [0, 1]\n",
    "    label = tf.one_hot(label, depth=ds_info.features['label'].num_classes)  # One-hot encode labels\n",
    "    return image, label\n",
    "\n",
    "# Preprocess train and test datasets\n",
    "ds_train = ds_train.map(preprocess).batch(32).shuffle(buffer_size=1000)\n",
    "ds_test = ds_test.map(preprocess).batch(32)\n",
    "\n",
    "# Convert the TensorFlow dataset to numpy arrays for x_train and y_train\n",
    "def dataset_to_numpy(ds):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image_batch, label_batch in ds:\n",
    "        images.append(image_batch.numpy())\n",
    "        labels.append(label_batch.numpy())\n",
    "    return np.concatenate(images), np.concatenate(labels)\n",
    "\n",
    "# Convert train and test datasets\n",
    "x_train, y_train = dataset_to_numpy(ds_train)\n",
    "x_test, y_test = dataset_to_numpy(ds_test)\n",
    "\n",
    "# Verify shapes\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78418789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc94b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e60f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c549f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "import glob\n",
    "\n",
    "target_size = (32, 32)  # Change the values as per your requirement\n",
    "# Load the pre-trained ResNet50 model with modified input shape\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(target_size[0], target_size[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c9ee00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 0</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature 2038</th>\n",
       "      <th>Feature 2039</th>\n",
       "      <th>Feature 2040</th>\n",
       "      <th>Feature 2041</th>\n",
       "      <th>Feature 2042</th>\n",
       "      <th>Feature 2043</th>\n",
       "      <th>Feature 2044</th>\n",
       "      <th>Feature 2045</th>\n",
       "      <th>Feature 2046</th>\n",
       "      <th>Feature 2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.285227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 0  Feature 1  Feature 2  Feature 3  Feature 4  Feature 5  \\\n",
       "0        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   Feature 6  Feature 7  Feature 8  Feature 9  ...  Feature 2038  \\\n",
       "0        0.0        0.0        0.0        0.0  ...           0.0   \n",
       "\n",
       "   Feature 2039  Feature 2040  Feature 2041  Feature 2042  Feature 2043  \\\n",
       "0           0.0      0.009121           0.0           0.0           0.0   \n",
       "\n",
       "   Feature 2044  Feature 2045  Feature 2046  Feature 2047  \n",
       "0           0.0      3.285227           0.0           0.0  \n",
       "\n",
       "[1 rows x 2048 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Define the file path\n",
    "file_path = r\"D:\\feature.csv\"\n",
    "# Read the tab-separated CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, delimiter='\\t')\n",
    "columns_to_drop = [df.columns[0], df.columns[-1]]\n",
    "data = df.drop(columns_to_drop, axis=1)\n",
    "# Display the head of the DataFrame\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06649222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4357, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afbd946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "424d8e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "ft = model.predict(np.array(X_train).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61ee04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 400\n",
    "batch_size = 100\n",
    "max_iter = 100\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter)\n",
    "kmeans.fit(ft)\n",
    "# Retrieve the cluster centers\n",
    "ct = kmeans.cluster_centers_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "400c3f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [01:00<00:00,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1000)\n",
      "(400, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "combo_list = []  # Initialize combo_list\n",
    "tot_dist = []\n",
    "# Iterate over ft\n",
    "for i in tqdm(range(len(ct))):\n",
    "    distances = []\n",
    "\n",
    "    # Calculate distances for each row in data\n",
    "    for index, row in data.iterrows():\n",
    "        row_array = row.to_numpy()  # Convert row to numpy array\n",
    "        distance = np.linalg.norm(ct[i] - row_array)  # Calculate Euclidean distance\n",
    "        distances.append(distance)\n",
    "\n",
    "    tot_dist.append(distances)\n",
    "print(np.shape(tot_dist))\n",
    "#new_shape = (1000, 30)\n",
    "#re_dist = np.transpose(tot_dist, (1, 0))\n",
    "re_dist = tot_dist\n",
    "print(np.shape(re_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ba3d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo list saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the list to a file in the D: drive\n",
    "file_path = r\"D:\\dist_list.pickle\"  # r prefix is used for raw string to avoid escape characters\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(re_dist, file)\n",
    "\n",
    "print(\"Combo list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1af5e5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mindex_list\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_list' is not defined"
     ]
    }
   ],
   "source": [
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43511453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee431086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo list loaded successfully.\n",
      "[1, 2, 3, 4, 5, 6, 11, 14, 18, 20, 21, 22, 23, 24, 27, 29, 31, 33, 34, 41, 42, 45, 47, 49, 50, 58, 65, 66, 70, 72, 73, 74, 75, 76, 78, 81, 83, 85, 86, 93, 96, 98, 102, 103, 105, 107, 108, 109, 115, 116, 117, 123, 127, 128, 129, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143, 147, 149, 152, 153, 156, 157, 165, 166, 167, 179, 181, 183, 188, 190, 195, 197, 204, 205, 206, 214, 215, 216, 217, 221, 223, 224, 229, 232, 234, 238, 239, 240, 241, 244, 247, 252, 256, 257, 259, 272, 274, 276, 277, 278, 280, 288, 291, 293, 297, 299, 302, 305, 306, 317, 318, 320, 321, 322, 324, 325, 326, 330, 331, 332, 342, 343, 344, 346, 351, 352, 353, 362, 363, 368, 369, 373, 375, 378, 379, 384, 386, 388, 393, 397, 399, 400, 403, 404, 405, 406, 408, 409, 411, 413, 414, 417, 424, 426, 432, 435, 436, 438, 442, 447, 449, 451, 453, 457, 458, 460, 465, 466, 468, 470, 472, 474, 475, 479, 484, 493, 495, 498, 500, 509, 510, 511, 512, 518, 520, 525, 527, 528, 531, 533, 535, 536, 538, 539, 540, 541, 542, 547, 550, 551, 553, 554, 555, 556, 562, 563, 564, 565, 567, 568, 570, 575, 578, 579, 581, 584, 585, 586, 590, 592, 593, 595, 597, 598, 600, 605, 607, 610, 614, 624, 625, 627, 628, 630, 632, 633, 635, 636, 639, 640, 643, 646, 648, 649, 650, 654, 657, 660, 664, 666, 667, 668, 669, 671, 675, 676, 678, 686, 688, 689, 690, 694, 695, 698, 701, 705, 707, 710, 717, 718, 724, 726, 727, 728, 730, 732, 734, 741, 743, 746, 747, 750, 751, 754, 757, 759, 762, 764, 768, 771, 773, 779, 780, 781, 782, 785, 789, 791, 794, 796, 798, 799, 800, 802, 803, 808, 809, 812, 814, 815, 817, 818, 820, 821, 823, 824, 825, 826, 827, 829, 831, 834, 836, 837, 841, 847, 851, 852, 853, 854, 856, 858, 863, 864, 866, 867, 869, 871, 874, 877, 884, 886, 891, 893, 894, 895, 902, 903, 904, 905, 906, 908, 912, 913, 914, 915, 916, 922, 923, 925, 928, 933, 935, 941, 946, 958, 959, 960, 961, 964, 965, 967, 969, 970, 972, 973, 974, 975, 976, 977, 978, 979, 980, 982, 983, 984, 985, 986, 992, 995]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Loading the list from the pickle file\n",
    "file_path = r\"D:\\index_list.pickle\"  # Update the file path if necessary\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    index_list = pickle.load(file)\n",
    "\n",
    "print(\"Combo list loaded successfully.\")\n",
    "print(index_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba86a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Directory', 'Feature 0', 'Feature 1', 'Feature 2', 'Feature 3',\n",
       "       'Feature 4', 'Feature 5', 'Feature 6', 'Feature 7', 'Feature 8',\n",
       "       ...\n",
       "       'Feature 2039', 'Feature 2040', 'Feature 2041', 'Feature 2042',\n",
       "       'Feature 2043', 'Feature 2044', 'Feature 2045', 'Feature 2046',\n",
       "       'Feature 2047', 'Unnamed: 2049'],\n",
       "      dtype='object', length=2050)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ede097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = df.loc[index_list, ['Name']].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89170a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3dc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f11d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo list saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the list to a file in the D: drive\n",
    "file_path = r\"D:\\combo_list.pickle\"  # r prefix is used for raw string to avoid escape characters\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(selected_data, file)\n",
    "\n",
    "print(\"Combo list saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebce419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combo list loaded successfully.\n",
      "['D:/data/imagenet\\\\n01443537\\\\' 'D:/data/imagenet\\\\n01484850\\\\'\n",
      " 'D:/data/imagenet\\\\n01491361\\\\' 'D:/data/imagenet\\\\n01494475\\\\'\n",
      " 'D:/data/imagenet\\\\n01496331\\\\' 'D:/data/imagenet\\\\n01498041\\\\'\n",
      " 'D:/data/imagenet\\\\n01531178\\\\' 'D:/data/imagenet\\\\n01537544\\\\'\n",
      " 'D:/data/imagenet\\\\n01582220\\\\' 'D:/data/imagenet\\\\n01601694\\\\'\n",
      " 'D:/data/imagenet\\\\n01608432\\\\' 'D:/data/imagenet\\\\n01614925\\\\'\n",
      " 'D:/data/imagenet\\\\n01616318\\\\' 'D:/data/imagenet\\\\n01622779\\\\'\n",
      " 'D:/data/imagenet\\\\n01631663\\\\' 'D:/data/imagenet\\\\n01632777\\\\'\n",
      " 'D:/data/imagenet\\\\n01644373\\\\' 'D:/data/imagenet\\\\n01664065\\\\'\n",
      " 'D:/data/imagenet\\\\n01665541\\\\' 'D:/data/imagenet\\\\n01685808\\\\'\n",
      " 'D:/data/imagenet\\\\n01687978\\\\' 'D:/data/imagenet\\\\n01692333\\\\'\n",
      " 'D:/data/imagenet\\\\n01694178\\\\' 'D:/data/imagenet\\\\n01697457\\\\'\n",
      " 'D:/data/imagenet\\\\n01698640\\\\' 'D:/data/imagenet\\\\n01737021\\\\'\n",
      " 'D:/data/imagenet\\\\n01751748\\\\' 'D:/data/imagenet\\\\n01753488\\\\'\n",
      " 'D:/data/imagenet\\\\n01770081\\\\' 'D:/data/imagenet\\\\n01773157\\\\'\n",
      " 'D:/data/imagenet\\\\n01773549\\\\' 'D:/data/imagenet\\\\n01773797\\\\'\n",
      " 'D:/data/imagenet\\\\n01774384\\\\' 'D:/data/imagenet\\\\n01774750\\\\'\n",
      " 'D:/data/imagenet\\\\n01776313\\\\' 'D:/data/imagenet\\\\n01796340\\\\'\n",
      " 'D:/data/imagenet\\\\n01798484\\\\' 'D:/data/imagenet\\\\n01806567\\\\'\n",
      " 'D:/data/imagenet\\\\n01807496\\\\' 'D:/data/imagenet\\\\n01829413\\\\'\n",
      " 'D:/data/imagenet\\\\n01843383\\\\' 'D:/data/imagenet\\\\n01855032\\\\'\n",
      " 'D:/data/imagenet\\\\n01872401\\\\' 'D:/data/imagenet\\\\n01873310\\\\'\n",
      " 'D:/data/imagenet\\\\n01882714\\\\' 'D:/data/imagenet\\\\n01910747\\\\'\n",
      " 'D:/data/imagenet\\\\n01914609\\\\' 'D:/data/imagenet\\\\n01917289\\\\'\n",
      " 'D:/data/imagenet\\\\n01950731\\\\' 'D:/data/imagenet\\\\n01955084\\\\'\n",
      " 'D:/data/imagenet\\\\n01968897\\\\' 'D:/data/imagenet\\\\n01984695\\\\'\n",
      " 'D:/data/imagenet\\\\n02002556\\\\' 'D:/data/imagenet\\\\n02002724\\\\'\n",
      " 'D:/data/imagenet\\\\n02006656\\\\' 'D:/data/imagenet\\\\n02009229\\\\'\n",
      " 'D:/data/imagenet\\\\n02009912\\\\' 'D:/data/imagenet\\\\n02011460\\\\'\n",
      " 'D:/data/imagenet\\\\n02017213\\\\' 'D:/data/imagenet\\\\n02018207\\\\'\n",
      " 'D:/data/imagenet\\\\n02018795\\\\' 'D:/data/imagenet\\\\n02025239\\\\'\n",
      " 'D:/data/imagenet\\\\n02027492\\\\' 'D:/data/imagenet\\\\n02028035\\\\'\n",
      " 'D:/data/imagenet\\\\n02033041\\\\' 'D:/data/imagenet\\\\n02037110\\\\'\n",
      " 'D:/data/imagenet\\\\n02066245\\\\' 'D:/data/imagenet\\\\n02074367\\\\'\n",
      " 'D:/data/imagenet\\\\n02085782\\\\' 'D:/data/imagenet\\\\n02085936\\\\'\n",
      " 'D:/data/imagenet\\\\n02086646\\\\' 'D:/data/imagenet\\\\n02086910\\\\'\n",
      " 'D:/data/imagenet\\\\n02089078\\\\' 'D:/data/imagenet\\\\n02089867\\\\'\n",
      " 'D:/data/imagenet\\\\n02089973\\\\' 'D:/data/imagenet\\\\n02093256\\\\'\n",
      " 'D:/data/imagenet\\\\n02093647\\\\' 'D:/data/imagenet\\\\n02093859\\\\'\n",
      " 'D:/data/imagenet\\\\n02095314\\\\' 'D:/data/imagenet\\\\n02095889\\\\'\n",
      " 'D:/data/imagenet\\\\n02096585\\\\' 'D:/data/imagenet\\\\n02097130\\\\'\n",
      " 'D:/data/imagenet\\\\n02098413\\\\' 'D:/data/imagenet\\\\n02099267\\\\'\n",
      " 'D:/data/imagenet\\\\n02099429\\\\' 'D:/data/imagenet\\\\n02101006\\\\'\n",
      " 'D:/data/imagenet\\\\n02101388\\\\' 'D:/data/imagenet\\\\n02101556\\\\'\n",
      " 'D:/data/imagenet\\\\n02102040\\\\' 'D:/data/imagenet\\\\n02102973\\\\'\n",
      " 'D:/data/imagenet\\\\n02104365\\\\' 'D:/data/imagenet\\\\n02105056\\\\'\n",
      " 'D:/data/imagenet\\\\n02105641\\\\' 'D:/data/imagenet\\\\n02106166\\\\'\n",
      " 'D:/data/imagenet\\\\n02106550\\\\' 'D:/data/imagenet\\\\n02107574\\\\'\n",
      " 'D:/data/imagenet\\\\n02107683\\\\' 'D:/data/imagenet\\\\n02107908\\\\'\n",
      " 'D:/data/imagenet\\\\n02108000\\\\' 'D:/data/imagenet\\\\n02108551\\\\'\n",
      " 'D:/data/imagenet\\\\n02109525\\\\' 'D:/data/imagenet\\\\n02110627\\\\'\n",
      " 'D:/data/imagenet\\\\n02111277\\\\' 'D:/data/imagenet\\\\n02111500\\\\'\n",
      " 'D:/data/imagenet\\\\n02112018\\\\' 'D:/data/imagenet\\\\n02114855\\\\'\n",
      " 'D:/data/imagenet\\\\n02115913\\\\' 'D:/data/imagenet\\\\n02117135\\\\'\n",
      " 'D:/data/imagenet\\\\n02119022\\\\' 'D:/data/imagenet\\\\n02119789\\\\'\n",
      " 'D:/data/imagenet\\\\n02120505\\\\' 'D:/data/imagenet\\\\n02128385\\\\'\n",
      " 'D:/data/imagenet\\\\n02129165\\\\' 'D:/data/imagenet\\\\n02130308\\\\'\n",
      " 'D:/data/imagenet\\\\n02134418\\\\' 'D:/data/imagenet\\\\n02138441\\\\'\n",
      " 'D:/data/imagenet\\\\n02167151\\\\' 'D:/data/imagenet\\\\n02172182\\\\'\n",
      " 'D:/data/imagenet\\\\n02174001\\\\' 'D:/data/imagenet\\\\n02259212\\\\'\n",
      " 'D:/data/imagenet\\\\n02264363\\\\' 'D:/data/imagenet\\\\n02268853\\\\'\n",
      " 'D:/data/imagenet\\\\n02276258\\\\' 'D:/data/imagenet\\\\n02277742\\\\'\n",
      " 'D:/data/imagenet\\\\n02280649\\\\' 'D:/data/imagenet\\\\n02281406\\\\'\n",
      " 'D:/data/imagenet\\\\n02281787\\\\' 'D:/data/imagenet\\\\n02325366\\\\'\n",
      " 'D:/data/imagenet\\\\n02326432\\\\' 'D:/data/imagenet\\\\n02328150\\\\'\n",
      " 'D:/data/imagenet\\\\n02396427\\\\' 'D:/data/imagenet\\\\n02397096\\\\'\n",
      " 'D:/data/imagenet\\\\n02398521\\\\' 'D:/data/imagenet\\\\n02408429\\\\'\n",
      " 'D:/data/imagenet\\\\n02422106\\\\' 'D:/data/imagenet\\\\n02422699\\\\'\n",
      " 'D:/data/imagenet\\\\n02423022\\\\' 'D:/data/imagenet\\\\n02447366\\\\'\n",
      " 'D:/data/imagenet\\\\n02454379\\\\' 'D:/data/imagenet\\\\n02483362\\\\'\n",
      " 'D:/data/imagenet\\\\n02483708\\\\' 'D:/data/imagenet\\\\n02487347\\\\'\n",
      " 'D:/data/imagenet\\\\n02488702\\\\' 'D:/data/imagenet\\\\n02492035\\\\'\n",
      " 'D:/data/imagenet\\\\n02492660\\\\' 'D:/data/imagenet\\\\n02500267\\\\'\n",
      " 'D:/data/imagenet\\\\n02504458\\\\' 'D:/data/imagenet\\\\n02510455\\\\'\n",
      " 'D:/data/imagenet\\\\n02607072\\\\' 'D:/data/imagenet\\\\n02655020\\\\'\n",
      " 'D:/data/imagenet\\\\n02667093\\\\' 'D:/data/imagenet\\\\n02669723\\\\'\n",
      " 'D:/data/imagenet\\\\n02687172\\\\' 'D:/data/imagenet\\\\n02690373\\\\'\n",
      " 'D:/data/imagenet\\\\n02692877\\\\' 'D:/data/imagenet\\\\n02699494\\\\'\n",
      " 'D:/data/imagenet\\\\n02704792\\\\' 'D:/data/imagenet\\\\n02708093\\\\'\n",
      " 'D:/data/imagenet\\\\n02730930\\\\' 'D:/data/imagenet\\\\n02749479\\\\'\n",
      " 'D:/data/imagenet\\\\n02769748\\\\' 'D:/data/imagenet\\\\n02782093\\\\'\n",
      " 'D:/data/imagenet\\\\n02791270\\\\' 'D:/data/imagenet\\\\n02794156\\\\'\n",
      " 'D:/data/imagenet\\\\n02804610\\\\' 'D:/data/imagenet\\\\n02808440\\\\'\n",
      " 'D:/data/imagenet\\\\n02814533\\\\' 'D:/data/imagenet\\\\n02815834\\\\'\n",
      " 'D:/data/imagenet\\\\n02825657\\\\' 'D:/data/imagenet\\\\n02841315\\\\'\n",
      " 'D:/data/imagenet\\\\n02859443\\\\' 'D:/data/imagenet\\\\n02865351\\\\'\n",
      " 'D:/data/imagenet\\\\n02870880\\\\' 'D:/data/imagenet\\\\n02883205\\\\'\n",
      " 'D:/data/imagenet\\\\n02892201\\\\' 'D:/data/imagenet\\\\n02894605\\\\'\n",
      " 'D:/data/imagenet\\\\n02916936\\\\' 'D:/data/imagenet\\\\n02917067\\\\'\n",
      " 'D:/data/imagenet\\\\n02930766\\\\' 'D:/data/imagenet\\\\n02948072\\\\'\n",
      " 'D:/data/imagenet\\\\n02951358\\\\' 'D:/data/imagenet\\\\n02963159\\\\'\n",
      " 'D:/data/imagenet\\\\n02965783\\\\' 'D:/data/imagenet\\\\n02974003\\\\'\n",
      " 'D:/data/imagenet\\\\n02981792\\\\' 'D:/data/imagenet\\\\n03016953\\\\'\n",
      " 'D:/data/imagenet\\\\n03018349\\\\' 'D:/data/imagenet\\\\n03032252\\\\'\n",
      " 'D:/data/imagenet\\\\n03042490\\\\' 'D:/data/imagenet\\\\n03089624\\\\'\n",
      " 'D:/data/imagenet\\\\n03095699\\\\' 'D:/data/imagenet\\\\n03100240\\\\'\n",
      " 'D:/data/imagenet\\\\n03109150\\\\' 'D:/data/imagenet\\\\n03127747\\\\'\n",
      " 'D:/data/imagenet\\\\n03131574\\\\' 'D:/data/imagenet\\\\n03160309\\\\'\n",
      " 'D:/data/imagenet\\\\n03180011\\\\' 'D:/data/imagenet\\\\n03187595\\\\'\n",
      " 'D:/data/imagenet\\\\n03197337\\\\' 'D:/data/imagenet\\\\n03207743\\\\'\n",
      " 'D:/data/imagenet\\\\n03208938\\\\' 'D:/data/imagenet\\\\n03216828\\\\'\n",
      " 'D:/data/imagenet\\\\n03220513\\\\' 'D:/data/imagenet\\\\n03223299\\\\'\n",
      " 'D:/data/imagenet\\\\n03240683\\\\' 'D:/data/imagenet\\\\n03249569\\\\'\n",
      " 'D:/data/imagenet\\\\n03250847\\\\' 'D:/data/imagenet\\\\n03272562\\\\'\n",
      " 'D:/data/imagenet\\\\n03297495\\\\' 'D:/data/imagenet\\\\n03314780\\\\'\n",
      " 'D:/data/imagenet\\\\n03337140\\\\' 'D:/data/imagenet\\\\n03344393\\\\'\n",
      " 'D:/data/imagenet\\\\n03345487\\\\' 'D:/data/imagenet\\\\n03347037\\\\'\n",
      " 'D:/data/imagenet\\\\n03388043\\\\' 'D:/data/imagenet\\\\n03388183\\\\'\n",
      " 'D:/data/imagenet\\\\n03388549\\\\' 'D:/data/imagenet\\\\n03393912\\\\'\n",
      " 'D:/data/imagenet\\\\n03400231\\\\' 'D:/data/imagenet\\\\n03404251\\\\'\n",
      " 'D:/data/imagenet\\\\n03424325\\\\' 'D:/data/imagenet\\\\n03445924\\\\'\n",
      " 'D:/data/imagenet\\\\n03450230\\\\' 'D:/data/imagenet\\\\n03452741\\\\'\n",
      " 'D:/data/imagenet\\\\n03459775\\\\' 'D:/data/imagenet\\\\n03476684\\\\'\n",
      " 'D:/data/imagenet\\\\n03476991\\\\' 'D:/data/imagenet\\\\n03478589\\\\'\n",
      " 'D:/data/imagenet\\\\n03485407\\\\' 'D:/data/imagenet\\\\n03492542\\\\'\n",
      " 'D:/data/imagenet\\\\n03494278\\\\' 'D:/data/imagenet\\\\n03496892\\\\'\n",
      " 'D:/data/imagenet\\\\n03527444\\\\' 'D:/data/imagenet\\\\n03529860\\\\'\n",
      " 'D:/data/imagenet\\\\n03532672\\\\' 'D:/data/imagenet\\\\n03584254\\\\'\n",
      " 'D:/data/imagenet\\\\n03590841\\\\' 'D:/data/imagenet\\\\n03595614\\\\'\n",
      " 'D:/data/imagenet\\\\n03617480\\\\' 'D:/data/imagenet\\\\n03661043\\\\'\n",
      " 'D:/data/imagenet\\\\n03662601\\\\' 'D:/data/imagenet\\\\n03670208\\\\'\n",
      " 'D:/data/imagenet\\\\n03673027\\\\' 'D:/data/imagenet\\\\n03680355\\\\'\n",
      " 'D:/data/imagenet\\\\n03691459\\\\' 'D:/data/imagenet\\\\n03692522\\\\'\n",
      " 'D:/data/imagenet\\\\n03706229\\\\' 'D:/data/imagenet\\\\n03709823\\\\'\n",
      " 'D:/data/imagenet\\\\n03710721\\\\' 'D:/data/imagenet\\\\n03717622\\\\'\n",
      " 'D:/data/imagenet\\\\n03724870\\\\' 'D:/data/imagenet\\\\n03733281\\\\'\n",
      " 'D:/data/imagenet\\\\n03742115\\\\' 'D:/data/imagenet\\\\n03743016\\\\'\n",
      " 'D:/data/imagenet\\\\n03759954\\\\' 'D:/data/imagenet\\\\n03769881\\\\'\n",
      " 'D:/data/imagenet\\\\n03773504\\\\' 'D:/data/imagenet\\\\n03776460\\\\'\n",
      " 'D:/data/imagenet\\\\n03782006\\\\' 'D:/data/imagenet\\\\n03786901\\\\'\n",
      " 'D:/data/imagenet\\\\n03787032\\\\' 'D:/data/imagenet\\\\n03788195\\\\'\n",
      " 'D:/data/imagenet\\\\n03788365\\\\' 'D:/data/imagenet\\\\n03792782\\\\'\n",
      " 'D:/data/imagenet\\\\n03796401\\\\' 'D:/data/imagenet\\\\n03803284\\\\'\n",
      " 'D:/data/imagenet\\\\n03814639\\\\' 'D:/data/imagenet\\\\n03843555\\\\'\n",
      " 'D:/data/imagenet\\\\n03857828\\\\' 'D:/data/imagenet\\\\n03866082\\\\'\n",
      " 'D:/data/imagenet\\\\n03868242\\\\' 'D:/data/imagenet\\\\n03874293\\\\'\n",
      " 'D:/data/imagenet\\\\n03874599\\\\' 'D:/data/imagenet\\\\n03877845\\\\'\n",
      " 'D:/data/imagenet\\\\n03888257\\\\' 'D:/data/imagenet\\\\n03895866\\\\'\n",
      " 'D:/data/imagenet\\\\n03902125\\\\' 'D:/data/imagenet\\\\n03908714\\\\'\n",
      " 'D:/data/imagenet\\\\n03930630\\\\' 'D:/data/imagenet\\\\n03933933\\\\'\n",
      " 'D:/data/imagenet\\\\n03947888\\\\' 'D:/data/imagenet\\\\n03954731\\\\'\n",
      " 'D:/data/imagenet\\\\n03956157\\\\' 'D:/data/imagenet\\\\n03958227\\\\'\n",
      " 'D:/data/imagenet\\\\n03967562\\\\' 'D:/data/imagenet\\\\n03976467\\\\'\n",
      " 'D:/data/imagenet\\\\n03977966\\\\' 'D:/data/imagenet\\\\n03998194\\\\'\n",
      " 'D:/data/imagenet\\\\n04005630\\\\' 'D:/data/imagenet\\\\n04019541\\\\'\n",
      " 'D:/data/imagenet\\\\n04023962\\\\' 'D:/data/imagenet\\\\n04033995\\\\'\n",
      " 'D:/data/imagenet\\\\n04037443\\\\' 'D:/data/imagenet\\\\n04041544\\\\'\n",
      " 'D:/data/imagenet\\\\n04065272\\\\' 'D:/data/imagenet\\\\n04069434\\\\'\n",
      " 'D:/data/imagenet\\\\n04081281\\\\' 'D:/data/imagenet\\\\n04090263\\\\'\n",
      " 'D:/data/imagenet\\\\n04118538\\\\' 'D:/data/imagenet\\\\n04125021\\\\'\n",
      " 'D:/data/imagenet\\\\n04131690\\\\' 'D:/data/imagenet\\\\n04146614\\\\'\n",
      " 'D:/data/imagenet\\\\n04147183\\\\' 'D:/data/imagenet\\\\n04149813\\\\'\n",
      " 'D:/data/imagenet\\\\n04152593\\\\' 'D:/data/imagenet\\\\n04162706\\\\'\n",
      " 'D:/data/imagenet\\\\n04201297\\\\' 'D:/data/imagenet\\\\n04204347\\\\'\n",
      " 'D:/data/imagenet\\\\n04209239\\\\' 'D:/data/imagenet\\\\n04229816\\\\'\n",
      " 'D:/data/imagenet\\\\n04238763\\\\' 'D:/data/imagenet\\\\n04239074\\\\'\n",
      " 'D:/data/imagenet\\\\n04243546\\\\' 'D:/data/imagenet\\\\n04252077\\\\'\n",
      " 'D:/data/imagenet\\\\n04252225\\\\' 'D:/data/imagenet\\\\n04259630\\\\'\n",
      " 'D:/data/imagenet\\\\n04263257\\\\' 'D:/data/imagenet\\\\n04266014\\\\'\n",
      " 'D:/data/imagenet\\\\n04273569\\\\' 'D:/data/imagenet\\\\n04275548\\\\'\n",
      " 'D:/data/imagenet\\\\n04285008\\\\' 'D:/data/imagenet\\\\n04286575\\\\'\n",
      " 'D:/data/imagenet\\\\n04310018\\\\' 'D:/data/imagenet\\\\n04311004\\\\'\n",
      " 'D:/data/imagenet\\\\n04317175\\\\' 'D:/data/imagenet\\\\n04325704\\\\'\n",
      " 'D:/data/imagenet\\\\n04326547\\\\' 'D:/data/imagenet\\\\n04328186\\\\'\n",
      " 'D:/data/imagenet\\\\n04330267\\\\' 'D:/data/imagenet\\\\n04335435\\\\'\n",
      " 'D:/data/imagenet\\\\n04344873\\\\' 'D:/data/imagenet\\\\n04350905\\\\'\n",
      " 'D:/data/imagenet\\\\n04355933\\\\' 'D:/data/imagenet\\\\n04356056\\\\'\n",
      " 'D:/data/imagenet\\\\n04370456\\\\' 'D:/data/imagenet\\\\n04389033\\\\'\n",
      " 'D:/data/imagenet\\\\n04404412\\\\' 'D:/data/imagenet\\\\n04409515\\\\'\n",
      " 'D:/data/imagenet\\\\n04417672\\\\' 'D:/data/imagenet\\\\n04418357\\\\'\n",
      " 'D:/data/imagenet\\\\n04428191\\\\' 'D:/data/imagenet\\\\n04435653\\\\'\n",
      " 'D:/data/imagenet\\\\n04458633\\\\' 'D:/data/imagenet\\\\n04461696\\\\'\n",
      " 'D:/data/imagenet\\\\n04465501\\\\' 'D:/data/imagenet\\\\n04467665\\\\'\n",
      " 'D:/data/imagenet\\\\n04479046\\\\' 'D:/data/imagenet\\\\n04483307\\\\'\n",
      " 'D:/data/imagenet\\\\n04487081\\\\' 'D:/data/imagenet\\\\n04501370\\\\'\n",
      " 'D:/data/imagenet\\\\n04523525\\\\' 'D:/data/imagenet\\\\n04525305\\\\'\n",
      " 'D:/data/imagenet\\\\n04542943\\\\' 'D:/data/imagenet\\\\n04548362\\\\'\n",
      " 'D:/data/imagenet\\\\n04550184\\\\' 'D:/data/imagenet\\\\n04552348\\\\'\n",
      " 'D:/data/imagenet\\\\n04579432\\\\' 'D:/data/imagenet\\\\n04584207\\\\'\n",
      " 'D:/data/imagenet\\\\n04589890\\\\' 'D:/data/imagenet\\\\n04590129\\\\'\n",
      " 'D:/data/imagenet\\\\n04591157\\\\' 'D:/data/imagenet\\\\n04592741\\\\'\n",
      " 'D:/data/imagenet\\\\n04604644\\\\' 'D:/data/imagenet\\\\n04606251\\\\'\n",
      " 'D:/data/imagenet\\\\n04612504\\\\' 'D:/data/imagenet\\\\n04613696\\\\'\n",
      " 'D:/data/imagenet\\\\n06359193\\\\' 'D:/data/imagenet\\\\n07565083\\\\'\n",
      " 'D:/data/imagenet\\\\n07579787\\\\' 'D:/data/imagenet\\\\n07584110\\\\'\n",
      " 'D:/data/imagenet\\\\n07614500\\\\' 'D:/data/imagenet\\\\n07697313\\\\'\n",
      " 'D:/data/imagenet\\\\n07711569\\\\' 'D:/data/imagenet\\\\n07717410\\\\'\n",
      " 'D:/data/imagenet\\\\n07730033\\\\' 'D:/data/imagenet\\\\n07802026\\\\'\n",
      " 'D:/data/imagenet\\\\n07831146\\\\' 'D:/data/imagenet\\\\n07836838\\\\'\n",
      " 'D:/data/imagenet\\\\n07860988\\\\' 'D:/data/imagenet\\\\n07875152\\\\'\n",
      " 'D:/data/imagenet\\\\n07880968\\\\' 'D:/data/imagenet\\\\n07920052\\\\'\n",
      " 'D:/data/imagenet\\\\n07932039\\\\' 'D:/data/imagenet\\\\n09193705\\\\'\n",
      " 'D:/data/imagenet\\\\n09246464\\\\' 'D:/data/imagenet\\\\n09256479\\\\'\n",
      " 'D:/data/imagenet\\\\n09288635\\\\' 'D:/data/imagenet\\\\n09332890\\\\'\n",
      " 'D:/data/imagenet\\\\n09399592\\\\' 'D:/data/imagenet\\\\n09421951\\\\'\n",
      " 'D:/data/imagenet\\\\n09428293\\\\' 'D:/data/imagenet\\\\n09468604\\\\'\n",
      " 'D:/data/imagenet\\\\n09472597\\\\' 'D:/data/imagenet\\\\n10148035\\\\'\n",
      " 'D:/data/imagenet\\\\n10565667\\\\' 'D:/data/imagenet\\\\n11879895\\\\'\n",
      " 'D:/data/imagenet\\\\n11939491\\\\' 'D:/data/imagenet\\\\n12057211\\\\'\n",
      " 'D:/data/imagenet\\\\n12998815\\\\' 'D:/data/imagenet\\\\n13044778\\\\']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Loading the list from the pickle file\n",
    "file_path = r\"D:\\combo_list.pickle\"  # Update the file path if necessary\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_combo_list = pickle.load(file)\n",
    "\n",
    "print(\"Combo list loaded successfully.\")\n",
    "print(loaded_combo_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12fe31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9b7ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bd347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fba0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ec790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87186a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc427dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a762d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d5acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571ec63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96fd7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: F:\\D-Video\\Python\n",
      "File is readable: F:\\D-Video\\Python\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = r\"F:\\D-Video\\Python\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"File exists: {file_path}\")\n",
    "else:\n",
    "    print(f\"File does not exist: {file_path}\")\n",
    "\n",
    "# Check if the file is readable\n",
    "if os.access(file_path, os.R_OK):\n",
    "    print(f\"File is readable: {file_path}\")\n",
    "else:\n",
    "    print(f\"File is not readable: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25eec36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0f1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e5a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
