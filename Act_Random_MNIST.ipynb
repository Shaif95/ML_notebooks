{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0626244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4569781f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d419458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_labels = to_categorical(train_labels)\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bb0f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a046e5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99ab00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecede22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_unlab, y_train, y_unlab = train_test_split( train_images, train_labels , test_size=0.6, random_state=42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb15a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split( x_train,y_train , test_size=0.5, random_state=40 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "184bc8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f8c7370",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.concatenate((X_train, x_unlab))\n",
    "X_all = arr\n",
    "arr = np.concatenate((Y_train, y_unlab))\n",
    "Y_all = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fd9e2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89812f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae66eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for merging new history objects with older ones\n",
    "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
    "    losses = losses + history.history[\"loss\"]\n",
    "    val_losses = val_losses + history.history[\"val_loss\"]\n",
    "    accuracy = accuracy + history.history[\"accuracy\"]\n",
    "    val_accuracy = val_accuracy + history.history[\"val_accuracy\"]\n",
    "    return losses, val_losses, accuracy, val_accuracy\n",
    "\n",
    "\n",
    "# Plotter function\n",
    "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
    "    plt.plot(losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.legend([\"train_loss\", \"val_loss\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(accuracies)\n",
    "    plt.plot(val_accuracies)\n",
    "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d97963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_config(self):\n",
    "\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'num_patches': self.num_patches,\n",
    "            'projection': self.projection,\n",
    "            'position_embedding': self.position_embedding\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bde3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(4, (3, 3), strides=(2,2),activation='relu')),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.LSTM(projection_dim,return_sequences=True,dropout=0.1),\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b54d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(8, (3, 3), input_shape=(28, 28, 1)))\n",
    "    model.add(layers.Conv2D(8, (3, 3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(16, (3, 3),))\n",
    "    model.add(layers.Conv2D(16, (3, 3),))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(32))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b829e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from keras import models\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    \n",
    "    model = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=(32,32,3),\n",
    "    alpha=1.0,\n",
    "    minimalistic=True,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    classes=10,\n",
    "    pooling=None,\n",
    "    dropout_rate=0.2,\n",
    "    classifier_activation=\"softmax\",\n",
    "    include_preprocessing=True,\n",
    ")\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_13 (Conv2D)          (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 24, 24, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 12, 12, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 10, 10, 16)        1168      \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 8, 8, 16)          2320      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 4, 4, 16)          0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                8224      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,706\n",
      "Trainable params: 12,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.4494 - accuracy: 0.8599\n",
      "Epoch 1: val_loss improved from inf to 0.14329, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 18s 29ms/step - loss: 0.4489 - accuracy: 0.8599 - val_loss: 0.1433 - val_accuracy: 0.9577\n",
      "Epoch 2/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9489\n",
      "Epoch 2: val_loss improved from 0.14329 to 0.08958, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 17s 28ms/step - loss: 0.1671 - accuracy: 0.9489 - val_loss: 0.0896 - val_accuracy: 0.9721\n",
      "Epoch 3/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9609\n",
      "Epoch 3: val_loss improved from 0.08958 to 0.07199, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 18s 31ms/step - loss: 0.1298 - accuracy: 0.9609 - val_loss: 0.0720 - val_accuracy: 0.9771\n",
      "Epoch 4/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9659\n",
      "Epoch 4: val_loss improved from 0.07199 to 0.06808, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 17s 28ms/step - loss: 0.1111 - accuracy: 0.9659 - val_loss: 0.0681 - val_accuracy: 0.9782\n",
      "Epoch 5/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9701\n",
      "Epoch 5: val_loss improved from 0.06808 to 0.06142, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 17s 28ms/step - loss: 0.0977 - accuracy: 0.9701 - val_loss: 0.0614 - val_accuracy: 0.9811\n",
      "Epoch 6/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9743\n",
      "Epoch 6: val_loss improved from 0.06142 to 0.06111, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.0869 - accuracy: 0.9743 - val_loss: 0.0611 - val_accuracy: 0.9816\n",
      "Epoch 7/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9749\n",
      "Epoch 7: val_loss improved from 0.06111 to 0.05546, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.0799 - accuracy: 0.9749 - val_loss: 0.0555 - val_accuracy: 0.9820\n",
      "Epoch 8/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0748 - accuracy: 0.9779\n",
      "Epoch 8: val_loss did not improve from 0.05546\n",
      "600/600 [==============================] - 17s 29ms/step - loss: 0.0748 - accuracy: 0.9779 - val_loss: 0.0568 - val_accuracy: 0.9823\n",
      "Epoch 9/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9782\n",
      "Epoch 9: val_loss improved from 0.05546 to 0.04902, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.0733 - accuracy: 0.9782 - val_loss: 0.0490 - val_accuracy: 0.9853\n",
      "Epoch 10/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9777\n",
      "Epoch 10: val_loss did not improve from 0.04902\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0696 - accuracy: 0.9777 - val_loss: 0.0505 - val_accuracy: 0.9843\n",
      "Epoch 11/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9793\n",
      "Epoch 11: val_loss did not improve from 0.04902\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0668 - accuracy: 0.9793 - val_loss: 0.0494 - val_accuracy: 0.9847\n",
      "Epoch 12/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0654 - accuracy: 0.9795\n",
      "Epoch 12: val_loss improved from 0.04902 to 0.04858, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0654 - accuracy: 0.9796 - val_loss: 0.0486 - val_accuracy: 0.9844\n",
      "Epoch 13/30\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9808\n",
      "Epoch 13: val_loss improved from 0.04858 to 0.04540, saving model to FullModelCheckpoint.h5\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0613 - accuracy: 0.9808 - val_loss: 0.0454 - val_accuracy: 0.9861\n",
      "Epoch 14/30\n",
      "599/600 [============================>.] - ETA: 0s - loss: 0.0634 - accuracy: 0.9799\n",
      "Epoch 14: val_loss did not improve from 0.04540\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0634 - accuracy: 0.9799 - val_loss: 0.0471 - val_accuracy: 0.9853\n",
      "Epoch 15/30\n",
      "518/600 [========================>.....] - ETA: 2s - loss: 0.0593 - accuracy: 0.9810"
     ]
    }
   ],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    model = create_model()\n",
    "    model.compile( loss=\"categorical_crossentropy\",  optimizer=\"Adam\",  metrics='accuracy' )\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=15, verbose=1)\n",
    "\n",
    "    history = model.fit( X_train, Y_train, batch_size = 64, epochs=30,\n",
    "                        validation_split=.20,callbacks=[ keras.callbacks.ModelCheckpoint( \"FullModelCheckpoint.h5\",\n",
    "                        verbose=1, save_best_only=True ),early_stopping ], )\n",
    "\n",
    "    plot_history(\n",
    "        history.history[\"loss\"],\n",
    "        history.history[\"val_loss\"],\n",
    "        history.history[\"accuracy\"],\n",
    "        history.history[\"val_accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate( X_test, Y_test , verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return model\n",
    "\n",
    "\n",
    "full_dataset_model = train_full_model(X_all, X_test, Y_all, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e25618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8977779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks=[keras.callbacks.EarlyStopping(patience=4, verbose=1), ],\n",
    "\n",
    "def train_full_model(X_train, X_test, Y_train, Y_test):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    model = create_model()\n",
    "    model.compile( loss=\"categorical_crossentropy\",  optimizer=\"Adam\",  metrics='accuracy' )\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=15, verbose=1)\n",
    "\n",
    "    history = model.fit( X_train, Y_train, batch_size = 64, epochs=30,\n",
    "                        validation_split=.20,callbacks=[ keras.callbacks.ModelCheckpoint( \"FullModelCheckpoint.h5\",\n",
    "                        verbose=1, save_best_only=True ),early_stopping ], )\n",
    "\n",
    "    plot_history(\n",
    "        history.history[\"loss\"],\n",
    "        history.history[\"val_loss\"],\n",
    "        history.history[\"accuracy\"],\n",
    "        history.history[\"val_accuracy\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(\n",
    "        \"Test set evaluation: \",\n",
    "        model.evaluate( X_test, Y_test , verbose=0, return_dict=True),\n",
    "    )\n",
    "    print(\"-\" * 100)\n",
    "    return model\n",
    "\n",
    "\n",
    "full_dataset_model = train_full_model(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,X_train, Y_train):\n",
    "    \n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    \n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        \"AL_Model.h5\", save_best_only=True, verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=15, verbose=1)\n",
    "\n",
    "    print(f\"Starting to train with {len(Y_train)} samples\")\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size = 64, epochs=30,validation_split=.20,\n",
    "                        callbacks=[checkpoint],\n",
    "    )\n",
    "    plot_history(\n",
    "        history.history[\"loss\"],\n",
    "        history.history[\"val_loss\"],\n",
    "        history.history[\"accuracy\"],\n",
    "        history.history[\"val_accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdd178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X_test, Y_test):\n",
    "    print(\"-\" * 100)\n",
    "    print(\"Test set evaluation: \", model.evaluate( X_test, Y_test , verbose=0, return_dict=True), )\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "\n",
    "def train_active_learning_models(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    x_unlab,\n",
    "    y_unlab,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    num_iterations=5\n",
    "):\n",
    "\n",
    "    # Creating lists for storing metrics\n",
    "    losses, val_losses, accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "    model = create_model()\n",
    "    # We will monitor the false positives and false negatives predicted by our model\n",
    "    # These will decide the subsequent sampling ratio for every Active Learning loop\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"Adam\",\n",
    "        metrics='accuracy',\n",
    "    )\n",
    "    \n",
    "    model = train(model,X_train, Y_train)\n",
    "    \n",
    "    d = 100/num_iterations\n",
    "    l = len(y_unlab)\n",
    "    x = int(np.round( l/d ))\n",
    "    \n",
    "    for iteration in range(num_iterations-1):\n",
    "        \n",
    "        l = len(y_unlab)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        print(\"Iteration : \")\n",
    "        \n",
    "        print(iteration)\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "         \n",
    "        #generate random number and substract from all numbers\n",
    "        rnd = random.sample(range(1, l), x)\n",
    "        all = list(range(1, l))\n",
    "        main_list = list(set(all) - set(rnd))\n",
    "        \n",
    "        #add those index to from unlablled set to training set\n",
    "        new_lab = x_unlab[rnd]\n",
    "        arr = np.concatenate((X_train, new_lab))\n",
    "        X_train = arr\n",
    "\n",
    "        #predict on the set and add to training data\n",
    "        new_y = np.round(model.predict(new_lab))\n",
    "        arr = np.concatenate((Y_train, new_y))\n",
    "        Y_train = arr\n",
    "        \n",
    "        \n",
    "        #create the new unlabelled set\n",
    "        x_unlab = x_unlab[main_list]\n",
    "\n",
    "        #create the new unlabelled label set\n",
    "        y_unlab = y_unlab[main_list]\n",
    "        \n",
    "        #test on data\n",
    "        test(model, X_test, Y_test)\n",
    "        \n",
    "        #train on data\n",
    "        model = train(model,X_train, Y_train)\n",
    "        \n",
    "    #Final round training set\n",
    "    arr = np.concatenate((X_train, x_unlab))\n",
    "    X_train = arr\n",
    "\n",
    "    #Final round training label set\n",
    "    new_y = np.round(model.predict(x_unlab))\n",
    "    arr = np.concatenate((Y_train, new_y))\n",
    "    Y_train = arr\n",
    "    \n",
    "    #train final\n",
    "    model = train(model,X_train, Y_train)\n",
    "    \n",
    "    #test for final time\n",
    "    test(model, X_test, Y_test)\n",
    "        \n",
    "    #return model\n",
    "    return model\n",
    "\n",
    "active_learning_model = train_active_learning_models(X_train,Y_train,x_unlab,y_unlab,X_test,Y_test,num_iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43920b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b56b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa8ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49471793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
