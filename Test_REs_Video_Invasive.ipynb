{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c00f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 5, 28, 28, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Define the folder location\n",
    "folder_location = r\"C:\\Users\\shaif\\Downloads\\Compressed\\Labeled Baylor 8-30-23\\Labeled Baylor 8-30-23\\Zebra Umbo 1a Image1a\\Umbonal\"\n",
    "\n",
    "folders = os.listdir(folder_location)\n",
    "\n",
    "# Define the desired image size\n",
    "target_size = (28, 28)\n",
    "\n",
    "# Initialize an empty list to store the processed images\n",
    "processed_images = []\n",
    "\n",
    "# Define the number of images to read from each subdirectory\n",
    "num_images_per_subdirectory = 5\n",
    "\n",
    "# Iterate through each folder\n",
    "for folder in folders:\n",
    "    # Construct the folder path\n",
    "    folder_path = os.path.join(folder_location, folder)\n",
    "    \n",
    "    # List all images in the folder\n",
    "    images = os.listdir(folder_path)\n",
    "    \n",
    "    # Initialize a list to store images from this subdirectory\n",
    "    subdirectory_images = []\n",
    "    \n",
    "    # Iterate through each image, up to the specified limit\n",
    "    for i in range(min(num_images_per_subdirectory, len(images))):\n",
    "        # Construct the image path\n",
    "        image_path = os.path.join(folder_path, images[i])\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Resize the image using TensorFlow\n",
    "        resized_image = tf.image.resize_with_crop_or_pad(\n",
    "            tf.keras.preprocessing.image.img_to_array(image),\n",
    "            target_size[0],\n",
    "            target_size[1]\n",
    "        )\n",
    "        \n",
    "        # Normalize the data for deep learning\n",
    "        normalized_image = (resized_image - 127.5) / 127.5\n",
    "        \n",
    "        # Append the processed image to the subdirectory list\n",
    "        subdirectory_images.append(normalized_image)\n",
    "    \n",
    "    # Convert the list of images for this subdirectory to a NumPy array\n",
    "    subdirectory_images = np.array(subdirectory_images)\n",
    "    \n",
    "    # Append the subdirectory images to the main list\n",
    "    processed_images.append(subdirectory_images)\n",
    "\n",
    "# Convert the list of images to a NumPy array\n",
    "processed_images = np.array(processed_images)\n",
    "processed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585710ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca0ef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5, 28, 28,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 5, 32)        11064       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 5, 32)       64          ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 5, 32)       25184       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 5, 32)        8320        ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 5, 32)        0           ['multi_head_attention[0][0]',   \n",
      "                                                                  'patch_encoder[0][0]',          \n",
      "                                                                  'lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 5, 32)       64          ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 5, 32)        0           ['sequential_1[0][0]',           \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 5, 32)       64          ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 5, 32)       25184       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 5, 32)        8320        ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 5, 32)        0           ['multi_head_attention_1[0][0]', \n",
      "                                                                  'add_1[0][0]',                  \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 5, 32)       64          ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 5, 32)        0           ['sequential_2[0][0]',           \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 5, 32)       64          ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 5, 32)       25184       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 5, 32)        8320        ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 5, 32)        0           ['multi_head_attention_2[0][0]', \n",
      "                                                                  'add_3[0][0]',                  \n",
      "                                                                  'lstm_2[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 5, 32)       64          ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 5, 32)        0           ['sequential_3[0][0]',           \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 5, 32)       64          ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 5, 32)       25184       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 5, 32)        8320        ['layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 5, 32)        0           ['multi_head_attention_3[0][0]', \n",
      "                                                                  'add_5[0][0]',                  \n",
      "                                                                  'lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 5, 32)       64          ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 5, 32)        0           ['sequential_4[0][0]',           \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 5, 32)       64          ['add_7[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 5, 32)       25184       ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 5, 32)        8320        ['layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 5, 32)        0           ['multi_head_attention_4[0][0]', \n",
      "                                                                  'add_7[0][0]',                  \n",
      "                                                                  'lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 5, 32)       64          ['add_8[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 5, 32)        0           ['sequential_5[0][0]',           \n",
      "                                                                  'add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 5, 32)       64          ['add_9[0][0]']                  \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 5, 32)       25184       ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 5, 32)        8320        ['layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 5, 32)        0           ['multi_head_attention_5[0][0]', \n",
      "                                                                  'add_9[0][0]',                  \n",
      "                                                                  'lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 5, 32)       64          ['add_10[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 5, 32)        1056        ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 5, 32)        0           ['sequential_6[0][0]',           \n",
      "                                                                  'add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 5, 32)       64          ['add_11[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 32)          0           ['layer_normalization_12[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 3)            99          ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 219,355\n",
      "Trainable params: 219,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Patch Encoder with Conv2D ,  LSTM , Pos_Emd\n",
    "from keras import layers\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super(PatchEncoder, self).__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.projection_dim = projection_dim\n",
    "        self.projection =keras.Sequential(\n",
    "            [\n",
    "                (layers.Conv2D(2, (3, 3), strides=(1,1),activation='relu')),\n",
    "                TimeDistributed(MaxPooling2D(2,2)),\n",
    "                TimeDistributed(Flatten()),\n",
    "                layers.Dense(projection_dim)\n",
    "            ]) \n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"projection_dim\": self.projection_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded \n",
    "\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import TimeDistributed, Conv2D, Dense, MaxPooling2D, Flatten, LSTM, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import load_model\n",
    "model_path = r'E:\\Video_Model\\Video_Model\\five_frame_model.h5'\n",
    "# Load the saved Keras model with the custom metric\n",
    "model = load_model(model_path, custom_objects={\"PatchEncoder\": PatchEncoder})\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ddc1517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 539\n",
      "Count of 1: 443\n",
      "Count of 2: 0\n"
     ]
    }
   ],
   "source": [
    "x = model.predict(processed_images)\n",
    "result = []\n",
    "\n",
    "for i in x:\n",
    "    arr = []\n",
    "    o = i[0] - .4999\n",
    "    n = i[1] + .4999\n",
    "    m = i[2]\n",
    "    \n",
    "    arr.append(o)\n",
    "    arr.append(n)\n",
    "    arr.append(m)\n",
    "    \n",
    "    result.append(arr)\n",
    "\n",
    "res = np.argmax(result,axis = 1)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "# Count the numbers\n",
    "for i in res:\n",
    "    if(i==0):\n",
    "        count_0  = count_0  + 1\n",
    "    elif(i==1):\n",
    "        count_1  = count_1  + 1\n",
    "    else:\n",
    "        count_2  = count_2  + 1\n",
    "\n",
    "# Print the counts\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)\n",
    "print(\"Count of 2:\", count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac85bd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0: 73\n",
      "Count of 1: 8\n",
      "Count of 2: 2\n"
     ]
    }
   ],
   "source": [
    "x = model.predict(processed_images)\n",
    "res = np.argmax(x,axis = 1)\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "# Count the numbers\n",
    "for i in res:\n",
    "    if(i==0):\n",
    "        count_0  = count_0  + 1\n",
    "    elif(i==1):\n",
    "        count_1  = count_1  + 1\n",
    "    else:\n",
    "        count_2  = count_2  + 1\n",
    "\n",
    "# Print the counts\n",
    "print(\"Count of 0:\", count_0)\n",
    "print(\"Count of 1:\", count_1)\n",
    "print(\"Count of 2:\", count_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf39941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d2624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c52de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9045916f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178ef8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd03a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9e799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2470fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41353bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809669d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
