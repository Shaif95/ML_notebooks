{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfd55cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0384eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "q = discard_random_image(image_parts[15])\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def resize_images(q,target_size=(56, 56)):\n",
    "    img = Image.fromarray(q)\n",
    "    resized_img = img.resize((56,56), Image.ANTIALIAS)\n",
    "    return resized_img\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "def prepare_videoes(image_paths, target_size=(56, 56)):\n",
    "    images = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        # Load image\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # Convert to RGB if not already\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Resize the image\n",
    "        img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Append the processed image to the list\n",
    "        images.append(img_array)\n",
    "    \n",
    "    num_images = len(images)\n",
    "    num_parts = 16\n",
    "    part_length = num_images // num_parts\n",
    "    remaining = num_images % num_parts\n",
    "    \n",
    "    image_parts = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        end_idx = start_idx + part_length + (1 if i < remaining else 0)\n",
    "        image_part = images[start_idx:end_idx]\n",
    "        image_parts.append(image_part)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    processed_parts = [discard_random_image(part) for part in image_parts]\n",
    "    \n",
    "    img_parts = [resize_images(part) for part in processed_parts]\n",
    "    \n",
    "    combined_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    for i, img_part in enumerate(img_parts):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        combined_image[row*56:(row+1)*56, col*56:(col+1)*56, :] = img_part\n",
    "    \n",
    "    return combined_image\n",
    "\n",
    "#combined_image = prepare_videoes(vid)\n",
    "#print(\"Combined image shape:\", combined_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "len(tr)\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))\n",
    "\n",
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)\n",
    "\n",
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    #print(\"..........................\")\n",
    "    #print(x)\n",
    "    #print(\"..........................\")\n",
    "    imgdata=prepare_videoes(x)\n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata.astype('float32') / 255.\n",
    "    #print(\"..........................\")\n",
    "    #print(np.shape(X_train))\n",
    "    #print(\"..........................\")\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)\n",
    "\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty arrays to store training and validation data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "batch_size = 16\n",
    "num_train_samples = len(train)  # Number of training samples\n",
    "num_val_samples = len(val)      # Number of validation samples\n",
    "\n",
    "# Initialize your custom batch generators\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "# Load training data\n",
    "for batch_idx in range(num_train_samples // batch_size):\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(batch_idx)\n",
    "    X_train.append(batch_images)\n",
    "    Y_train.append(batch_labels)\n",
    "\n",
    "# Load any remaining training data (if num_train_samples is not a multiple of batch_size)\n",
    "if num_train_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(num_train_samples // batch_size)\n",
    "    X_train.append(batch_images[:num_train_samples % batch_size])\n",
    "    Y_train.append(batch_labels[:num_train_samples % batch_size])\n",
    "\n",
    "# Load validation data\n",
    "for batch_idx in range(num_val_samples // batch_size):\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(batch_idx)\n",
    "    X_val.append(batch_images)\n",
    "    Y_val.append(batch_labels)\n",
    "\n",
    "# Load any remaining validation data (if num_val_samples is not a multiple of batch_size)\n",
    "if num_val_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(num_val_samples // batch_size)\n",
    "    X_val.append(batch_images[:num_val_samples % batch_size])\n",
    "    Y_val.append(batch_labels[:num_val_samples % batch_size])\n",
    "\n",
    "# Concatenate the loaded batches into arrays\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "Y_val = np.concatenate(Y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b760a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f18639f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b524c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52fc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08b935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7fadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49fa93e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf27a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e407131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc010782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588eedd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc865f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c1f5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b7ef99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e814b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827799a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090ec4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "799b5d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbb61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89200411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "153/153 [==============================] - 44s 176ms/step - loss: 3.9425 - accuracy: 0.0695 - val_loss: 7.9276 - val_accuracy: 0.0978\n",
      "Epoch 2/100\n",
      "153/153 [==============================] - 24s 159ms/step - loss: 3.4085 - accuracy: 0.1263 - val_loss: 3.6552 - val_accuracy: 0.1144\n",
      "Epoch 3/100\n",
      "153/153 [==============================] - 25s 165ms/step - loss: 3.2121 - accuracy: 0.1497 - val_loss: 4.7729 - val_accuracy: 0.1550\n",
      "Epoch 4/100\n",
      "153/153 [==============================] - 26s 171ms/step - loss: 3.2678 - accuracy: 0.1532 - val_loss: 51.4894 - val_accuracy: 0.0277\n",
      "Epoch 5/100\n",
      "153/153 [==============================] - 38s 247ms/step - loss: 3.1796 - accuracy: 0.1758 - val_loss: 3.1580 - val_accuracy: 0.1753\n",
      "Epoch 6/100\n",
      "153/153 [==============================] - 52s 340ms/step - loss: 2.9199 - accuracy: 0.2149 - val_loss: 3.0649 - val_accuracy: 0.1956\n",
      "Epoch 7/100\n",
      "153/153 [==============================] - 55s 359ms/step - loss: 2.7510 - accuracy: 0.2490 - val_loss: 3.1173 - val_accuracy: 0.2196\n",
      "Epoch 8/100\n",
      "153/153 [==============================] - 58s 383ms/step - loss: 2.5297 - accuracy: 0.3099 - val_loss: 3.2300 - val_accuracy: 0.1863\n",
      "Epoch 9/100\n",
      "153/153 [==============================] - 58s 377ms/step - loss: 2.4050 - accuracy: 0.3445 - val_loss: 2.9070 - val_accuracy: 0.2546\n",
      "Epoch 10/100\n",
      "153/153 [==============================] - 66s 430ms/step - loss: 2.3851 - accuracy: 0.3564 - val_loss: 3.6094 - val_accuracy: 0.2325\n",
      "Epoch 11/100\n",
      "153/153 [==============================] - 59s 389ms/step - loss: 2.4974 - accuracy: 0.3365 - val_loss: 3.2302 - val_accuracy: 0.2454\n",
      "Epoch 12/100\n",
      "153/153 [==============================] - 55s 355ms/step - loss: 2.0284 - accuracy: 0.4331 - val_loss: 2.9824 - val_accuracy: 0.2565\n",
      "Epoch 13/100\n",
      "153/153 [==============================] - 57s 371ms/step - loss: 1.9471 - accuracy: 0.5016 - val_loss: 3690.6309 - val_accuracy: 0.0295\n",
      "Epoch 14/100\n",
      "153/153 [==============================] - 59s 388ms/step - loss: 2.2454 - accuracy: 0.4567 - val_loss: 3.0255 - val_accuracy: 0.2731\n",
      "Epoch 15/100\n",
      "153/153 [==============================] - 64s 419ms/step - loss: 1.5393 - accuracy: 0.5693 - val_loss: 2.9832 - val_accuracy: 0.3044\n",
      "Epoch 16/100\n",
      "153/153 [==============================] - 66s 432ms/step - loss: 1.1995 - accuracy: 0.6591 - val_loss: 3.2056 - val_accuracy: 0.2952\n",
      "Epoch 17/100\n",
      "153/153 [==============================] - 63s 413ms/step - loss: 1.1201 - accuracy: 0.6963 - val_loss: 4.4169 - val_accuracy: 0.2380\n",
      "Epoch 18/100\n",
      "153/153 [==============================] - 67s 437ms/step - loss: 0.9195 - accuracy: 0.7404 - val_loss: 3.4908 - val_accuracy: 0.2860\n",
      "Epoch 19/100\n",
      "153/153 [==============================] - 72s 473ms/step - loss: 0.7596 - accuracy: 0.7855 - val_loss: 3.5940 - val_accuracy: 0.2620\n",
      "Epoch 20/100\n",
      "153/153 [==============================] - 68s 444ms/step - loss: 0.6725 - accuracy: 0.8126 - val_loss: 3.8766 - val_accuracy: 0.2897\n",
      "Epoch 21/100\n",
      "153/153 [==============================] - 69s 452ms/step - loss: 1.4285 - accuracy: 0.6345 - val_loss: 5.0496 - val_accuracy: 0.2620\n",
      "Epoch 22/100\n",
      "153/153 [==============================] - 66s 430ms/step - loss: 1.6297 - accuracy: 0.5742 - val_loss: 4.0366 - val_accuracy: 0.1790\n",
      "Epoch 23/100\n",
      "153/153 [==============================] - 74s 481ms/step - loss: 1.3130 - accuracy: 0.6302 - val_loss: 3.6583 - val_accuracy: 0.3100\n",
      "Epoch 24/100\n",
      "153/153 [==============================] - 65s 426ms/step - loss: 0.8715 - accuracy: 0.7619 - val_loss: 3.9924 - val_accuracy: 0.2638\n",
      "Epoch 25/100\n",
      "153/153 [==============================] - 69s 451ms/step - loss: 0.7282 - accuracy: 0.7974 - val_loss: 3.6903 - val_accuracy: 0.3266\n",
      "Epoch 26/100\n",
      "153/153 [==============================] - 83s 538ms/step - loss: 0.5648 - accuracy: 0.8456 - val_loss: 4.0089 - val_accuracy: 0.2841\n",
      "Epoch 27/100\n",
      "153/153 [==============================] - 76s 497ms/step - loss: 0.5775 - accuracy: 0.8470 - val_loss: 5.2394 - val_accuracy: 0.2251\n",
      "Epoch 28/100\n",
      "153/153 [==============================] - 67s 431ms/step - loss: 0.5731 - accuracy: 0.8472 - val_loss: 4.2656 - val_accuracy: 0.3063\n",
      "Epoch 29/100\n",
      "153/153 [==============================] - 77s 505ms/step - loss: 0.3424 - accuracy: 0.9100 - val_loss: 4.6220 - val_accuracy: 0.2952\n",
      "Epoch 30/100\n",
      "153/153 [==============================] - 75s 493ms/step - loss: 0.3205 - accuracy: 0.9223 - val_loss: 4.4983 - val_accuracy: 0.2915\n",
      "Epoch 31/100\n",
      "153/153 [==============================] - 70s 457ms/step - loss: 0.7165 - accuracy: 0.8400 - val_loss: 4.8091 - val_accuracy: 0.2712\n",
      "Epoch 32/100\n",
      "153/153 [==============================] - 78s 512ms/step - loss: 0.4134 - accuracy: 0.8923 - val_loss: 4.5698 - val_accuracy: 0.2620\n",
      "Epoch 33/100\n",
      "153/153 [==============================] - 76s 500ms/step - loss: 0.4249 - accuracy: 0.8845 - val_loss: 5.5185 - val_accuracy: 0.2509\n",
      "Epoch 34/100\n",
      "153/153 [==============================] - 73s 478ms/step - loss: 0.5286 - accuracy: 0.8632 - val_loss: 4.9857 - val_accuracy: 0.2694\n",
      "Epoch 35/100\n",
      "153/153 [==============================] - 69s 453ms/step - loss: 0.5573 - accuracy: 0.8388 - val_loss: 4.8399 - val_accuracy: 0.3081\n",
      "Epoch 36/100\n",
      "153/153 [==============================] - 74s 481ms/step - loss: 0.4268 - accuracy: 0.8841 - val_loss: 7.3539 - val_accuracy: 0.1568\n",
      "Epoch 37/100\n",
      "153/153 [==============================] - 72s 467ms/step - loss: 0.5480 - accuracy: 0.8489 - val_loss: 4.5793 - val_accuracy: 0.2915\n",
      "Epoch 38/100\n",
      "153/153 [==============================] - 61s 398ms/step - loss: 0.3058 - accuracy: 0.9241 - val_loss: 4.9475 - val_accuracy: 0.3303\n",
      "Epoch 39/100\n",
      "153/153 [==============================] - 66s 435ms/step - loss: 0.2867 - accuracy: 0.9297 - val_loss: 4.9132 - val_accuracy: 0.3358\n",
      "Epoch 40/100\n",
      "153/153 [==============================] - 72s 474ms/step - loss: 0.2908 - accuracy: 0.9276 - val_loss: 5.1284 - val_accuracy: 0.3044\n",
      "Epoch 41/100\n",
      "153/153 [==============================] - 72s 474ms/step - loss: 0.3143 - accuracy: 0.9219 - val_loss: 5.1059 - val_accuracy: 0.2897\n",
      "Epoch 42/100\n",
      "153/153 [==============================] - 65s 424ms/step - loss: 0.3038 - accuracy: 0.9190 - val_loss: 5.4026 - val_accuracy: 0.2915\n",
      "Epoch 43/100\n",
      "153/153 [==============================] - 66s 436ms/step - loss: 0.2420 - accuracy: 0.9387 - val_loss: 5.0943 - val_accuracy: 0.3137\n",
      "Epoch 44/100\n",
      "153/153 [==============================] - 67s 436ms/step - loss: 0.1976 - accuracy: 0.9506 - val_loss: 5.0626 - val_accuracy: 0.3118\n",
      "Epoch 45/100\n",
      "153/153 [==============================] - 74s 484ms/step - loss: 0.2665 - accuracy: 0.9340 - val_loss: 6.6415 - val_accuracy: 0.2048\n",
      "Epoch 46/100\n",
      "153/153 [==============================] - 75s 494ms/step - loss: 0.5752 - accuracy: 0.8454 - val_loss: 5.3409 - val_accuracy: 0.2472\n",
      "Epoch 47/100\n",
      "153/153 [==============================] - 82s 537ms/step - loss: 0.9025 - accuracy: 0.7672 - val_loss: 4.6537 - val_accuracy: 0.2712\n",
      "Epoch 48/100\n",
      "153/153 [==============================] - 67s 443ms/step - loss: 0.3935 - accuracy: 0.9061 - val_loss: 4.8816 - val_accuracy: 0.2878\n",
      "Epoch 49/100\n",
      "153/153 [==============================] - 73s 478ms/step - loss: 0.5123 - accuracy: 0.8575 - val_loss: 5.2442 - val_accuracy: 0.2638\n",
      "Epoch 50/100\n",
      "153/153 [==============================] - 77s 507ms/step - loss: 0.4876 - accuracy: 0.8675 - val_loss: 5.0809 - val_accuracy: 0.3044\n",
      "Epoch 51/100\n",
      "153/153 [==============================] - 76s 499ms/step - loss: 0.4700 - accuracy: 0.8735 - val_loss: 5.3390 - val_accuracy: 0.2768\n",
      "Epoch 52/100\n",
      "153/153 [==============================] - 65s 429ms/step - loss: 0.3172 - accuracy: 0.9153 - val_loss: 5.2236 - val_accuracy: 0.2878\n",
      "Epoch 53/100\n",
      "153/153 [==============================] - 73s 478ms/step - loss: 0.2788 - accuracy: 0.9305 - val_loss: 22.0652 - val_accuracy: 0.2232\n",
      "Epoch 54/100\n",
      "153/153 [==============================] - 73s 481ms/step - loss: 0.5777 - accuracy: 0.8597 - val_loss: 4.9118 - val_accuracy: 0.2989\n",
      "Epoch 55/100\n",
      "153/153 [==============================] - 70s 459ms/step - loss: 0.2622 - accuracy: 0.9292 - val_loss: 5.3651 - val_accuracy: 0.2638\n",
      "Epoch 56/100\n",
      "153/153 [==============================] - 70s 458ms/step - loss: 0.2390 - accuracy: 0.9399 - val_loss: 5.6912 - val_accuracy: 0.2601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "153/153 [==============================] - 74s 483ms/step - loss: 0.2395 - accuracy: 0.9379 - val_loss: 5.7348 - val_accuracy: 0.2214\n",
      "Epoch 58/100\n",
      "153/153 [==============================] - 77s 508ms/step - loss: 0.4174 - accuracy: 0.8993 - val_loss: 5.7277 - val_accuracy: 0.2177\n",
      "Epoch 59/100\n",
      "153/153 [==============================] - 82s 537ms/step - loss: 0.3125 - accuracy: 0.9331 - val_loss: 6.3060 - val_accuracy: 0.2768\n",
      "Epoch 60/100\n",
      "153/153 [==============================] - 76s 498ms/step - loss: 0.2734 - accuracy: 0.9299 - val_loss: 6.0974 - val_accuracy: 0.2362\n",
      "Epoch 61/100\n",
      "153/153 [==============================] - 77s 508ms/step - loss: 0.3424 - accuracy: 0.9038 - val_loss: 5.7418 - val_accuracy: 0.2749\n",
      "Epoch 62/100\n",
      "153/153 [==============================] - 76s 496ms/step - loss: 0.3845 - accuracy: 0.8921 - val_loss: 5.6991 - val_accuracy: 0.2657\n",
      "Epoch 63/100\n",
      "153/153 [==============================] - 75s 492ms/step - loss: 0.4196 - accuracy: 0.8864 - val_loss: 5.9437 - val_accuracy: 0.2712\n",
      "Epoch 64/100\n",
      "153/153 [==============================] - 80s 522ms/step - loss: 0.3567 - accuracy: 0.9094 - val_loss: 5.5231 - val_accuracy: 0.2970\n",
      "Epoch 65/100\n",
      "153/153 [==============================] - 78s 507ms/step - loss: 0.2978 - accuracy: 0.9194 - val_loss: 5.9939 - val_accuracy: 0.2232\n",
      "Epoch 66/100\n",
      "153/153 [==============================] - 69s 454ms/step - loss: 0.3032 - accuracy: 0.9155 - val_loss: 5.7642 - val_accuracy: 0.2841\n",
      "Epoch 67/100\n",
      "153/153 [==============================] - 67s 442ms/step - loss: 0.2137 - accuracy: 0.9516 - val_loss: 5.9702 - val_accuracy: 0.2989\n",
      "Epoch 68/100\n",
      "153/153 [==============================] - 69s 450ms/step - loss: 0.1916 - accuracy: 0.9522 - val_loss: 5.9233 - val_accuracy: 0.2823\n",
      "Epoch 69/100\n",
      "153/153 [==============================] - 80s 522ms/step - loss: 0.3204 - accuracy: 0.9210 - val_loss: 5.7960 - val_accuracy: 0.3044\n",
      "Epoch 70/100\n",
      "153/153 [==============================] - 84s 543ms/step - loss: 0.4050 - accuracy: 0.9075 - val_loss: 5.4726 - val_accuracy: 0.2712\n",
      "Epoch 71/100\n",
      "153/153 [==============================] - 87s 568ms/step - loss: 0.4082 - accuracy: 0.8901 - val_loss: 6.0563 - val_accuracy: 0.2915\n",
      "Epoch 72/100\n",
      "153/153 [==============================] - 81s 529ms/step - loss: 0.4228 - accuracy: 0.8913 - val_loss: 5.3533 - val_accuracy: 0.2934\n",
      "Epoch 73/100\n",
      "153/153 [==============================] - 83s 544ms/step - loss: 0.2850 - accuracy: 0.9219 - val_loss: 6.1325 - val_accuracy: 0.2601\n",
      "Epoch 74/100\n",
      "153/153 [==============================] - 85s 555ms/step - loss: 0.2492 - accuracy: 0.9387 - val_loss: 5.6360 - val_accuracy: 0.3063\n",
      "Epoch 75/100\n",
      "153/153 [==============================] - 83s 545ms/step - loss: 0.4225 - accuracy: 0.8843 - val_loss: 6.4133 - val_accuracy: 0.2435\n",
      "Epoch 76/100\n",
      "153/153 [==============================] - 86s 566ms/step - loss: 0.3262 - accuracy: 0.9132 - val_loss: 6.9404 - val_accuracy: 0.1771\n",
      "Epoch 77/100\n",
      "153/153 [==============================] - 83s 544ms/step - loss: 0.6640 - accuracy: 0.8236 - val_loss: 5.9721 - val_accuracy: 0.1661\n",
      "Epoch 78/100\n",
      "153/153 [==============================] - 84s 545ms/step - loss: 0.5488 - accuracy: 0.8544 - val_loss: 5.4147 - val_accuracy: 0.2786\n",
      "Epoch 79/100\n",
      "153/153 [==============================] - 84s 548ms/step - loss: 0.3554 - accuracy: 0.9243 - val_loss: 5.3252 - val_accuracy: 0.3137\n",
      "Epoch 80/100\n",
      "153/153 [==============================] - 85s 557ms/step - loss: 0.3030 - accuracy: 0.9190 - val_loss: 7.3484 - val_accuracy: 0.2066\n",
      "Epoch 81/100\n",
      "153/153 [==============================] - 84s 553ms/step - loss: 0.6792 - accuracy: 0.8150 - val_loss: 5.0363 - val_accuracy: 0.2915\n",
      "Epoch 82/100\n",
      "153/153 [==============================] - 83s 544ms/step - loss: 0.3461 - accuracy: 0.9085 - val_loss: 5.7230 - val_accuracy: 0.2196\n",
      "Epoch 83/100\n",
      "153/153 [==============================] - 83s 547ms/step - loss: 0.3034 - accuracy: 0.9192 - val_loss: 6.2107 - val_accuracy: 0.2970\n",
      "Epoch 84/100\n",
      "153/153 [==============================] - 84s 550ms/step - loss: 0.2940 - accuracy: 0.9411 - val_loss: 6.1554 - val_accuracy: 0.2823\n",
      "Epoch 85/100\n",
      "153/153 [==============================] - 86s 564ms/step - loss: 0.2215 - accuracy: 0.9440 - val_loss: 6.3793 - val_accuracy: 0.2860\n",
      "Epoch 86/100\n",
      "153/153 [==============================] - 89s 585ms/step - loss: 0.1876 - accuracy: 0.9543 - val_loss: 6.7911 - val_accuracy: 0.3100\n",
      "Epoch 87/100\n",
      "153/153 [==============================] - 85s 559ms/step - loss: 0.2398 - accuracy: 0.9374 - val_loss: 6.7683 - val_accuracy: 0.2804\n",
      "Epoch 88/100\n",
      "153/153 [==============================] - 80s 527ms/step - loss: 0.2461 - accuracy: 0.9350 - val_loss: 6.2816 - val_accuracy: 0.2565\n",
      "Epoch 89/100\n",
      "153/153 [==============================] - 83s 540ms/step - loss: 0.3074 - accuracy: 0.9135 - val_loss: 5.9441 - val_accuracy: 0.2731\n",
      "Epoch 90/100\n",
      "153/153 [==============================] - 77s 500ms/step - loss: 0.2048 - accuracy: 0.9438 - val_loss: 6.3442 - val_accuracy: 0.2897\n",
      "Epoch 91/100\n",
      "153/153 [==============================] - 79s 516ms/step - loss: 0.1847 - accuracy: 0.9537 - val_loss: 5.8715 - val_accuracy: 0.2712\n",
      "Epoch 92/100\n",
      "153/153 [==============================] - 95s 622ms/step - loss: 0.2159 - accuracy: 0.9440 - val_loss: 6.4264 - val_accuracy: 0.3100\n",
      "Epoch 93/100\n",
      "153/153 [==============================] - 96s 631ms/step - loss: 0.1813 - accuracy: 0.9526 - val_loss: 6.5467 - val_accuracy: 0.2989\n",
      "Epoch 94/100\n",
      "153/153 [==============================] - 93s 606ms/step - loss: 0.1665 - accuracy: 0.9588 - val_loss: 6.3992 - val_accuracy: 0.2970\n",
      "Epoch 95/100\n",
      "153/153 [==============================] - 96s 633ms/step - loss: 0.1659 - accuracy: 0.9569 - val_loss: 6.5828 - val_accuracy: 0.3137\n",
      "Epoch 96/100\n",
      "153/153 [==============================] - 83s 543ms/step - loss: 0.7330 - accuracy: 0.8097 - val_loss: 5.5726 - val_accuracy: 0.2749\n",
      "Epoch 97/100\n",
      "153/153 [==============================] - 86s 557ms/step - loss: 0.4851 - accuracy: 0.8919 - val_loss: 5.6407 - val_accuracy: 0.2417\n",
      "Epoch 98/100\n",
      "153/153 [==============================] - 82s 539ms/step - loss: 0.6449 - accuracy: 0.8281 - val_loss: 5.1992 - val_accuracy: 0.2934\n",
      "Epoch 99/100\n",
      "153/153 [==============================] - 104s 678ms/step - loss: 0.3464 - accuracy: 0.9204 - val_loss: 5.4864 - val_accuracy: 0.3044\n",
      "Epoch 100/100\n",
      "153/153 [==============================] - 104s 677ms/step - loss: 0.3045 - accuracy: 0.9198 - val_loss: 6.3090 - val_accuracy: 0.2694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d95f1ee1c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50V2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "base_model = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(51, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.fit(x=X_train, y=Y_train, epochs=100, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0ee5880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2610759493670886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Assuming y_val and predf are your ground truth and predicted values, respectively\n",
    "accuracy = accuracy_score( np.argmax(Y_val,axis = 1) , np.argmax ( model.predict(X_val) ,axis = 1 ) )\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d79f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2babaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f71367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "num_classes =15\n",
    "y_train = np.argmax(Y_train,axis = 1)\n",
    "y_val = np.argmax(Y_val,axis=1)\n",
    "\n",
    "x_train = X_train.reshape(len(X_train), 3, 224, 224)\n",
    "x_val = X_val.reshape(len(X_val), 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a59fc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(632, 3, 224, 224)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68222940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1c96e2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx]),\n\u001b[0;32m     18\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# Ensure labels have the correct data type\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         }\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Assuming you have x_train and y_train defined somewhere\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#custom_dataset = CustomDataset(x_train, y_train)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m custom_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_training_batch_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Define your model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-large-patch32-224-in21k\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'pixel_values': torch.tensor(self.x[idx]),\n",
    "            'labels': torch.tensor(self.y[idx], dtype=torch.long)  # Ensure labels have the correct data type\n",
    "        }\n",
    "\n",
    "# Assuming you have x_train and y_train defined somewhere\n",
    "#custom_dataset = CustomDataset(x_train, y_train)\n",
    "custom_dataset = CustomDataset(my_training_batch_generator)\n",
    "\n",
    "# Define your model\n",
    "model_name_or_path = \"google/vit-large-patch32-224-in21k\"\n",
    "labels = list(range(num_classes))\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    ")\n",
    "\n",
    "# Update your training arguments and remove load_best_model_at_end\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-base-beans\",\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"no\",  # Keep evaluation strategy as \"epoch\"\n",
    "    num_train_epochs=50,\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    # Remove load_best_model_at_end\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=None,  # You don't need data_collator when using a custom DataLoader\n",
    "    train_dataset=custom_dataset,  # Pass the custom training dataset directly\n",
    ")\n",
    "\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming x_val is a tensor with shape (batch_size, channels, height, width)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x_val_tensor = x_val_tensor.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(x_val_tensor)  # This assumes your model outputs logits\n",
    "\n",
    "logits = output.logits\n",
    "pred = logits.argmax(dim=1).cpu().numpy()\n",
    "pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f51a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming y_val and predf are your ground truth and predicted values, respectively\n",
    "accuracy = accuracy_score(y_val, pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cd2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b678a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e922a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7ad8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
