{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bfd55cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import collections\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import shutil\n",
    "import keras\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import keras\n",
    "import json\n",
    "import tensorflow as tf \n",
    "from keras.layers import Input\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, LSTM,Flatten, TimeDistributed, Conv2D, Dropout\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D,Reshape, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten, UpSampling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras import layers\n",
    "from keras import models\n",
    "import keras\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dense, Dropout, Activation, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n",
    "\n",
    "def get_f1(y_true, y_pred): #taken from old keras source code\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    \n",
    "    tn = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip((1-y_true) * (y_pred), 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip((y_true) * (1-y_pred), 0, 1)))\n",
    "    \n",
    "\n",
    "    f1_val = tp / ( tp + ( (1/2) * (fp+fn) ) + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "\n",
    "len(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0384eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        #print(np.shape(images))\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "q = discard_random_image(image_parts[15])\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def resize_images(q,target_size=(56, 56)):\n",
    "    img = Image.fromarray(q)\n",
    "    resized_img = img.resize((56,56), Image.ANTIALIAS)\n",
    "    return resized_img\n",
    "\n",
    "def discard_random_image(images):\n",
    "    if len(images) == 3:\n",
    "        idx_to_discard = 0\n",
    "        images.pop(idx_to_discard)\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 2:\n",
    "        return np.concatenate((images[0], images[1]), axis=0)\n",
    "    elif len(images) == 1:\n",
    "        #print(np.shape(images))\n",
    "        return images[0]\n",
    "    elif len(images) > 3:\n",
    "        middle = len(images) // 2\n",
    "        left_part = images[:middle]\n",
    "        right_part = images[middle:]\n",
    "        return np.concatenate((discard_random_image(left_part), discard_random_image(right_part)), axis=0)\n",
    "\n",
    "def prepare_videoes(image_paths, target_size=(56, 56)):\n",
    "    images = []\n",
    "    \n",
    "    for path in image_paths:\n",
    "        # Load image\n",
    "        img = Image.open(path)\n",
    "        \n",
    "        # Convert to RGB if not already\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        \n",
    "        # Resize the image\n",
    "        img = img.resize(target_size, Image.ANTIALIAS)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Append the processed image to the list\n",
    "        images.append(img_array)\n",
    "    \n",
    "    num_images = len(images)\n",
    "    num_parts = 16\n",
    "    part_length = num_images // num_parts\n",
    "    remaining = num_images % num_parts\n",
    "    \n",
    "    image_parts = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in range(num_parts):\n",
    "        end_idx = start_idx + part_length + (1 if i < remaining else 0)\n",
    "        image_part = images[start_idx:end_idx]\n",
    "        image_parts.append(image_part)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    processed_parts = [discard_random_image(part) for part in image_parts]\n",
    "    \n",
    "    img_parts = [resize_images(part) for part in processed_parts]\n",
    "    \n",
    "    combined_image = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    for i, img_part in enumerate(img_parts):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        combined_image[row*56:(row+1)*56, col*56:(col+1)*56, :] = img_part\n",
    "    \n",
    "    return combined_image\n",
    "\n",
    "#combined_image = prepare_videoes(vid)\n",
    "#print(\"Combined image shape:\", combined_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b85bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_10228\\3781248220.py:61: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize(target_size, Image.ANTIALIAS)\n",
      "C:\\Users\\shaif\\AppData\\Local\\Temp\\ipykernel_10228\\3781248220.py:30: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  resized_img = img.resize((56,56), Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "trn='E:/D/PennA/Penn_Action/*/'\n",
    "tr= glob(trn)\n",
    "len(tr)\n",
    "\n",
    "train = []\n",
    "val = []\n",
    "test = []\n",
    "train_y = []\n",
    "val_y = []\n",
    "test_y = []\n",
    "\n",
    "y = 0\n",
    "for i in tr:\n",
    "    \n",
    "    #print(i)\n",
    "    x = glob(i+'/*/')\n",
    "    \n",
    "    #shuffle(x)\n",
    "    t,tt = train_test_split( x , test_size=0.1, random_state=42)\n",
    "    t, vv = train_test_split( t , test_size=0.1, random_state=42)\n",
    "    \n",
    "    for j in t:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "        \n",
    "        train.append(j)\n",
    "        train_y.append(y)\n",
    "    \n",
    "    for j in vv:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        val.append(j)\n",
    "        val_y.append(y)\n",
    "        \n",
    "    for j in tt:\n",
    "        \n",
    "        mm = len(glob(j+'/*'))\n",
    "        \n",
    "        if(mm<10):\n",
    "            continue\n",
    "            \n",
    "        test.append(j)\n",
    "        test_y.append(y)\n",
    "        \n",
    "    y = y+1\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "tra_y =  np.array(to_categorical(train_y))\n",
    "va_y  =  np.array(to_categorical(val_y))\n",
    "te_y  =  np.array(to_categorical(test_y))\n",
    "\n",
    "(train, tra_y) = shuffle(train, tra_y)\n",
    "(val, va_y) = shuffle(val, va_y)\n",
    "(test, te_y) = shuffle(test, te_y)\n",
    "\n",
    "\n",
    "def get_te(k , a) :\n",
    "    x = glob(k+'/*')\n",
    "    #print(\"..........................\")\n",
    "    #print(x)\n",
    "    #print(\"..........................\")\n",
    "    imgdata=prepare_videoes(x)\n",
    "    idata = np.array(imgdata)\n",
    "    X_train = idata.astype('float32') / 255.\n",
    "    #print(\"..........................\")\n",
    "    #print(np.shape(X_train))\n",
    "    #print(\"..........................\")\n",
    "    return X_train\n",
    "\n",
    "def get_cat(k) :\n",
    "    return np.array(k)\n",
    "\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "class My_Custom_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , labels, batch_size) :\n",
    "    self.filename = filename\n",
    "    self.labels = labels\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    y_train = get_cat(batch_y)\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x]), np.array( y_train )\n",
    "\n",
    "\n",
    "class My_Test_Generator(keras.utils.Sequence) :\n",
    "  \n",
    "  def __init__(self, filename , batch_size) :\n",
    "    self.filename = filename\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "  def __len__(self) :\n",
    "    return (np.ceil(len(self.filename) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "  def __getitem__(self, idx) :\n",
    "    batch_x = self.filename[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "    i=0\n",
    "    return np.array([get_te(i,self.filename)for i in batch_x])\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize empty arrays to store training and validation data\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "\n",
    "batch_size = 16\n",
    "num_train_samples = len(train)  # Number of training samples\n",
    "num_val_samples = len(val)      # Number of validation samples\n",
    "\n",
    "# Initialize your custom batch generators\n",
    "my_training_batch_generator = My_Custom_Generator(train, tra_y, batch_size)\n",
    "my_validation_batch_generator = My_Custom_Generator(val, va_y, batch_size)\n",
    "\n",
    "# Load training data\n",
    "for batch_idx in range(num_train_samples // batch_size):\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(batch_idx)\n",
    "    X_train.append(batch_images)\n",
    "    Y_train.append(batch_labels)\n",
    "\n",
    "# Load any remaining training data (if num_train_samples is not a multiple of batch_size)\n",
    "if num_train_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_training_batch_generator.__getitem__(num_train_samples // batch_size)\n",
    "    X_train.append(batch_images[:num_train_samples % batch_size])\n",
    "    Y_train.append(batch_labels[:num_train_samples % batch_size])\n",
    "\n",
    "# Load validation data\n",
    "for batch_idx in range(num_val_samples // batch_size):\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(batch_idx)\n",
    "    X_val.append(batch_images)\n",
    "    Y_val.append(batch_labels)\n",
    "\n",
    "# Load any remaining validation data (if num_val_samples is not a multiple of batch_size)\n",
    "if num_val_samples % batch_size != 0:\n",
    "    batch_images, batch_labels = my_validation_batch_generator.__getitem__(num_val_samples // batch_size)\n",
    "    X_val.append(batch_images[:num_val_samples % batch_size])\n",
    "    Y_val.append(batch_labels[:num_val_samples % batch_size])\n",
    "\n",
    "# Concatenate the loaded batches into arrays\n",
    "X_train = np.concatenate(X_train, axis=0)\n",
    "Y_train = np.concatenate(Y_train, axis=0)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "Y_val = np.concatenate(Y_val, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b760a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f18639f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b524c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52fc54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08b935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7fadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49fa93e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf27a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e407131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc010782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588eedd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc865f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09c1f5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7ef99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e814b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827799a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c090ec4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b5d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fbb61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89200411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee5880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d79f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2babaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f71367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a59fc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68222940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c96e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b4894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f51a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cd2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b678a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf7afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e922a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7ad8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
