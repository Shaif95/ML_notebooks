{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de27a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet50,\n",
    "    EfficientNetB0,\n",
    "    resnet50,\n",
    "    efficientnet\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Load the CIFAR-100 Dataset\n",
    "# ---------------------------------------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "nums = 10\n",
    "\n",
    "# Convert to float32\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "# Convert labels to one-hot\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ee5ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8492c02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning ResNet50 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 145s 91ms/step - loss: 0.5195 - accuracy: 0.8395 - val_loss: 0.3193 - val_accuracy: 0.8968\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 138s 88ms/step - loss: 0.2533 - accuracy: 0.9167 - val_loss: 0.3002 - val_accuracy: 0.9060\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 147s 94ms/step - loss: 0.1762 - accuracy: 0.9421 - val_loss: 0.3288 - val_accuracy: 0.9067\n",
      "ResNet50 - CIFAR-100 Accuracy: 0.9067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll define two separate pipelines:\n",
    "#   - One for ResNet50\n",
    "#   - One for EfficientNetB0\n",
    "\n",
    "# ------------------------------\n",
    "# 2. tf.data Pipeline for ResNet\n",
    "# ------------------------------\n",
    "batch_size = 32\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    # Resize and apply ResNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = resnet50.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "# Create training dataset\n",
    "train_ds_resnet = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_resnet = train_ds_resnet.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "# Create validation (test) dataset\n",
    "val_ds_resnet = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_resnet = val_ds_resnet.map(preprocess_resnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Define Baseline Models to Fine-Tune\n",
    "# ------------------------------------------\n",
    "def create_resnet50_finetune(input_shape, num_classes):\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 5. Fine-Tune ResNet50 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "resnet_model = create_resnet50_finetune((224, 224, 3), num_classes)\n",
    "resnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning ResNet50 on CIFAR-100 ---\")\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_ds_resnet,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds_resnet,\n",
    "    verbose=1\n",
    ")\n",
    "resnet_loss, resnet_acc = resnet_model.evaluate(val_ds_resnet, verbose=0)\n",
    "print(f\"ResNet50 - CIFAR-100 Accuracy: {resnet_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6804e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb5f7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 109s 67ms/step - loss: 0.5143 - accuracy: 0.8315 - val_loss: 0.3506 - val_accuracy: 0.8838\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 100s 64ms/step - loss: 0.2873 - accuracy: 0.9016 - val_loss: 0.3390 - val_accuracy: 0.8914\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 96s 62ms/step - loss: 0.2076 - accuracy: 0.9289 - val_loss: 0.3440 - val_accuracy: 0.8962\n",
      "EfficientNetB0 - CIFAR-100 Accuracy: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 3. tf.data Pipeline for EfficientNet\n",
    "# ------------------------------\n",
    "def preprocess_efficientnet(image, label):\n",
    "    # Resize and apply EfficientNet-specific preprocessing\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = efficientnet.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "train_ds_eff = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds_eff = train_ds_eff.shuffle(buffer_size=50000) \\\n",
    "    .map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_eff = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_ds_eff = val_ds_eff.map(preprocess_efficientnet, num_parallel_calls=AUTOTUNE) \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "def create_efficientnet_finetune(input_shape, num_classes):\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = layers.Flatten()(base_model.output)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 6. Fine-Tune EfficientNetB0 on CIFAR-100\n",
    "# ----------------------------------------------\n",
    "eff_model = create_efficientnet_finetune((224, 224, 3), num_classes)\n",
    "eff_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning EfficientNetB0 on CIFAR-100 ---\")\n",
    "history_eff = eff_model.fit(\n",
    "    train_ds_eff,\n",
    "    epochs=3,\n",
    "    validation_data=val_ds_eff,\n",
    "    verbose=1\n",
    ")\n",
    "eff_loss, eff_acc = eff_model.evaluate(val_ds_eff, verbose=0)\n",
    "print(f\"EfficientNetB0 - CIFAR-100 Accuracy: {eff_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fbd444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f129143",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "################\n",
    "#   GATING     #\n",
    "################\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eda9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd89cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a4b87a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning Dynamic-Gating EfficientNetB0 on CIFAR-100 ---\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 338s 211ms/step - loss: 0.4076 - accuracy: 0.8723 - val_loss: 0.1384 - val_accuracy: 0.9531\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 327s 209ms/step - loss: 0.1431 - accuracy: 0.9530 - val_loss: 0.1142 - val_accuracy: 0.9613\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 321s 205ms/step - loss: 0.0881 - accuracy: 0.9708 - val_loss: 0.1121 - val_accuracy: 0.9648\n",
      "Final Accuracy with Dynamic Gating: 0.9648\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DynamicChannelGate(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A learnable gating mechanism to dynamically prune (or re-expand) channels.\n",
    "    Each channel has a gating parameter in [0, 1], learned via a sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        # We'll store one gating param per channel\n",
    "        # Initialize them around 1.0 to start with minimal pruning.\n",
    "        self.gate_params = tf.Variable(\n",
    "            initial_value=tf.ones((num_channels,), dtype=tf.float32),\n",
    "            trainable=True,\n",
    "            name=\"gate_params\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Inputs shape: (batch, H, W, C)\n",
    "        gate_params shape: (C,)\n",
    "        Returns: inputs * gate, shape (batch, H, W, C)\n",
    "        \"\"\"\n",
    "        # Sigmoid ensures gating stays between 0 and 1\n",
    "        gate = tf.sigmoid(self.gate_params)\n",
    "        # Reshape to broadcast across (batch, H, W, C)\n",
    "        gate = tf.reshape(gate, (1, 1, 1, -1))\n",
    "        return inputs * gate\n",
    "\n",
    "    \n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "def create_efficientnet_dynamic_gating(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Loads EfficientNetB0 with ImageNet weights, then appends a custom\n",
    "    gating layer + classification head for dynamic channel pruning/re-expansion.\n",
    "    \"\"\"\n",
    "    # 1. Load base EfficientNet (feature extractor)\n",
    "    #    include_top=False => no final classification layers\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Optionally unfreeze part or all of the base model for fine-tuning\n",
    "    # For demonstration, we'll unfreeze everything.\n",
    "    # (You could freeze some initial layers if you wish.)\n",
    "    base_model.trainable = True\n",
    "\n",
    "    # 2. Get the final feature map\n",
    "    x = base_model.output  # shape: (batch, 7, 7, channels) for EFN-B0\n",
    "\n",
    "    # 3. Insert the DynamicChannelGate\n",
    "    #    We'll figure out how many channels the base model outputs:\n",
    "    num_channels = x.shape[-1]\n",
    "    gating_layer = DynamicChannelGate(num_channels=num_channels, name=\"dynamic_gating\")\n",
    "    x = gating_layer(x)\n",
    "\n",
    "    # 4. Classification Head\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)  # you can tune dropout\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # 5. Wrap into a Model\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs, name=\"EFN_DynamicGating\")\n",
    "    return model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Create the dynamic-gating EfficientNet\n",
    "model = create_efficientnet_dynamic_gating((224, 224, 3), num_classes=nums)\n",
    "\n",
    "# Compile the model (tweak hyperparameters as needed)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Dynamic-Gating EfficientNetB0 on CIFAR-100 ---\")\n",
    "history = model.fit(\n",
    "    train_ds_eff,             # from your existing pipeline\n",
    "    epochs=3,                 # increase for better results\n",
    "    validation_data=val_ds_eff,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(val_ds_eff, verbose=0)\n",
    "print(f\"Final Accuracy with Dynamic Gating: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7afceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e33faee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuning Dynamic-Gating ResNet50 on CIFAR-10 ---\n",
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 279s 174ms/step - loss: 0.3492 - accuracy: 0.8820 - val_loss: 0.2802 - val_accuracy: 0.9078\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 273s 174ms/step - loss: 0.1462 - accuracy: 0.9512 - val_loss: 0.2325 - val_accuracy: 0.9230\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 271s 173ms/step - loss: 0.0976 - accuracy: 0.9669 - val_loss: 0.2217 - val_accuracy: 0.9304\n",
      "\n",
      "Final Accuracy with Dynamic Gating: 0.9304\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50, resnet50\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "###############################################################################\n",
    "# 1. Data: CIFAR-10 loading & preprocessing for ResNet50\n",
    "###############################################################################\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test,  num_classes)\n",
    "\n",
    "def preprocess_resnet(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    # Use ResNet50's preprocessing\n",
    "    image = resnet50.preprocess_input(image)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(50000)\n",
    "    .batch(batch_size)\n",
    "    .map(preprocess_resnet, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "    .batch(batch_size)\n",
    "    .map(preprocess_resnet, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 2. DynamicChannelGate Layer\n",
    "###############################################################################\n",
    "class DynamicChannelGate(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A learnable gating mechanism to dynamically prune (or re-expand) channels.\n",
    "    Each channel has a gating parameter in [0, 1], learned via a sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.gate_params = tf.Variable(\n",
    "            initial_value=tf.ones((num_channels,), dtype=tf.float32),\n",
    "            trainable=True,\n",
    "            name=\"gate_params\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        gate = tf.sigmoid(self.gate_params)          # shape (C,)\n",
    "        gate = tf.reshape(gate, (1, 1, 1, -1))       # broadcast to (1, 1, 1, C)\n",
    "        return inputs * gate\n",
    "\n",
    "###############################################################################\n",
    "# 3. Create a ResNet50 model with Dynamic Gating\n",
    "###############################################################################\n",
    "def create_resnet_dynamic_gating(input_shape, num_classes):\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    # Optionally unfreeze the base for deeper fine-tuning\n",
    "    base_model.trainable = True\n",
    "\n",
    "    x = base_model.output\n",
    "    num_channels = x.shape[-1]  # Typically 2048 for ResNet50\n",
    "    gating_layer = DynamicChannelGate(num_channels=num_channels, name=\"dynamic_gating\")\n",
    "    x = gating_layer(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model = models.Model(inputs=base_model.input, outputs=outputs, name=\"ResNet_DynamicGating\")\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# 4. Build, Train, and Evaluate the Model\n",
    "###############################################################################\n",
    "model = create_resnet_dynamic_gating((224, 224, 3), num_classes)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fine-Tuning Dynamic-Gating ResNet50 on CIFAR-10 ---\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=3,\n",
    "    validation_data=test_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(test_ds, verbose=0)\n",
    "print(f\"\\nFinal Accuracy with Dynamic Gating: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3bd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bbba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdcdd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157799d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31ac0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344c0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c8c290a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee374bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bbc85e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3c07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa5a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52fdbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722b63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205a3d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
