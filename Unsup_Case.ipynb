{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c816a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [10:01<00:00, 31.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2872, 32, 32, 3)\n",
      "Y_train shape: (2872,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"G:\\datasets\\Underwater_Image\\WHOI\\archive\\dataset_pm\\training\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_train = []\n",
    "Y_train = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((32, 32))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_train.append(img_array)\n",
    "            Y_train.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf13620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [01:30<00:00,  9.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (29009, 32, 32, 3)\n",
      "Y_train shape: (29009,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the path to the directory\n",
    "directory_path = r\"E:\\imbcifar\\train\"\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "X_train = []\n",
    "Y_train = []\n",
    "class_label = 0\n",
    "\n",
    "# Loop through subdirectories (classes)\n",
    "for class_folder in tqdm(sorted(os.listdir(directory_path))):\n",
    "    class_path = os.path.join(directory_path, class_folder)\n",
    "    \n",
    "    for image_file in os.listdir(class_path):\n",
    "        if image_file.endswith('.png'):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Load image, convert to RGB and resize\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img = img.resize((32, 32))\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            # Append image and label to lists\n",
    "            X_train.append(img_array)\n",
    "            Y_train.append(class_label)\n",
    "            \n",
    "    class_label = class_label + 1\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'Y_train shape: {Y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b609b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "602c030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing DataFrames: 100%|████████████████████████████████████████████████████████████| 4/4 [17:43<00:00, 265.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_paths = [\n",
    "    \"G:/image_features.csv\",\n",
    "    \"G:/image_features1.csv\",\n",
    "    \"G:/image_features2.csv\",\n",
    "    \"G:/image_features3.csv\"\n",
    "]\n",
    "\n",
    "# Load the CSV files into Dask DataFrames\n",
    "dfs = [dd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Function to get 25,000 random indexes and retrieve the full rows\n",
    "def get_random_rows(df, n=25000):\n",
    "    # Convert the Dask DataFrame to a Pandas DataFrame\n",
    "    pandas_df = df.compute()\n",
    "    \n",
    "    # Get the total number of rows in the DataFrame\n",
    "    total_rows = len(pandas_df)\n",
    "    \n",
    "    # Generate 25,000 random indexes\n",
    "    random_indexes = np.random.choice(total_rows, n, replace=False)\n",
    "    \n",
    "    # Retrieve the full rows for these random indexes\n",
    "    random_rows = pandas_df.iloc[random_indexes].values.tolist()\n",
    "    \n",
    "    return random_rows\n",
    "\n",
    "# Initialize an empty list to store all rows\n",
    "all_random_rows = []\n",
    "\n",
    "# Get the rows from each DataFrame\n",
    "for df in tqdm(dfs, desc=\"Processing DataFrames\"):\n",
    "    rows = get_random_rows(df)\n",
    "    all_random_rows.extend(rows)\n",
    "\n",
    "# Print the total number of rows collected\n",
    "print(len(all_random_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cfa4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59381467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the first two items from each element in all_random_rows\n",
    "new_all_random_rows = [row[2:] for row in all_random_rows]\n",
    "np.shape(new_all_random_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9caf10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the cluster centers: (1000, 2048)\n",
      "Cluster centers have been saved to 'cluster_centers.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "# Assuming new_all_random_rows has been populated as described\n",
    "\n",
    "# Convert new_all_random_rows to a NumPy array\n",
    "data = np.array(new_all_random_rows)\n",
    "\n",
    "batch_size = 10000\n",
    "max_iter = 100\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = MiniBatchKMeans(n_clusters=1000, batch_size=batch_size, max_iter=max_iter, random_state=42)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Store the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Print the shape of the cluster centers\n",
    "print(f\"Shape of the cluster centers: {centers.shape}\")\n",
    "\n",
    "# Save the cluster centers to a pickle file\n",
    "with open('cluster_centers.pkl', 'wb') as file:\n",
    "    pickle.dump(centers, file)\n",
    "\n",
    "print(\"Cluster centers have been saved to 'cluster_centers.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cd015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bba5731",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cifar10\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\__init__.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# from tensorflow.python import keras\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column_lib \u001b[38;5;28;01mas\u001b[39;00m feature_column\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# from tensorflow.python.layers import layers\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"FeatureColumns: tools for ingesting and representing features.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_column\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence_feature_column\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:143\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m sparse_tensor_lib\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_ops\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy_tf_layers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[0;32m     18\u001b[0m InputSpec \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mInputSpec\n\u001b[0;32m     20\u001b[0m keras_style_scope \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mkeras_style_scope\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\keras\\distribute\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' Distribution Strategy library.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sidecar_evaluator\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\keras\\distribute\\sidecar_evaluator.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_utils\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util \u001b[38;5;28;01mas\u001b[39;00m tracking_util\n\u001b[0;32m     26\u001b[0m _PRINT_EVAL_STEP_EVERY_SEC \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60.0\u001b[39m\n\u001b[0;32m     27\u001b[0m _ITERATIONS_UNINITIALIZED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py:44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils_impl\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_saved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_management\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\saved_model\\utils_impl.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_io\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nested_structure_coder\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecation\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\saved_model\\nested_structure_coder.py:39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iterator_ops\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optional_ops\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extension_type\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\distribute\\values.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values_util\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tens\\lib\\site-packages\\tensorflow\\python\\distribute\\values_util.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variable_scope \u001b[38;5;28;01mas\u001b[39;00m vs\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_context\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_options\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saveable_object\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_object_proto\u001b[39m(var, proto, options):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the pixel values between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 100)\n",
    "y_test = keras.utils.to_categorical(y_test, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92419d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train\n",
    "y_train = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306f205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10d8d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\tens\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "import glob\n",
    "\n",
    "target_size = (32, 32)  # Change the values as per your requirement\n",
    "# Load the pre-trained ResNet50 model with modified input shape\n",
    "model = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "ft = model.predict(np.array(x_train).astype(\"float32\"))\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "n_clusters = 400\n",
    "batch_size = 100\n",
    "max_iter = 100\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=batch_size, max_iter=max_iter)\n",
    "kmeans.fit(ft)\n",
    "# Retrieve the cluster centers\n",
    "ct = kmeans.cluster_centers_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9d0753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4179a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [00:52<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the distance array: (400, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a list to store the distances\n",
    "tot_dist = []\n",
    "\n",
    "# Calculate L2 distances\n",
    "for i in tqdm(range(len(ct))):\n",
    "    distances = []\n",
    "    for j in range(len(centers)):\n",
    "        distance = np.linalg.norm(ct[i] - centers[j])\n",
    "        distances.append(distance)\n",
    "    tot_dist.append(distances)\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "tot_dist = np.array(tot_dist)\n",
    "\n",
    "# Print the shape of the resulting array\n",
    "print(f\"Shape of the distance array: {tot_dist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d3ca2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01589f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:45<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost = 32801.97330471577\n",
      "\n",
      "Cluster 40 assigned to Class 1. Cost: 94.44012756332727\n",
      "Cluster 66 assigned to Class 2. Cost: 65.65385500358805\n",
      "Cluster 260 assigned to Class 4. Cost: 107.91698041256726\n",
      "Cluster 323 assigned to Class 7. Cost: 79.73425665974744\n",
      "Cluster 123 assigned to Class 9. Cost: 80.77498255823996\n",
      "Cluster 180 assigned to Class 11. Cost: 107.51334376793154\n",
      "Cluster 49 assigned to Class 12. Cost: 93.1283273848305\n",
      "Cluster 373 assigned to Class 20. Cost: 67.251754973473\n",
      "Cluster 134 assigned to Class 24. Cost: 104.13333045235372\n",
      "Cluster 326 assigned to Class 26. Cost: 75.45176169095468\n",
      "Cluster 283 assigned to Class 27. Cost: 81.23256333938774\n",
      "Cluster 122 assigned to Class 28. Cost: 79.4000587102142\n",
      "Cluster 33 assigned to Class 29. Cost: 67.46971997570354\n",
      "Cluster 51 assigned to Class 32. Cost: 79.6654916162415\n",
      "Cluster 129 assigned to Class 34. Cost: 69.85664356967173\n",
      "Cluster 310 assigned to Class 35. Cost: 101.75434388245162\n",
      "Cluster 302 assigned to Class 36. Cost: 49.67731523205649\n",
      "Cluster 344 assigned to Class 37. Cost: 80.83301160795406\n",
      "Cluster 26 assigned to Class 39. Cost: 106.77134142971511\n",
      "Cluster 21 assigned to Class 40. Cost: 64.82322660240546\n",
      "Cluster 315 assigned to Class 45. Cost: 52.795927391658076\n",
      "Cluster 235 assigned to Class 51. Cost: 68.95401978589969\n",
      "Cluster 360 assigned to Class 52. Cost: 74.79678584262257\n",
      "Cluster 307 assigned to Class 53. Cost: 97.46105047942319\n",
      "Cluster 340 assigned to Class 55. Cost: 81.95757835040176\n",
      "Cluster 262 assigned to Class 57. Cost: 77.2772535689492\n",
      "Cluster 221 assigned to Class 58. Cost: 70.80281840605548\n",
      "Cluster 19 assigned to Class 62. Cost: 92.38176191185151\n",
      "Cluster 52 assigned to Class 63. Cost: 76.08914154511453\n",
      "Cluster 219 assigned to Class 66. Cost: 95.17939783567819\n",
      "Cluster 247 assigned to Class 67. Cost: 94.32891450652826\n",
      "Cluster 319 assigned to Class 68. Cost: 67.05355647027844\n",
      "Cluster 285 assigned to Class 69. Cost: 46.23228610302594\n",
      "Cluster 343 assigned to Class 70. Cost: 54.01928898177727\n",
      "Cluster 282 assigned to Class 74. Cost: 94.83015432108347\n",
      "Cluster 291 assigned to Class 75. Cost: 82.08942368346996\n",
      "Cluster 27 assigned to Class 80. Cost: 93.14866123937563\n",
      "Cluster 125 assigned to Class 81. Cost: 98.06316358162611\n",
      "Cluster 159 assigned to Class 84. Cost: 99.5047779167525\n",
      "Cluster 158 assigned to Class 87. Cost: 110.51178835305969\n",
      "Cluster 53 assigned to Class 89. Cost: 76.60893647381427\n",
      "Cluster 316 assigned to Class 91. Cost: 90.33901608751404\n",
      "Cluster 124 assigned to Class 96. Cost: 60.63451957434782\n",
      "Cluster 234 assigned to Class 98. Cost: 90.38525253537252\n",
      "Cluster 132 assigned to Class 99. Cost: 105.21623495519985\n",
      "Cluster 183 assigned to Class 100. Cost: 95.3780027841764\n",
      "Cluster 296 assigned to Class 101. Cost: 85.91308717293224\n",
      "Cluster 84 assigned to Class 102. Cost: 134.31028832331376\n",
      "Cluster 217 assigned to Class 104. Cost: 56.58430439750528\n",
      "Cluster 380 assigned to Class 106. Cost: 99.57817295264758\n",
      "Cluster 115 assigned to Class 107. Cost: 66.86090001403588\n",
      "Cluster 68 assigned to Class 108. Cost: 67.43375814626432\n",
      "Cluster 274 assigned to Class 109. Cost: 74.23978729457248\n",
      "Cluster 150 assigned to Class 113. Cost: 64.97936549662009\n",
      "Cluster 126 assigned to Class 123. Cost: 55.11140095218497\n",
      "Cluster 215 assigned to Class 124. Cost: 82.8140677681424\n",
      "Cluster 4 assigned to Class 125. Cost: 64.62001998266587\n",
      "Cluster 381 assigned to Class 131. Cost: 91.65050691046766\n",
      "Cluster 181 assigned to Class 132. Cost: 87.18963108185977\n",
      "Cluster 195 assigned to Class 133. Cost: 52.67640034898489\n",
      "Cluster 169 assigned to Class 134. Cost: 82.47925779118296\n",
      "Cluster 113 assigned to Class 136. Cost: 103.70409653936007\n",
      "Cluster 155 assigned to Class 137. Cost: 100.56912984334211\n",
      "Cluster 320 assigned to Class 140. Cost: 75.51740524776643\n",
      "Cluster 393 assigned to Class 143. Cost: 48.85243244517915\n",
      "Cluster 265 assigned to Class 150. Cost: 48.37730576419251\n",
      "Cluster 361 assigned to Class 152. Cost: 61.81735071267327\n",
      "Cluster 90 assigned to Class 153. Cost: 109.85058068036973\n",
      "Cluster 214 assigned to Class 154. Cost: 73.72239289154103\n",
      "Cluster 114 assigned to Class 157. Cost: 72.5654884445681\n",
      "Cluster 379 assigned to Class 158. Cost: 66.46273778797361\n",
      "Cluster 258 assigned to Class 161. Cost: 44.758850694380214\n",
      "Cluster 352 assigned to Class 166. Cost: 77.81009005263418\n",
      "Cluster 105 assigned to Class 167. Cost: 66.74910758674804\n",
      "Cluster 232 assigned to Class 168. Cost: 64.98348687124815\n",
      "Cluster 333 assigned to Class 181. Cost: 98.57833437344709\n",
      "Cluster 116 assigned to Class 182. Cost: 53.48735391211225\n",
      "Cluster 252 assigned to Class 187. Cost: 93.42026212575297\n",
      "Cluster 175 assigned to Class 190. Cost: 88.20553244697025\n",
      "Cluster 7 assigned to Class 192. Cost: 82.285740891293\n",
      "Cluster 233 assigned to Class 193. Cost: 82.00696497924775\n",
      "Cluster 255 assigned to Class 195. Cost: 106.79453641833621\n",
      "Cluster 305 assigned to Class 207. Cost: 70.399105326752\n",
      "Cluster 349 assigned to Class 209. Cost: 83.50181638053357\n",
      "Cluster 368 assigned to Class 216. Cost: 82.87347857444526\n",
      "Cluster 311 assigned to Class 218. Cost: 83.70154843735389\n",
      "Cluster 145 assigned to Class 219. Cost: 71.53402209913094\n",
      "Cluster 275 assigned to Class 220. Cost: 71.80613059913559\n",
      "Cluster 70 assigned to Class 221. Cost: 57.45347565811042\n",
      "Cluster 337 assigned to Class 222. Cost: 53.15586261179435\n",
      "Cluster 374 assigned to Class 223. Cost: 90.82203549262798\n",
      "Cluster 174 assigned to Class 226. Cost: 55.978794856119414\n",
      "Cluster 294 assigned to Class 227. Cost: 50.76902444747907\n",
      "Cluster 43 assigned to Class 228. Cost: 70.62406348010983\n",
      "Cluster 75 assigned to Class 229. Cost: 79.17642736143239\n",
      "Cluster 369 assigned to Class 231. Cost: 114.0795484687486\n",
      "Cluster 142 assigned to Class 232. Cost: 68.45866850451874\n",
      "Cluster 348 assigned to Class 234. Cost: 98.11195199024768\n",
      "Cluster 86 assigned to Class 238. Cost: 81.789071457253\n",
      "Cluster 342 assigned to Class 242. Cost: 83.53006383496057\n",
      "Cluster 0 assigned to Class 245. Cost: 85.06481357248887\n",
      "Cluster 59 assigned to Class 251. Cost: 110.42413852666986\n",
      "Cluster 261 assigned to Class 252. Cost: 80.68756973204104\n",
      "Cluster 339 assigned to Class 257. Cost: 97.01077386653553\n",
      "Cluster 24 assigned to Class 258. Cost: 134.93715217612086\n",
      "Cluster 225 assigned to Class 264. Cost: 78.21386785611607\n",
      "Cluster 390 assigned to Class 265. Cost: 78.54130729411021\n",
      "Cluster 82 assigned to Class 268. Cost: 61.235166296319555\n",
      "Cluster 263 assigned to Class 270. Cost: 88.7328696152783\n",
      "Cluster 240 assigned to Class 272. Cost: 80.94687889417106\n",
      "Cluster 13 assigned to Class 273. Cost: 88.18852480653418\n",
      "Cluster 322 assigned to Class 274. Cost: 84.45279372612552\n",
      "Cluster 15 assigned to Class 281. Cost: 52.29078530290412\n",
      "Cluster 290 assigned to Class 284. Cost: 67.12720300898548\n",
      "Cluster 56 assigned to Class 285. Cost: 119.89654351800652\n",
      "Cluster 313 assigned to Class 286. Cost: 101.14302150225576\n",
      "Cluster 131 assigned to Class 289. Cost: 112.3565620213118\n",
      "Cluster 201 assigned to Class 290. Cost: 95.40829667295361\n",
      "Cluster 100 assigned to Class 292. Cost: 82.03723204813473\n",
      "Cluster 162 assigned to Class 297. Cost: 58.382889199134766\n",
      "Cluster 85 assigned to Class 299. Cost: 64.45408258344567\n",
      "Cluster 167 assigned to Class 300. Cost: 97.82258053747425\n",
      "Cluster 218 assigned to Class 301. Cost: 43.346936202688696\n",
      "Cluster 146 assigned to Class 304. Cost: 101.81249560330878\n",
      "Cluster 121 assigned to Class 311. Cost: 129.52226015141835\n",
      "Cluster 193 assigned to Class 315. Cost: 75.93228971215977\n",
      "Cluster 273 assigned to Class 316. Cost: 74.34390357686382\n",
      "Cluster 94 assigned to Class 318. Cost: 48.65913418121099\n",
      "Cluster 42 assigned to Class 319. Cost: 63.517168193623846\n",
      "Cluster 140 assigned to Class 326. Cost: 61.98356214499962\n",
      "Cluster 297 assigned to Class 327. Cost: 108.62788334809848\n",
      "Cluster 210 assigned to Class 328. Cost: 66.95814556124823\n",
      "Cluster 321 assigned to Class 333. Cost: 71.24204833542622\n",
      "Cluster 241 assigned to Class 336. Cost: 75.90148635133221\n",
      "Cluster 200 assigned to Class 337. Cost: 68.13343011595148\n",
      "Cluster 47 assigned to Class 338. Cost: 100.58708674541852\n",
      "Cluster 301 assigned to Class 340. Cost: 76.59262454831206\n",
      "Cluster 372 assigned to Class 341. Cost: 102.06093231446616\n",
      "Cluster 345 assigned to Class 344. Cost: 76.22588285165101\n",
      "Cluster 1 assigned to Class 347. Cost: 65.88907214586047\n",
      "Cluster 204 assigned to Class 350. Cost: 82.0784690120768\n",
      "Cluster 80 assigned to Class 351. Cost: 72.72629111521441\n",
      "Cluster 228 assigned to Class 353. Cost: 74.53496595946413\n",
      "Cluster 95 assigned to Class 355. Cost: 78.73744385242013\n",
      "Cluster 48 assigned to Class 358. Cost: 81.32203066460912\n",
      "Cluster 160 assigned to Class 360. Cost: 96.92511543997978\n",
      "Cluster 203 assigned to Class 364. Cost: 54.555466047154624\n",
      "Cluster 356 assigned to Class 365. Cost: 83.93770776567389\n",
      "Cluster 153 assigned to Class 366. Cost: 95.74092609536308\n",
      "Cluster 397 assigned to Class 367. Cost: 67.7338817158819\n",
      "Cluster 191 assigned to Class 371. Cost: 109.9873485179472\n",
      "Cluster 243 assigned to Class 372. Cost: 76.37245133675053\n",
      "Cluster 166 assigned to Class 373. Cost: 73.19518393660577\n",
      "Cluster 96 assigned to Class 374. Cost: 82.81226813885047\n",
      "Cluster 88 assigned to Class 375. Cost: 45.188671422697475\n",
      "Cluster 208 assigned to Class 381. Cost: 90.06585989151922\n",
      "Cluster 289 assigned to Class 382. Cost: 81.68653585343674\n",
      "Cluster 144 assigned to Class 383. Cost: 90.4338547561407\n",
      "Cluster 60 assigned to Class 387. Cost: 88.64894506198777\n",
      "Cluster 173 assigned to Class 389. Cost: 49.20330103134214\n",
      "Cluster 57 assigned to Class 391. Cost: 83.59538616437455\n",
      "Cluster 188 assigned to Class 395. Cost: 63.3220489407177\n",
      "Cluster 358 assigned to Class 396. Cost: 79.57368901283209\n",
      "Cluster 309 assigned to Class 397. Cost: 83.15265915820682\n",
      "Cluster 147 assigned to Class 398. Cost: 82.61353114506419\n",
      "Cluster 367 assigned to Class 400. Cost: 119.25610236945518\n",
      "Cluster 242 assigned to Class 401. Cost: 75.86990603971705\n",
      "Cluster 198 assigned to Class 402. Cost: 99.45678296843575\n",
      "Cluster 5 assigned to Class 405. Cost: 77.22494912756164\n",
      "Cluster 328 assigned to Class 407. Cost: 82.64306632344342\n",
      "Cluster 108 assigned to Class 408. Cost: 124.90386127135916\n",
      "Cluster 119 assigned to Class 409. Cost: 99.57001295647412\n",
      "Cluster 365 assigned to Class 410. Cost: 108.50555099352576\n",
      "Cluster 388 assigned to Class 411. Cost: 106.24215512795907\n",
      "Cluster 392 assigned to Class 414. Cost: 109.70396672030533\n",
      "Cluster 213 assigned to Class 416. Cost: 53.99564577581425\n",
      "Cluster 382 assigned to Class 420. Cost: 98.18885070394117\n",
      "Cluster 74 assigned to Class 431. Cost: 75.93475169919996\n",
      "Cluster 331 assigned to Class 437. Cost: 90.86946851920607\n",
      "Cluster 303 assigned to Class 444. Cost: 115.43357462137992\n",
      "Cluster 184 assigned to Class 445. Cost: 74.53168231989812\n",
      "Cluster 78 assigned to Class 446. Cost: 75.61074052797089\n",
      "Cluster 308 assigned to Class 448. Cost: 78.5847163722593\n",
      "Cluster 292 assigned to Class 451. Cost: 59.83301659120675\n",
      "Cluster 97 assigned to Class 460. Cost: 119.24298866329572\n",
      "Cluster 76 assigned to Class 462. Cost: 75.02311857920799\n",
      "Cluster 268 assigned to Class 463. Cost: 73.99818184429023\n",
      "Cluster 50 assigned to Class 464. Cost: 68.73089963868247\n",
      "Cluster 128 assigned to Class 470. Cost: 51.12936251899554\n",
      "Cluster 139 assigned to Class 473. Cost: 102.80092562853481\n",
      "Cluster 196 assigned to Class 476. Cost: 78.846659288785\n",
      "Cluster 220 assigned to Class 480. Cost: 71.39735262224092\n",
      "Cluster 152 assigned to Class 491. Cost: 81.77753397645805\n",
      "Cluster 81 assigned to Class 492. Cost: 54.8905062212613\n",
      "Cluster 277 assigned to Class 493. Cost: 68.03913297499282\n",
      "Cluster 304 assigned to Class 499. Cost: 84.40654447063477\n",
      "Cluster 178 assigned to Class 500. Cost: 103.6490690859168\n",
      "Cluster 293 assigned to Class 503. Cost: 78.8388808295817\n",
      "Cluster 250 assigned to Class 504. Cost: 56.8410790714706\n",
      "Cluster 171 assigned to Class 516. Cost: 97.87188180346166\n",
      "Cluster 25 assigned to Class 517. Cost: 113.21447324540445\n",
      "Cluster 194 assigned to Class 525. Cost: 100.99935205974025\n",
      "Cluster 8 assigned to Class 530. Cost: 44.605170483994364\n",
      "Cluster 109 assigned to Class 538. Cost: 97.7578805651931\n",
      "Cluster 385 assigned to Class 542. Cost: 80.78391045251263\n",
      "Cluster 281 assigned to Class 545. Cost: 75.88560272610657\n",
      "Cluster 176 assigned to Class 547. Cost: 92.02723468622143\n",
      "Cluster 206 assigned to Class 548. Cost: 67.32559565468493\n",
      "Cluster 386 assigned to Class 549. Cost: 112.65082092433735\n",
      "Cluster 39 assigned to Class 552. Cost: 99.66818097391794\n",
      "Cluster 72 assigned to Class 554. Cost: 121.18572862862982\n",
      "Cluster 61 assigned to Class 555. Cost: 95.04324873365626\n",
      "Cluster 120 assigned to Class 557. Cost: 88.32962743106806\n",
      "Cluster 58 assigned to Class 559. Cost: 44.272131292417434\n",
      "Cluster 151 assigned to Class 562. Cost: 103.3054298972582\n",
      "Cluster 92 assigned to Class 565. Cost: 66.60517732020352\n",
      "Cluster 133 assigned to Class 566. Cost: 64.74437581126422\n",
      "Cluster 237 assigned to Class 568. Cost: 94.15445313828523\n",
      "Cluster 222 assigned to Class 569. Cost: 78.80712318645465\n",
      "Cluster 272 assigned to Class 570. Cost: 74.81699964678612\n",
      "Cluster 330 assigned to Class 571. Cost: 81.28767533985159\n",
      "Cluster 154 assigned to Class 573. Cost: 95.01093222612882\n",
      "Cluster 79 assigned to Class 574. Cost: 77.783187646732\n",
      "Cluster 77 assigned to Class 575. Cost: 97.50210263819012\n",
      "Cluster 199 assigned to Class 576. Cost: 50.611318539857784\n",
      "Cluster 36 assigned to Class 582. Cost: 95.92071361960024\n",
      "Cluster 341 assigned to Class 583. Cost: 89.57336427810925\n",
      "Cluster 28 assigned to Class 584. Cost: 78.4420297882149\n",
      "Cluster 389 assigned to Class 585. Cost: 75.34243959740837\n",
      "Cluster 338 assigned to Class 588. Cost: 76.83109222679789\n",
      "Cluster 127 assigned to Class 590. Cost: 86.26840344011248\n",
      "Cluster 244 assigned to Class 592. Cost: 100.42564086514855\n",
      "Cluster 101 assigned to Class 593. Cost: 62.621936322518174\n",
      "Cluster 65 assigned to Class 594. Cost: 79.82372113335401\n",
      "Cluster 143 assigned to Class 596. Cost: 63.462002452462364\n",
      "Cluster 359 assigned to Class 597. Cost: 118.80753687374776\n",
      "Cluster 110 assigned to Class 598. Cost: 42.32306470866709\n",
      "Cluster 329 assigned to Class 599. Cost: 67.78053149096057\n",
      "Cluster 14 assigned to Class 600. Cost: 75.06137971745062\n",
      "Cluster 254 assigned to Class 601. Cost: 98.26314744065976\n",
      "Cluster 187 assigned to Class 607. Cost: 69.64885794661336\n",
      "Cluster 347 assigned to Class 608. Cost: 88.91021232402205\n",
      "Cluster 229 assigned to Class 614. Cost: 61.07495454175797\n",
      "Cluster 256 assigned to Class 615. Cost: 66.05522105321194\n",
      "Cluster 46 assigned to Class 616. Cost: 65.04552822374504\n",
      "Cluster 230 assigned to Class 619. Cost: 89.57225696920624\n",
      "Cluster 364 assigned to Class 621. Cost: 84.17536516647276\n",
      "Cluster 103 assigned to Class 622. Cost: 86.2891887183654\n",
      "Cluster 172 assigned to Class 623. Cost: 87.82221605979467\n",
      "Cluster 205 assigned to Class 624. Cost: 84.71849609929818\n",
      "Cluster 264 assigned to Class 627. Cost: 62.86093845483644\n",
      "Cluster 376 assigned to Class 633. Cost: 111.25509925938758\n",
      "Cluster 177 assigned to Class 634. Cost: 67.63689541183876\n",
      "Cluster 370 assigned to Class 639. Cost: 72.72202943512683\n",
      "Cluster 398 assigned to Class 640. Cost: 125.56512713752716\n",
      "Cluster 387 assigned to Class 642. Cost: 89.79676749785223\n",
      "Cluster 99 assigned to Class 646. Cost: 109.88517721329964\n",
      "Cluster 16 assigned to Class 650. Cost: 88.71134340196203\n",
      "Cluster 83 assigned to Class 651. Cost: 64.70901467994359\n",
      "Cluster 157 assigned to Class 654. Cost: 56.6229420743968\n",
      "Cluster 336 assigned to Class 656. Cost: 100.1964956608319\n",
      "Cluster 87 assigned to Class 658. Cost: 109.45524780699769\n",
      "Cluster 249 assigned to Class 662. Cost: 97.97737996660571\n",
      "Cluster 366 assigned to Class 663. Cost: 69.31121736169389\n",
      "Cluster 54 assigned to Class 665. Cost: 114.94617585766836\n",
      "Cluster 93 assigned to Class 668. Cost: 87.95959636084945\n",
      "Cluster 130 assigned to Class 670. Cost: 94.15032811782066\n",
      "Cluster 164 assigned to Class 672. Cost: 53.31997675482118\n",
      "Cluster 104 assigned to Class 673. Cost: 63.31486479986156\n",
      "Cluster 298 assigned to Class 674. Cost: 65.18467230295387\n",
      "Cluster 325 assigned to Class 679. Cost: 65.42157771236262\n",
      "Cluster 212 assigned to Class 680. Cost: 84.2560369073635\n",
      "Cluster 231 assigned to Class 682. Cost: 87.60281314165964\n",
      "Cluster 136 assigned to Class 688. Cost: 64.58748851925817\n",
      "Cluster 257 assigned to Class 689. Cost: 117.48749125700823\n",
      "Cluster 295 assigned to Class 690. Cost: 108.53382699445527\n",
      "Cluster 31 assigned to Class 696. Cost: 96.86090202465115\n",
      "Cluster 391 assigned to Class 700. Cost: 115.62528929723503\n",
      "Cluster 378 assigned to Class 702. Cost: 78.12468789646405\n",
      "Cluster 259 assigned to Class 705. Cost: 94.17955611278481\n",
      "Cluster 239 assigned to Class 706. Cost: 91.39631798165193\n",
      "Cluster 135 assigned to Class 707. Cost: 83.80627675780734\n",
      "Cluster 6 assigned to Class 710. Cost: 61.54135955584447\n",
      "Cluster 267 assigned to Class 712. Cost: 66.33257726122127\n",
      "Cluster 223 assigned to Class 720. Cost: 62.309897372232854\n",
      "Cluster 306 assigned to Class 721. Cost: 69.8740763084033\n",
      "Cluster 216 assigned to Class 723. Cost: 64.04806084980191\n",
      "Cluster 137 assigned to Class 725. Cost: 63.83552621005081\n",
      "Cluster 353 assigned to Class 726. Cost: 71.44682666980421\n",
      "Cluster 63 assigned to Class 728. Cost: 67.81918378184335\n",
      "Cluster 271 assigned to Class 729. Cost: 91.83035107878973\n",
      "Cluster 202 assigned to Class 731. Cost: 56.14303939070112\n",
      "Cluster 362 assigned to Class 733. Cost: 97.6212748107902\n",
      "Cluster 192 assigned to Class 737. Cost: 70.40593451624913\n",
      "Cluster 112 assigned to Class 739. Cost: 116.49246486438585\n",
      "Cluster 211 assigned to Class 741. Cost: 53.68163790583896\n",
      "Cluster 288 assigned to Class 742. Cost: 97.63756805444041\n",
      "Cluster 287 assigned to Class 745. Cost: 146.61993513381515\n",
      "Cluster 383 assigned to Class 746. Cost: 49.3345884264503\n",
      "Cluster 44 assigned to Class 747. Cost: 90.08218132916738\n",
      "Cluster 276 assigned to Class 748. Cost: 83.96727276361801\n",
      "Cluster 334 assigned to Class 749. Cost: 65.00782130141948\n",
      "Cluster 23 assigned to Class 750. Cost: 60.13230233598007\n",
      "Cluster 91 assigned to Class 752. Cost: 79.87694049366507\n",
      "Cluster 300 assigned to Class 753. Cost: 76.53084103892591\n",
      "Cluster 245 assigned to Class 759. Cost: 67.94357580329547\n",
      "Cluster 363 assigned to Class 761. Cost: 132.46675457555565\n",
      "Cluster 69 assigned to Class 765. Cost: 76.09894108096876\n",
      "Cluster 3 assigned to Class 766. Cost: 140.40233642520542\n",
      "Cluster 161 assigned to Class 769. Cost: 95.13145665282137\n",
      "Cluster 312 assigned to Class 773. Cost: 63.08250327874526\n",
      "Cluster 17 assigned to Class 775. Cost: 73.47674117864614\n",
      "Cluster 156 assigned to Class 776. Cost: 97.94477068745769\n",
      "Cluster 18 assigned to Class 777. Cost: 29.74008642760601\n",
      "Cluster 32 assigned to Class 779. Cost: 76.36282032974218\n",
      "Cluster 141 assigned to Class 780. Cost: 62.3172511809378\n",
      "Cluster 238 assigned to Class 781. Cost: 64.98255016772761\n",
      "Cluster 354 assigned to Class 786. Cost: 70.43000598571741\n",
      "Cluster 118 assigned to Class 788. Cost: 103.38819160594987\n",
      "Cluster 190 assigned to Class 791. Cost: 95.81814958296538\n",
      "Cluster 266 assigned to Class 792. Cost: 110.85661118655445\n",
      "Cluster 111 assigned to Class 793. Cost: 91.2030493723267\n",
      "Cluster 189 assigned to Class 795. Cost: 112.7433363291975\n",
      "Cluster 327 assigned to Class 796. Cost: 76.22879485371949\n",
      "Cluster 20 assigned to Class 798. Cost: 81.63226759061727\n",
      "Cluster 89 assigned to Class 799. Cost: 70.61372826170125\n",
      "Cluster 227 assigned to Class 802. Cost: 74.93425904349384\n",
      "Cluster 299 assigned to Class 803. Cost: 70.81903914458775\n",
      "Cluster 2 assigned to Class 811. Cost: 90.72011777604325\n",
      "Cluster 207 assigned to Class 812. Cost: 83.582644707197\n",
      "Cluster 182 assigned to Class 814. Cost: 52.28576944533919\n",
      "Cluster 22 assigned to Class 818. Cost: 98.65726560591219\n",
      "Cluster 185 assigned to Class 822. Cost: 77.33656127848583\n",
      "Cluster 270 assigned to Class 823. Cost: 93.39876013553817\n",
      "Cluster 226 assigned to Class 827. Cost: 63.57010814729113\n",
      "Cluster 377 assigned to Class 828. Cost: 61.85392432917674\n",
      "Cluster 317 assigned to Class 829. Cost: 80.5182136016427\n",
      "Cluster 253 assigned to Class 830. Cost: 66.4746548139915\n",
      "Cluster 41 assigned to Class 832. Cost: 74.66009519257973\n",
      "Cluster 67 assigned to Class 833. Cost: 107.20933638220227\n",
      "Cluster 37 assigned to Class 836. Cost: 89.18963869477757\n",
      "Cluster 318 assigned to Class 841. Cost: 66.98951030859727\n",
      "Cluster 149 assigned to Class 842. Cost: 121.4590854545663\n",
      "Cluster 62 assigned to Class 856. Cost: 75.95524364000349\n",
      "Cluster 9 assigned to Class 860. Cost: 101.31504385483862\n",
      "Cluster 10 assigned to Class 863. Cost: 79.624112217914\n",
      "Cluster 332 assigned to Class 867. Cost: 65.00151600915652\n",
      "Cluster 73 assigned to Class 871. Cost: 43.78946278008542\n",
      "Cluster 38 assigned to Class 875. Cost: 123.06859174796529\n",
      "Cluster 246 assigned to Class 876. Cost: 74.74738396157105\n",
      "Cluster 357 assigned to Class 877. Cost: 137.20953851971254\n",
      "Cluster 98 assigned to Class 878. Cost: 67.82725682155657\n",
      "Cluster 30 assigned to Class 880. Cost: 92.5325393417965\n",
      "Cluster 165 assigned to Class 883. Cost: 98.87597207380145\n",
      "Cluster 284 assigned to Class 888. Cost: 74.08222122934373\n",
      "Cluster 399 assigned to Class 891. Cost: 97.40945172666217\n",
      "Cluster 269 assigned to Class 893. Cost: 83.59040396739088\n",
      "Cluster 163 assigned to Class 898. Cost: 83.07508561160398\n",
      "Cluster 224 assigned to Class 900. Cost: 59.008807370722415\n",
      "Cluster 396 assigned to Class 901. Cost: 63.279350143762635\n",
      "Cluster 355 assigned to Class 902. Cost: 60.31455790760945\n",
      "Cluster 148 assigned to Class 904. Cost: 124.01805833167992\n",
      "Cluster 29 assigned to Class 907. Cost: 101.37559275070875\n",
      "Cluster 170 assigned to Class 909. Cost: 91.98965028820427\n",
      "Cluster 314 assigned to Class 912. Cost: 60.782474504747796\n",
      "Cluster 106 assigned to Class 916. Cost: 98.66836978272954\n",
      "Cluster 278 assigned to Class 917. Cost: 77.04350605288172\n",
      "Cluster 350 assigned to Class 918. Cost: 74.12637840575935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 197 assigned to Class 923. Cost: 86.31802956676735\n",
      "Cluster 186 assigned to Class 926. Cost: 86.92674292104897\n",
      "Cluster 71 assigned to Class 928. Cost: 70.58901486184187\n",
      "Cluster 11 assigned to Class 929. Cost: 51.452289046637574\n",
      "Cluster 384 assigned to Class 931. Cost: 83.38630339695554\n",
      "Cluster 394 assigned to Class 933. Cost: 91.4122146395504\n",
      "Cluster 107 assigned to Class 937. Cost: 83.94031064838425\n",
      "Cluster 279 assigned to Class 945. Cost: 73.79807976798624\n",
      "Cluster 395 assigned to Class 946. Cost: 84.50342036779023\n",
      "Cluster 375 assigned to Class 949. Cost: 92.94888561365207\n",
      "Cluster 286 assigned to Class 951. Cost: 64.83537540284316\n",
      "Cluster 371 assigned to Class 952. Cost: 74.38065032759785\n",
      "Cluster 351 assigned to Class 957. Cost: 58.87938086933378\n",
      "Cluster 248 assigned to Class 962. Cost: 88.78646131678764\n",
      "Cluster 324 assigned to Class 963. Cost: 87.8170962399355\n",
      "Cluster 251 assigned to Class 964. Cost: 71.06080974298604\n",
      "Cluster 117 assigned to Class 965. Cost: 91.79611597974522\n",
      "Cluster 280 assigned to Class 970. Cost: 91.79133875626012\n",
      "Cluster 35 assigned to Class 972. Cost: 100.20446048409664\n",
      "Cluster 168 assigned to Class 973. Cost: 63.960049835166934\n",
      "Cluster 55 assigned to Class 974. Cost: 84.59739448703532\n",
      "Cluster 346 assigned to Class 980. Cost: 87.38636110826513\n",
      "Cluster 64 assigned to Class 982. Cost: 87.54683143359907\n",
      "Cluster 236 assigned to Class 983. Cost: 91.69249741606203\n",
      "Cluster 209 assigned to Class 985. Cost: 82.62574601008387\n",
      "Cluster 45 assigned to Class 986. Cost: 67.2675526733505\n",
      "Cluster 102 assigned to Class 987. Cost: 91.2918720988809\n",
      "Cluster 179 assigned to Class 988. Cost: 81.27496191391758\n",
      "Cluster 12 assigned to Class 989. Cost: 83.78941239460514\n",
      "Cluster 335 assigned to Class 990. Cost: 52.74264751786065\n",
      "Cluster 138 assigned to Class 991. Cost: 94.15676730563366\n",
      "Cluster 34 assigned to Class 993. Cost: 103.98279182547314\n"
     ]
    }
   ],
   "source": [
    "from ortools.linear_solver import pywraplp\n",
    "dist_list =  np.transpose(tot_dist, (1, 0))\n",
    "from tqdm import tqdm\n",
    "\n",
    "costs = dist_list\n",
    "\n",
    "num_workers = len(costs)\n",
    "num_tasks = len(costs[0])\n",
    "# Create the mip solver with the SCIP backend.\n",
    "solver = pywraplp.Solver.CreateSolver(\"SCIP\")\n",
    "\n",
    "# x[i, j] is an array of 0-1 variables, which will be 1\n",
    "# if worker i is assigned to task j.\n",
    "x = {}\n",
    "for i in range(num_workers):\n",
    "    for j in range(num_tasks):\n",
    "        x[i, j] = solver.IntVar(0, 1, \"\")\n",
    "        \n",
    "# Each worker is assigned to at most 1 task.\n",
    "for i in range(num_workers):\n",
    "    solver.Add(solver.Sum([x[i, j] for j in range(num_tasks)]) <= 1)\n",
    "\n",
    "# Each task is assigned to exactly one worker.\n",
    "for j in range(num_tasks):\n",
    "    solver.Add(solver.Sum([x[i, j] for i in range(num_workers)]) == 1)\n",
    "    \n",
    "objective_terms = []\n",
    "for i in tqdm(range(num_workers)):\n",
    "    for j in range(num_tasks):\n",
    "        objective_terms.append(costs[i][j] * x[i, j])\n",
    "solver.Minimize(solver.Sum(objective_terms))\n",
    "\n",
    "status = solver.Solve()\n",
    "\n",
    "sol_indexes = []\n",
    "if status == pywraplp.Solver.OPTIMAL or status == pywraplp.Solver.FEASIBLE:\n",
    "    print(f\"Total cost = {solver.Objective().Value()}\\n\")\n",
    "    for i in range(num_workers):\n",
    "        for j in range(num_tasks):\n",
    "            # Test if x[i,j] is 1 (with tolerance for floating point arithmetic).\n",
    "            if x[i, j].solution_value() > 0.1:\n",
    "                print(f\"Cluster {j} assigned to Class {i}.\" + f\" Cost: {costs[i][j]}\")\n",
    "                sol_indexes.append(i)\n",
    "else:\n",
    "    print(\"No solution found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e8b000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sol_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c015e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes have been saved to 'cvfinal.pkl'.\n"
     ]
    }
   ],
   "source": [
    "with open('cvfinal.pkl', 'wb') as file:\n",
    "    pickle.dump(sol_indexes, file)\n",
    "\n",
    "print(\"Indexes have been saved to 'cvfinal.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118f0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6783421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.6200830936431885 seconds\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1575fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 52763 instead\n",
      "  warnings.warn(\n",
      "Processing DataFrames:   0%|                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                               | 0/97 [00:00<?, ?it/s]\u001b[A\n",
      "Processing chunks:  53%|███████████████████████████▊                         | 51/97 [00:00<00:00, 473.27it/s]\u001b[A\n",
      "                                                                                                              \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.00 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  25%|████████████▊                                      | 1/4 [12:21<37:04, 741.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                              | 0/101 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                              \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.35 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  50%|█████████████████████████                         | 2/4 [35:25<37:18, 1119.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                               | 0/57 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                              \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.62 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames:  75%|████████████████████████████████████            | 3/4 [1:04:14<23:17, 1397.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing chunks:   0%|                                                               | 0/43 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                              \u001b[AC:\\Users\\shaif\\anaconda3\\envs\\dasks\\lib\\site-packages\\distributed\\client.py:3164: UserWarning: Sending large graph of size 16.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n",
      "Processing DataFrames: 100%|████████████████████████████████████████████████| 4/4 [1:37:18<00:00, 1459.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5867.43959903717 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from dask.distributed import Client\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "\n",
    "# Initialize Dask client\n",
    "client = Client()\n",
    "\n",
    "# Function to compute closest indexes\n",
    "def compute_closest_indexes(chunk, centers, index_set):\n",
    "    result_list = []\n",
    "    for idx, row in chunk.iterrows():\n",
    "        features = np.array(row[2:], dtype=float).reshape(1, -1)\n",
    "        distances = cdist(features, centers, metric='euclidean')\n",
    "        closest_center_idx = np.argmin(distances)\n",
    "        if closest_center_idx in index_set:\n",
    "            result_list.append((closest_center_idx, row[1]))\n",
    "    return result_list\n",
    "\n",
    "# Function to process elements of all DataFrames\n",
    "def process_elements(dfs, centers, index_set, max_len_per_index=500):\n",
    "    result_list = []\n",
    "    index_count = {idx: 0 for idx in index_set}\n",
    "    delayed_results = []\n",
    "\n",
    "    for df in tqdm(dfs, desc=\"Processing DataFrames\"):\n",
    "        print(len(result_list))\n",
    "        # Iterate over chunks of the DataFrame\n",
    "        for chunk in tqdm(df.to_delayed(), desc=\"Processing chunks\", leave=False):\n",
    "            # Process each chunk in parallel\n",
    "            delayed_result = dask.delayed(compute_closest_indexes)(chunk, centers, index_set)\n",
    "            delayed_results.append(delayed_result)\n",
    "\n",
    "        # Compute and collect results\n",
    "        if delayed_results:\n",
    "            chunk_results = dask.compute(*delayed_results)\n",
    "            for chunk_result in chunk_results:\n",
    "                for closest_center_idx, row_1 in chunk_result:\n",
    "                    if index_count[closest_center_idx] < max_len_per_index:\n",
    "                        result_list.append(row_1)\n",
    "                        index_count[closest_center_idx] += 1\n",
    "                        if index_count[closest_center_idx] >= max_len_per_index:\n",
    "                            index_set.remove(closest_center_idx)\n",
    "                        if all(count >= max_len_per_index for count in index_count.values()):\n",
    "                            return result_list\n",
    "\n",
    "    return result_list\n",
    "\n",
    "# Measure the execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the pickle files\n",
    "with open(\"D:/ML_notebooks/cvfinal.pkl\", 'rb') as file:\n",
    "    index = pickle.load(file)\n",
    "with open(\"D:/ML_notebooks/cluster_centers.pkl\", 'rb') as file:\n",
    "    centers = pickle.load(file)\n",
    "\n",
    "# Convert index to a set for faster lookup\n",
    "index_set = set(index)\n",
    "\n",
    "# Define the paths to the CSV files\n",
    "file_paths = [\n",
    "    \"G:/image_features.csv\",\n",
    "    \"G:/image_features1.csv\",\n",
    "    \"G:/image_features2.csv\",\n",
    "    \"G:/image_features3.csv\"\n",
    "]\n",
    "\n",
    "# Load the CSV files into Dask DataFrames\n",
    "dfs = [dd.read_csv(file_path) for file_path in file_paths]\n",
    "\n",
    "# Call the function to process elements of each DataFrame\n",
    "result_list = process_elements(dfs, centers, index_set)\n",
    "\n",
    "# Save the result_list in a pickle file\n",
    "with open(\"cifar_images.pkl\", \"wb\") as file:\n",
    "    pickle.dump(result_list, file)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the execution time\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90056f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d18729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196705\n"
     ]
    }
   ],
   "source": [
    "print(len(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c9cd4-a50e-4b6c-b298-a8d179d3c77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c450d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f43ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
